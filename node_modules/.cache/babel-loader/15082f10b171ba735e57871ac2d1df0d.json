{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils'; // Default batch size used during tensor-based validation.\n\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\n\nfunction standardizeDataIteratorOutput( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n  let xs;\n  let ys;\n  const iteratorOutObj = iteratorOut;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' + 'objects of the form `{xs: xVal, ys: yVal}`, where the two ' + 'values may be `tf.Tensor`, an array of Tensors, or a map of ' + 'string to Tensor.  The provided Dataset instead generates ' + \"\".concat(iteratorOut));\n  const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n  const batchSize = flattenedXs[0].shape[0];\n  tfc.util.assert(flattenedXs.length === model.inputs.length, () => \"LayersModel has \".concat(model.inputs.length, \" inputs, but the dataset \") + \"provides \".concat(flattenedXs.length, \" inputs.  (Expected input keys: \") + \"\".concat(JSON.stringify(model.inputNames), \")\"));\n  tfc.util.assert(flattenedYs.length === model.outputs.length, () => \"LayersModel has \".concat(model.outputs.length, \" outputs, but the dataset \") + \"provides \".concat(flattenedYs.length, \" outputs.  (Expected output keys: \") + \"\".concat(JSON.stringify(model.outputNames), \")\"));\n\n  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => \"Batch size mismatch: input \" + \"\".concat(model.inputNames[xIndex], \" has \").concat(flattenedXs[xIndex].shape[0], \"; \") + \"expected  \".concat(batchSize, \" based on input \").concat(model.inputNames[0], \".\"));\n  }\n\n  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => \"Batch size mismatch: output \" + \"\".concat(model.outputNames[yIndex], \" has \").concat(flattenedYs[yIndex].shape[0], \"; \") + \"expected  \".concat(batchSize, \" based on input \").concat(model.inputNames[0], \".\"));\n  }\n\n  return {\n    xs: flattenedXs,\n    ys: flattenedYs\n  };\n}\n\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(values.length === names.length, () => \"Received an array of \".concat(values.length, \" Tensors, but expected \").concat(names.length, \" to match the \").concat(inputOrOutput, \" keys \").concat(names, \".\"));\n    return values;\n  } else {\n    const result = []; // Check that all the required keys are available.\n\n    for (const name of names) {\n      if (values[name] == null) {\n        throw new ValueError(\"The feature data generated by the dataset lacks the required \" + \"\".concat(inputOrOutput, \" key '\").concat(name, \"'.\"));\n      }\n\n      result.push(values[name]);\n    }\n\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData(data) {\n  if (data.length === 3) {\n    throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n  }\n\n  return {\n    xs: data[0],\n    ys: data[1]\n  };\n}\n\nexport async function fitDataset( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n  const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n  tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileConfig).');\n  tfc.util.assert(args != null, () => \"For fitDataset(), the 2nd argument (config) is required, \" + \"but it is not provided in this call.\");\n  tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => \"For fitDataset(), config.epochs is expected to be a positive \" + \"integer, but got \".concat(args.epochs));\n  tfc.util.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), () => \"For fitDataset(), config.batchesPerEpoch is expected to be a \" + \"positive integer if specified, but got \".concat(args.batchesPerEpoch));\n  tfc.util.assert( // tslint:disable-next-line:no-any\n  args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' + 'Use validationData instead.');\n\n  if (model.isTraining) {\n    throw new Error('Cannot start training because another fit() call is ongoing.');\n  }\n\n  model.isTraining = true;\n\n  try {\n    const doValidation = args.validationData != null;\n    let valXs;\n    let valYs;\n\n    if (doValidation) {\n      if (isDatasetObject(args.validationData)) {\n        tfc.util.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), () => \"For fitDataset() with dataset-based validation, \" + \"config.validationBatches is expected not to be provided, \" + \"or to be a positive integer, \" + \"but got \".concat(args.validationBatches));\n      } else {\n        const validationData = standardizeTensorValidationData(args.validationData);\n        valXs = validationData.xs;\n        valYs = validationData.ys;\n      }\n    }\n\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames();\n    let callbackMetrics;\n\n    if (doValidation) {\n      callbackMetrics = outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const verbose = args.verbose == null ? 1 : args.verbose;\n\n    const _configureCallbacks = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n    doValidation, callbackMetrics),\n          callbackList = _configureCallbacks.callbackList,\n          history = _configureCallbacks.history;\n\n    callbackList.setModel(model);\n    model.history = history;\n    await callbackList.onTrainBegin();\n    model.stopTraining_ = false;\n    let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n    let dataIterator = await dataset.iterator();\n\n    while (epoch < args.epochs) {\n      const epochLogs = {};\n      await callbackList.onEpochBegin(epoch);\n      let stepsDone = 0;\n      let batchIndex = 0;\n\n      if (!hasBatchesPerEpoch) {\n        dataIterator = await dataset.iterator();\n      }\n\n      while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n        const iteratorOut = await dataIterator.next(); // If `batchesPerEpoch` is specified, the dataset should not be\n        // exhausted until all epoches are done.\n\n        if (hasBatchesPerEpoch && iteratorOut.done) {\n          console.warn('You provided `batchesPerEpoch` as ' + \"\".concat(args.batchesPerEpoch, \", \") + 'but your dataset iterator ran out of data after ' + \"\".concat(stepsDone, \" batches; \") + 'interrupting training. Make sure that your ' + 'dataset can generate at least `batchesPerEpoch * epochs` ' + 'batches (in this case, ' + \"\".concat(args.batchesPerEpoch * args.epochs, \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n          break;\n        }\n\n        if (iteratorOut.value != null) {\n          const _standardizeDataItera = standardizeDataIteratorOutput(model, iteratorOut.value),\n                xs = _standardizeDataItera.xs,\n                ys = _standardizeDataItera.ys;\n\n          const batchLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n          const sampleWeights = [];\n\n          if (args.classWeight != null) {\n            const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n\n            for (let i = 0; i < standardClassWeights.length; ++i) {\n              sampleWeights.push((await standardizeWeights(ys[i], null, standardClassWeights[i])));\n            }\n          } // Train on batch.\n\n\n          const ins = xs.concat(ys).concat(sampleWeights);\n          const outs = trainFunction(ins);\n          tfc.dispose(ins);\n\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n          batchIndex++;\n          stepsDone++;\n        }\n\n        if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done) {\n          // Epoch finished. Perform validation.\n          if (doValidation) {\n            let valOuts;\n\n            if (isDatasetObject(args.validationData)) {\n              valOuts = toList((await model.evaluateDataset(args.validationData, {\n                batches: args.validationBatches\n              })));\n            } else {\n              valOuts = toList(model.evaluate(valXs, valYs, {\n                batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,\n                verbose: 0\n              }));\n            }\n\n            for (let i = 0; i < model.metricsNames.length; ++i) {\n              epochLogs[\"val_\".concat(model.metricsNames[i])] = valOuts[i];\n            }\n          } // Call `break` to exit one epoch lopp after validation is done. If\n          // config.batchesPerEpoch is specified, an epoch while loop will\n          // stop when `stepsDone >= config.batchesPerEpoch`. When\n          // config.batchesPerEpoch is not provided, the following `break` is\n          // required to exit the while lopp after dataset is exhausted.\n\n\n          break;\n        }\n\n        if (model.stopTraining_) {\n          break;\n        }\n      }\n\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      epoch++;\n\n      if (model.stopTraining_) {\n        break;\n      }\n    }\n\n    await callbackList.onTrainEnd();\n    await model.history.syncData();\n    return model.history;\n  } finally {\n    model.isTraining = false;\n  }\n}\n/** Helper function that determines number of steps (batches) per epoch. */\n\nfunction getStepsPerEpoch(dataset, args) {\n  // Attempt to determine # of batches in an epoch.\n  let stepsPerEpoch = null;\n\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n\n  return stepsPerEpoch;\n} // Check if provided object is a Dataset object by checking its .iterator\n// element.\n\n\nfunction isDatasetObject(dataset) {\n  return typeof dataset.iterator === 'function';\n} // Check if provided object is a LazyIterator object by checking it's .next\n// element.\n\n\nfunction isLazyIteratorObject(iterator) {\n  return typeof iterator.next === 'function';\n}\n\nexport async function evaluateDataset( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n  args = args || {};\n  const hasBatches = args.batches != null;\n  const f = model.testFunction;\n  let outs = [];\n\n  if (args.verbose > 0) {\n    throw new NotImplementedError('Verbose mode is not implemented yet.');\n  }\n\n  tfc.util.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), () => 'Test loop expects `batches` to be a positive integer, but ' + \"received \".concat(JSON.stringify(args.batches)));\n  const dataIterator = isLazyIteratorObject(dataset) ? dataset : await dataset.iterator(); // Keeps track of number of examples used in this evaluation.\n\n  let numExamples = 0;\n  let batch = 0;\n\n  while (hasBatches ? batch < args.batches : true) {\n    const iteratorOut = await dataIterator.next();\n    outs = tfc.tidy(() => {\n      if (iteratorOut.value) {\n        // TODO(cais): Once real dataset is available, use\n        //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n        const _standardizeDataItera2 = standardizeDataIteratorOutput(model, iteratorOut.value),\n              xs = _standardizeDataItera2.xs,\n              ys = _standardizeDataItera2.ys;\n\n        const xsAndYs = xs.concat(ys);\n        const batchOuts = tfc.tidy(() => f(xsAndYs));\n        tfc.dispose(xsAndYs);\n\n        if (batch === 0) {\n          for (let i = 0; i < batchOuts.length; ++i) {\n            outs.push(scalar(0));\n          }\n        }\n\n        const batchSize = xsAndYs[0].shape[0];\n\n        for (let i = 0; i < batchOuts.length; ++i) {\n          const batchOut = batchOuts[i];\n          const oldScalar = outs[i];\n          outs[i] = tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n\n          if (batch > 0) {\n            tfc.dispose(oldScalar);\n          }\n        }\n\n        tfc.dispose(batchOuts);\n        numExamples += batchSize;\n        ++batch;\n      }\n\n      return outs;\n    });\n\n    if (iteratorOut.done) {\n      if (hasBatches) {\n        console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' + 'Interrupting evalution. Make sure that your ' + 'dataset can generate at least `batches` ' + \"batches (in this case, \".concat(args.batches, \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n      }\n\n      break;\n    }\n  }\n\n  for (let i = 0; i < outs.length; ++i) {\n    const oldScalar = outs[i];\n    outs[i] = tfc.div(outs[i], numExamples);\n    tfc.dispose(oldScalar);\n  }\n\n  return singletonOrArray(outs);\n}","map":null,"metadata":{},"sourceType":"module"}