{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { FusedBatchNorm } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { xAs4D } from './batchnorm_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Batch normalization.\n *\n * As described in\n * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).\n *\n * Mean, variance, scale, and offset can be of two shapes:\n *   - The same shape as the input.\n *   - In the common case, the depth dimension is the last dimension of x, so\n *     the values would be an `tf.Tensor1D` of shape [depth].\n *\n * Also available are stricter rank-specific methods with the same signature\n * as this method that assert that parameters passed are of given rank\n *   - `tf.batchNorm2d`\n *   - `tf.batchNorm3d`\n *   - `tf.batchNorm4d`\n *\n * @param x The input Tensor.\n * @param mean A mean Tensor.\n * @param variance A variance Tensor.\n * @param offset An offset Tensor.\n * @param scale A scale Tensor.\n * @param varianceEpsilon A small float number to avoid dividing by 0.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Normalization'} */\n\nfunction batchNorm_(x, mean, variance, offset, scale, varianceEpsilon) {\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n\n  const $x = convertToTensor(x, 'x', 'batchNorm');\n  const $mean = convertToTensor(mean, 'mean', 'batchNorm');\n  const $variance = convertToTensor(variance, 'variance', 'batchNorm');\n  let $scale;\n\n  if (scale != null) {\n    $scale = convertToTensor(scale, 'scale', 'batchNorm');\n  }\n\n  let $offset;\n\n  if (offset != null) {\n    $offset = convertToTensor(offset, 'offset', 'batchNorm');\n  }\n\n  util.assert($mean.rank === $variance.rank, () => 'Batch normalization gradient requires mean and variance to have ' + 'equal ranks.');\n  util.assert($offset == null || $mean.rank === $offset.rank, () => 'Batch normalization gradient requires mean and offset to have ' + 'equal ranks.');\n  util.assert($scale == null || $mean.rank === $scale.rank, () => 'Batch normalization gradient requires mean and scale to have ' + 'equal ranks.');\n  const x4D = xAs4D($x);\n\n  const forward = (backend, save) => {\n    save([x4D, $mean, $variance, $scale]);\n    return backend.batchNorm(x4D, as1DOr4D($mean), as1DOr4D($variance), as1DOr4D($offset), as1DOr4D($scale), varianceEpsilon);\n  };\n\n  const inputs = {\n    x: x4D,\n    scale: $scale,\n    offset: $offset,\n    mean: $mean,\n    variance: $variance\n  };\n  const attrs = {\n    varianceEpsilon\n  };\n  const res = ENGINE.runKernelFunc(forward, inputs, null\n  /* gradient */\n  , FusedBatchNorm, attrs);\n  return reshape(res, $x.shape);\n}\n\nfunction as1DOr4D(x) {\n  if (x == null) {\n    return null;\n  }\n\n  if (x.rank === 0) {\n    // tslint:disable-next-line:no-unnecessary-type-assertion\n    return reshape(x, [x.size]);\n  } else if (x.rank === 1) {\n    return x;\n  } else if (x.rank === 2) {\n    // tslint:disable-next-line:no-unnecessary-type-assertion\n    return reshape(x, [1, 1, x.shape[0], x.shape[1]]);\n  } else if (x.rank === 3) {\n    // tslint:disable-next-line:no-unnecessary-type-assertion\n    return reshape(x, [1, x.shape[0], x.shape[1], x.shape[2]]);\n  }\n\n  return x;\n}\n\nexport const batchNorm = op({\n  batchNorm_\n});","map":null,"metadata":{},"sourceType":"module"}