{"ast":null,"code":"import _slicedToArray from \"/home/victor/COVID-19-Coding-Fest/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\n\n/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Import webgl flags.\nimport './flags_webgl';\nimport * as tf from '@tensorflow/tfjs-core';\nimport { complex, div, engine, env, imag, max, range, real, reshape, scalar, softmax, tensor, tidy, transpose } from '@tensorflow/tfjs-core';\nimport { backend_util, buffer, kernel_impls, slice_util, util } from '@tensorflow/tfjs-core';\nimport { DataStorage, KernelBackend, upcastType } from '@tensorflow/tfjs-core';\nconst segment_util = backend_util.segment_util;\nconst split = kernel_impls.split;\nconst tile = kernel_impls.tile;\nconst topkImpl = kernel_impls.topkImpl;\nconst whereImpl = kernel_impls.whereImpl;\nimport { AddNProgram } from './addn_gpu';\nimport { AddNPackedProgram } from './addn_packed_gpu';\nimport { ArgMinMaxProgram } from './argminmax_gpu';\nimport { ArgMinMaxPackedProgram } from './argminmax_packed_gpu';\nimport { AvgPool2DBackpropProgram, AvgPool3DBackpropProgram } from './avg_pool_backprop_gpu';\nimport { BatchNormProgram } from './batchnorm_gpu';\nimport { BatchNormPackedProgram } from './batchnorm_packed_gpu';\nimport * as binaryop_complex_gpu from './binaryop_complex_gpu';\nimport { BinaryOpComplexProgram } from './binaryop_complex_gpu';\nimport * as binaryop_gpu from './binaryop_gpu';\nimport { BinaryOpProgram } from './binaryop_gpu';\nimport * as binaryop_packed_gpu from './binaryop_packed_gpu';\nimport { BinaryOpPackedProgram } from './binaryop_packed_gpu';\nimport { getWebGLContext } from './canvas_util';\nimport { ClipProgram } from './clip_gpu';\nimport { ClipPackedProgram } from './clip_packed_gpu';\nimport { ComplexAbsProgram } from './complex_abs_gpu';\nimport { ConcatProgram } from './concat_gpu';\nimport { ConcatPackedProgram } from './concat_packed_gpu';\nimport { Conv2DDerFilterProgram, Conv2DDerInputProgram, Conv3DDerFilterProgram, Conv3DDerInputProgram } from './conv_backprop_gpu';\nimport { DepthwiseConv2DDerFilterProgram, DepthwiseConv2DDerInputProgram } from './conv_backprop_gpu_depthwise';\nimport { Conv2DProgram, Conv3DProgram } from './conv_gpu';\nimport { DepthwiseConv2DProgram } from './conv_gpu_depthwise';\nimport { DepthwiseConvPacked2DProgram } from './conv_packed_gpu_depthwise';\nimport { CropAndResizeProgram } from './crop_and_resize_gpu';\nimport { CumSumProgram } from './cumsum_gpu';\nimport { DecodeMatrixProgram } from './decode_matrix_gpu';\nimport { DecodeMatrixPackedProgram } from './decode_matrix_packed_gpu';\nimport { DepthToSpaceProgram } from './depth_to_space_gpu';\nimport { DiagProgram } from './diag_gpu';\nimport { EncodeFloatProgram } from './encode_float_gpu';\nimport { EncodeFloatPackedProgram } from './encode_float_packed_gpu';\nimport { EncodeMatrixProgram } from './encode_matrix_gpu';\nimport { EncodeMatrixPackedProgram } from './encode_matrix_packed_gpu';\nimport * as fft_gpu from './fft_gpu';\nimport { FFTProgram } from './fft_gpu';\nimport { FillProgram } from './fill_gpu';\nimport { GatherProgram } from './gather_gpu';\nimport { GatherNDProgram } from './gather_nd_gpu';\nimport { GPGPUContext } from './gpgpu_context';\nimport * as gpgpu_math from './gpgpu_math';\nimport { Im2ColPackedProgram } from './im2col_packed_gpu';\nimport { LRNProgram } from './lrn_gpu';\nimport { LRNGradProgram } from './lrn_grad_gpu';\nimport { LRNPackedProgram } from './lrn_packed_gpu';\nimport { MaxPool2DBackpropProgram, MaxPool3DBackpropProgram } from './max_pool_backprop_gpu';\nimport { MatMulPackedProgram } from './mulmat_packed_gpu';\nimport { MultinomialProgram } from './multinomial_gpu';\nimport { OneHotProgram } from './onehot_gpu';\nimport { PackProgram } from './pack_gpu';\nimport { PadProgram } from './pad_gpu';\nimport { PadPackedProgram } from './pad_packed_gpu';\nimport { Pool2DProgram, Pool3DProgram } from './pool_gpu';\nimport { ReduceProgram } from './reduce_gpu';\nimport { ReshapePackedProgram } from './reshape_packed_gpu';\nimport { ResizeBilinearBackpropProgram } from './resize_bilinear_backprop_gpu';\nimport { ResizeBilinearProgram } from './resize_bilinear_gpu';\nimport { ResizeBilinearPackedProgram } from './resize_bilinear_packed_gpu';\nimport { ResizeNearestNeigborBackpropProgram } from './resize_nearest_neighbor_backprop_gpu';\nimport { ResizeNearestNeighborProgram } from './resize_nearest_neighbor_gpu';\nimport { ReverseProgram } from './reverse_gpu';\nimport { ReversePackedProgram } from './reverse_packed_gpu';\nimport { ScatterProgram } from './scatter_gpu';\nimport { SegmentOpProgram } from './segment_gpu';\nimport { SelectProgram } from './select_gpu';\nimport { SliceProgram } from './slice_gpu';\nimport { SlicePackedProgram } from './slice_packed_gpu';\nimport { StridedSliceProgram } from './strided_slice_gpu';\nimport * as tex_util from './tex_util';\nimport { TextureUsage } from './tex_util';\nimport { TextureManager } from './texture_manager';\nimport { TileProgram } from './tile_gpu';\nimport * as unary_op from './unaryop_gpu';\nimport { UnaryOpProgram } from './unaryop_gpu';\nimport * as unary_packed_op from './unaryop_packed_gpu';\nimport { UnaryOpPackedProgram } from './unaryop_packed_gpu';\nimport { UnpackProgram } from './unpack_gpu';\nimport * as webgl_util from './webgl_util';\nexport const EPSILON_FLOAT32 = 1e-7;\nexport const EPSILON_FLOAT16 = 1e-4;\nconst binaryCaches = {};\nexport function getBinaryCache(webGLVersion) {\n  if (webGLVersion in binaryCaches) {\n    return binaryCaches[webGLVersion];\n  }\n\n  binaryCaches[webGLVersion] = {};\n  return binaryCaches[webGLVersion];\n}\n\nfunction mapActivationToShaderProgram(activation, packed = false) {\n  if (activation === 'linear') {\n    if (packed) {\n      return unary_packed_op.LINEAR;\n    }\n\n    return unary_op.LINEAR;\n  } else if (activation === 'relu') {\n    if (packed) {\n      return unary_packed_op.RELU;\n    }\n\n    return unary_op.RELU;\n  } else if (activation === 'elu') {\n    if (packed) {\n      return unary_packed_op.ELU;\n    }\n\n    return unary_op.ELU;\n  } else if (activation === 'relu6') {\n    if (packed) {\n      return unary_packed_op.RELU6;\n    }\n\n    return unary_op.RELU6;\n  } else if (activation === 'prelu') {\n    if (packed) {\n      return binaryop_packed_gpu.PRELU;\n    }\n\n    return binaryop_gpu.PRELU;\n  }\n\n  throw new Error(\"Activation \".concat(activation, \" has not been implemented for the WebGL backend.\"));\n} // Empirically determined constant used to determine size threshold for handing\n// off execution to the CPU.\n\n\nconst CPU_HANDOFF_SIZE_THRESHOLD = 128; // Empirically determined constant used to decide the number of MB on GPU\n// before we warn about high memory use. The MB are this constant * screen area\n// * dpi / 1024 / 1024.\n\nconst BEFORE_PAGING_CONSTANT = 600;\n\nfunction numMBBeforeWarning() {\n  if (env().global.screen == null) {\n    return 1024; // 1 GB.\n  }\n\n  return env().global.screen.height * env().global.screen.width * window.devicePixelRatio * BEFORE_PAGING_CONSTANT / 1024 / 1024;\n} // Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\n\n\nexport const MATMUL_SHARED_DIM_THRESHOLD = 1000;\nexport class MathBackendWebGL extends KernelBackend {\n  constructor(gpgpu) {\n    super(); // Maps data ids that have a pending read operation, to list of subscribers.\n\n    this.pendingRead = new WeakMap(); // List of data ids that are scheduled for disposal, but are waiting on a\n    // pending read operation.\n\n    this.pendingDisposal = new WeakSet(); // Used to count the number of 'shallow' sliced tensors that point to the\n    // same data id.\n\n    this.dataRefCount = new WeakMap();\n    this.numBytesInGPU = 0; // Accumulated time spent (including blocking) in uploading data to webgl.\n\n    this.uploadWaitMs = 0; // Accumulated time spent (including blocking in downloading data from webgl.\n\n    this.downloadWaitMs = 0;\n    this.warnedAboutMemory = false;\n    this.warnedAboutCPUBackend = false;\n    this.pendingDeletes = 0;\n    this.disposed = false;\n\n    if (!env().getBool('HAS_WEBGL')) {\n      throw new Error('WebGL is not supported on this device');\n    }\n\n    if (gpgpu == null) {\n      const gl = getWebGLContext(env().getNumber('WEBGL_VERSION'));\n      this.binaryCache = getBinaryCache(env().getNumber('WEBGL_VERSION'));\n      this.gpgpu = new GPGPUContext(gl);\n      this.canvas = gl.canvas;\n      this.gpgpuCreatedLocally = true;\n    } else {\n      this.gpgpu = gpgpu;\n      this.binaryCache = {};\n      this.gpgpuCreatedLocally = false;\n      this.canvas = gpgpu.gl.canvas;\n    }\n\n    this.textureManager = new TextureManager(this.gpgpu);\n    this.numMBBeforeWarning = numMBBeforeWarning();\n    this.texData = new DataStorage(this, engine());\n  }\n\n  numDataIds() {\n    return this.texData.numDataIds() + (this.cpuBackend ? this.cpuBackend.numDataIds() : 0) - this.pendingDeletes;\n  }\n\n  write(values, shape, dtype) {\n    if (env().getBool('WEBGL_CHECK_NUMERICAL_PROBLEMS') || env().getBool('DEBUG')) {\n      this.checkNumericalProblems(values);\n    }\n\n    if (dtype === 'complex64' && values != null) {\n      throw new Error(\"Cannot write to a complex64 dtype. \" + \"Please use tf.complex(real, imag).\");\n    }\n\n    const dataId = {};\n    this.texData.set(dataId, {\n      shape,\n      dtype,\n      values,\n      usage: TextureUsage.UPLOAD\n    });\n    return dataId;\n  }\n\n  move(dataId, values, shape, dtype) {\n    if (env().getBool('DEBUG')) {\n      this.checkNumericalProblems(values);\n    }\n\n    if (dtype === 'complex64') {\n      throw new Error(\"Cannot write to a complex64 dtype. \" + \"Please use tf.complex(real, imag).\");\n    }\n\n    this.texData.set(dataId, {\n      shape,\n      dtype,\n      values,\n      usage: TextureUsage.UPLOAD\n    });\n  }\n\n  readSync(dataId) {\n    const texData = this.texData.get(dataId);\n    const values = texData.values,\n          dtype = texData.dtype,\n          complexTensors = texData.complexTensors,\n          slice = texData.slice,\n          shape = texData.shape,\n          isPacked = texData.isPacked;\n\n    if (slice != null) {\n      let program;\n\n      if (isPacked) {\n        program = new UnaryOpPackedProgram(shape, unary_op.CLONE);\n      } else {\n        program = new UnaryOpProgram(shape, unary_op.CLONE);\n      }\n\n      const res = this.runWebGLProgram(program, [{\n        dataId,\n        shape,\n        dtype\n      }], dtype);\n      const data = this.readSync(res.dataId);\n      this.disposeData(res.dataId);\n      return data;\n    }\n\n    if (values != null) {\n      return this.convertAndCacheOnCPU(dataId);\n    }\n\n    if (dtype === 'string') {\n      return values;\n    }\n\n    const shouldTimeProgram = this.activeTimers != null;\n    let start;\n\n    if (shouldTimeProgram) {\n      start = util.now();\n    }\n\n    let result;\n\n    if (dtype === 'complex64') {\n      const realValues = complexTensors.real.dataSync();\n      const imagValues = complexTensors.imag.dataSync();\n      result = backend_util.mergeRealAndImagArrays(realValues, imagValues);\n    } else {\n      result = this.getValuesFromTexture(dataId);\n    }\n\n    if (shouldTimeProgram) {\n      this.downloadWaitMs += util.now() - start;\n    }\n\n    return this.convertAndCacheOnCPU(dataId, result);\n  }\n\n  async read(dataId) {\n    if (this.pendingRead.has(dataId)) {\n      const subscribers = this.pendingRead.get(dataId);\n      return new Promise(resolve => subscribers.push(resolve));\n    }\n\n    const texData = this.texData.get(dataId);\n    const values = texData.values,\n          shape = texData.shape,\n          slice = texData.slice,\n          dtype = texData.dtype,\n          complexTensors = texData.complexTensors,\n          isPacked = texData.isPacked;\n\n    if (slice != null) {\n      let program;\n\n      if (isPacked) {\n        program = new UnaryOpPackedProgram(shape, unary_op.CLONE);\n      } else {\n        program = new UnaryOpProgram(shape, unary_op.CLONE);\n      }\n\n      const res = this.runWebGLProgram(program, [{\n        dataId,\n        shape,\n        dtype\n      }], dtype);\n      const data = this.read(res.dataId);\n      this.disposeData(res.dataId);\n      return data;\n    }\n\n    if (values != null) {\n      return this.convertAndCacheOnCPU(dataId);\n    }\n\n    if (!env().getBool('WEBGL_DOWNLOAD_FLOAT_ENABLED') && env().getNumber('WEBGL_VERSION') === 2) {\n      throw new Error(\"tensor.data() with WEBGL_DOWNLOAD_FLOAT_ENABLED=false and \" + \"WEBGL_VERSION=2 not yet supported.\");\n    }\n\n    let buffer = null;\n    let tmpDownloadTarget;\n\n    if (dtype !== 'complex64' && env().get('WEBGL_BUFFER_SUPPORTED')) {\n      // Possibly copy the texture into a buffer before inserting a fence.\n      tmpDownloadTarget = this.decode(dataId);\n      const tmpData = this.texData.get(tmpDownloadTarget.dataId);\n      buffer = this.gpgpu.createBufferFromTexture(tmpData.texture, ...tex_util.getDenseTexShape(shape));\n    }\n\n    this.pendingRead.set(dataId, []);\n\n    if (dtype !== 'complex64') {\n      // Create a fence and wait for it to resolve.\n      await this.gpgpu.createAndWaitForFence();\n    } // Download the values from the GPU.\n\n\n    let vals;\n\n    if (dtype === 'complex64') {\n      const ps = await Promise.all([complexTensors.real.data(), complexTensors.imag.data()]);\n      const realValues = ps[0];\n      const imagValues = ps[1];\n      vals = backend_util.mergeRealAndImagArrays(realValues, imagValues);\n    } else if (buffer == null) {\n      vals = this.getValuesFromTexture(dataId);\n    } else {\n      const size = util.sizeFromShape(shape);\n      vals = this.gpgpu.downloadFloat32MatrixFromBuffer(buffer, size);\n    }\n\n    if (tmpDownloadTarget != null) {\n      this.disposeData(tmpDownloadTarget.dataId);\n    }\n\n    const dTypeVals = this.convertAndCacheOnCPU(dataId, vals);\n    const subscribers = this.pendingRead.get(dataId);\n    this.pendingRead.delete(dataId); // Notify all pending reads.\n\n    subscribers.forEach(resolve => resolve(dTypeVals));\n\n    if (this.pendingDisposal.has(dataId)) {\n      this.pendingDisposal.delete(dataId);\n      this.disposeData(dataId);\n      this.pendingDeletes--;\n    }\n\n    return dTypeVals;\n  }\n\n  checkNumericalProblems(values) {\n    if (values == null) {\n      return;\n    }\n\n    for (let i = 0; i < values.length; i++) {\n      const num = values[i];\n\n      if (!webgl_util.canBeRepresented(num)) {\n        if (env().getBool('WEBGL_RENDER_FLOAT32_CAPABLE')) {\n          throw Error(\"The value \".concat(num, \" cannot be represented with your \") + \"current settings. Consider enabling float32 rendering: \" + \"'tf.env().set('WEBGL_RENDER_FLOAT32_ENABLED', true);'\");\n        }\n\n        throw Error(\"The value \".concat(num, \" cannot be represented on this device.\"));\n      }\n    }\n  }\n\n  getValuesFromTexture(dataId) {\n    const _this$texData$get = this.texData.get(dataId),\n          shape = _this$texData$get.shape,\n          dtype = _this$texData$get.dtype,\n          isPacked = _this$texData$get.isPacked;\n\n    const size = util.sizeFromShape(shape);\n\n    if (env().getBool('WEBGL_DOWNLOAD_FLOAT_ENABLED')) {\n      const tmpTarget = this.decode(dataId);\n      const tmpData = this.texData.get(tmpTarget.dataId);\n      const vals = this.gpgpu.downloadMatrixFromPackedTexture(tmpData.texture, ...tex_util.getDenseTexShape(shape)).subarray(0, size);\n      this.disposeData(tmpTarget.dataId);\n      return vals;\n    }\n\n    const shouldUsePackedProgram = env().getBool('WEBGL_PACK') && isPacked === true;\n    const outputShape = shouldUsePackedProgram ? webgl_util.getShapeAs3D(shape) : shape;\n    const program = shouldUsePackedProgram ? new EncodeFloatPackedProgram(outputShape) : new EncodeFloatProgram(outputShape);\n    const output = this.runWebGLProgram(program, [{\n      shape: outputShape,\n      dtype,\n      dataId\n    }], 'float32');\n    const tmpData = this.texData.get(output.dataId);\n    const vals = this.gpgpu.downloadByteEncodedFloatMatrixFromOutputTexture(tmpData.texture, tmpData.texShape[0], tmpData.texShape[1]).subarray(0, size);\n    this.disposeData(output.dataId);\n    return vals;\n  }\n\n  async time(f) {\n    const oldActiveTimers = this.activeTimers;\n    const newActiveTimers = [];\n    let outerMostTime = false;\n\n    if (this.programTimersStack == null) {\n      this.programTimersStack = newActiveTimers;\n      outerMostTime = true;\n    } else {\n      this.activeTimers.push(newActiveTimers);\n    }\n\n    this.activeTimers = newActiveTimers;\n    f(); // needing to split these up because util.flatten only accepts certain types\n\n    const flattenedActiveTimerQueries = util.flatten(this.activeTimers.map(d => d.query)).filter(d => d != null);\n    const flattenedActiveTimerNames = util.flatten(this.activeTimers.map(d => d.name)).filter(d => d != null);\n    this.activeTimers = oldActiveTimers;\n\n    if (outerMostTime) {\n      this.programTimersStack = null;\n    }\n\n    const res = {\n      uploadWaitMs: this.uploadWaitMs,\n      downloadWaitMs: this.downloadWaitMs,\n      kernelMs: null,\n      wallMs: null // will be filled by the engine\n\n    };\n\n    if (env().getNumber('WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE') > 0) {\n      const kernelMs = await Promise.all(flattenedActiveTimerQueries);\n      res['kernelMs'] = util.sum(kernelMs);\n\n      res['getExtraProfileInfo'] = () => kernelMs.map((d, i) => ({\n        name: flattenedActiveTimerNames[i],\n        ms: d\n      })).map(d => \"\".concat(d.name, \": \").concat(d.ms)).join(', ');\n    } else {\n      res['kernelMs'] = {\n        error: 'WebGL query timers are not supported in this environment.'\n      };\n    }\n\n    this.uploadWaitMs = 0;\n    this.downloadWaitMs = 0;\n    return res;\n  }\n\n  memory() {\n    return {\n      unreliable: false,\n      numBytesInGPU: this.numBytesInGPU,\n      numBytesInGPUAllocated: this.textureManager.numBytesAllocated,\n      numBytesInGPUFree: this.textureManager.numBytesFree\n    };\n  }\n\n  startTimer() {\n    if (env().getNumber('WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE') > 0) {\n      return this.gpgpu.beginQuery();\n    }\n\n    return {\n      startMs: util.now(),\n      endMs: null\n    };\n  }\n\n  endTimer(query) {\n    if (env().getNumber('WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE') > 0) {\n      this.gpgpu.endQuery();\n      return query;\n    }\n\n    query.endMs = util.now();\n    return query;\n  }\n\n  async getQueryTime(query) {\n    if (env().getNumber('WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE') > 0) {\n      return this.gpgpu.waitForQueryAndGetTime(query);\n    }\n\n    const timerQuery = query;\n    return timerQuery.endMs - timerQuery.startMs;\n  }\n\n  disposeData(dataId) {\n    if (this.pendingDisposal.has(dataId)) {\n      return;\n    }\n\n    if (this.pendingRead.has(dataId)) {\n      this.pendingDisposal.add(dataId);\n      this.pendingDeletes++;\n      return;\n    } // No-op if already disposed.\n\n\n    if (!this.texData.has(dataId)) {\n      return;\n    }\n\n    this.releaseGPUData(dataId);\n\n    const _this$texData$get2 = this.texData.get(dataId),\n          complexTensors = _this$texData$get2.complexTensors;\n\n    if (complexTensors != null) {\n      complexTensors.real.dispose();\n      complexTensors.imag.dispose();\n    }\n\n    this.texData.delete(dataId);\n  }\n\n  releaseGPUData(dataId) {\n    const _this$texData$get3 = this.texData.get(dataId),\n          texture = _this$texData$get3.texture,\n          dtype = _this$texData$get3.dtype,\n          texShape = _this$texData$get3.texShape,\n          usage = _this$texData$get3.usage,\n          isPacked = _this$texData$get3.isPacked,\n          slice = _this$texData$get3.slice;\n\n    const key = slice && slice.origDataId || dataId;\n    const refCount = this.dataRefCount.get(key);\n\n    if (refCount > 1) {\n      this.dataRefCount.set(key, refCount - 1);\n    } else {\n      this.dataRefCount.delete(key);\n\n      if (texture != null) {\n        this.numBytesInGPU -= this.computeBytes(texShape, dtype);\n        this.textureManager.releaseTexture(texture, texShape, usage, isPacked);\n      }\n    }\n\n    const texData = this.texData.get(dataId);\n    texData.texture = null;\n    texData.texShape = null;\n    texData.isPacked = false;\n    texData.slice = null;\n  }\n\n  getTexture(dataId) {\n    this.uploadToGPU(dataId);\n    return this.texData.get(dataId).texture;\n  }\n  /**\n   * Returns internal information for the specific data bucket. Used in unit\n   * tests.\n   */\n\n\n  getDataInfo(dataId) {\n    return this.texData.get(dataId);\n  }\n\n  getCPUBackend() {\n    if (!env().getBool('WEBGL_CPU_FORWARD')) {\n      return null;\n    }\n\n    if (this.cpuBackend == null) {\n      this.cpuBackend = engine().findBackend('cpu');\n    }\n\n    return this.cpuBackend;\n  }\n  /*\n  Tests whether all the inputs to an op are small and on the CPU. This heuristic\n  determines when it would be faster to execute a kernel on the CPU. WebGL\n  kernels opt into running this check and forwarding when appropriate.\n  TODO(https://github.com/tensorflow/tfjs/issues/872): Develop a more\n  sustainable strategy for optimizing backend execution of ops.\n   */\n\n\n  shouldExecuteOnCPU(inputs, sizeThreshold = CPU_HANDOFF_SIZE_THRESHOLD) {\n    const cpuBackend = this.getCPUBackend();\n\n    if (!this.warnedAboutCPUBackend && cpuBackend == null && !env().getBool('IS_TEST')) {\n      console.warn('Your application contains ops that are small enough to be ' + 'executed on the CPU backend, however the CPU backend cannot ' + 'be found. Consider importing the CPU backend ' + '(@tensorflow/tfjs-backend-cpu) for better performance.');\n      this.warnedAboutCPUBackend = true;\n    }\n\n    return cpuBackend != null && inputs.every(input => this.texData.get(input.dataId).texture == null && util.sizeFromShape(input.shape) < sizeThreshold);\n  }\n\n  getGPGPUContext() {\n    return this.gpgpu;\n  }\n\n  complex(real, imag) {\n    const result = this.makeOutput(real.shape, 'complex64');\n    const resultData = this.texData.get(result.dataId); // The backend owns the reference to the underlying real and imaginary\n    // clones. These will explicitly get disposed when the complex tensor is\n    // disposed.\n\n    resultData.complexTensors = {\n      real: engine().keep(real.clone()),\n      imag: engine().keep(imag.clone())\n    };\n    return result;\n  }\n\n  real(input) {\n    const resultData = this.texData.get(input.dataId);\n    return resultData.complexTensors.real.clone();\n  }\n\n  imag(input) {\n    const resultData = this.texData.get(input.dataId);\n    return resultData.complexTensors.imag.clone();\n  }\n\n  slice(x, begin, size) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.slice(x, begin, size);\n    } // Short-circuit computation if the slice is zero-sized.\n\n\n    if (util.sizeFromShape(size) === 0) {\n      return tensor([], size, x.dtype);\n    }\n\n    const _this$texData$get4 = this.texData.get(x.dataId),\n          isPacked = _this$texData$get4.isPacked;\n\n    const isContinous = slice_util.isSliceContinous(x.shape, begin, size);\n\n    if (isPacked || !isContinous) {\n      const program = env().getBool('WEBGL_PACK_ARRAY_OPERATIONS') ? new SlicePackedProgram(size) : new SliceProgram(size);\n      const customSetup = program.getCustomSetupFunc(begin);\n      return this.compileAndRun(program, [x], null, customSetup);\n    }\n\n    this.uploadToGPU(x.dataId);\n    return this.shallowSlice(x, begin, size);\n  }\n\n  shallowSlice(x, begin, size) {\n    const xTexData = this.texData.get(x.dataId);\n    const t = this.makeOutput(size, x.dtype);\n    const newTexData = this.texData.get(t.dataId); // Copy texture data from the original tensor.\n\n    Object.assign(newTexData, xTexData);\n    newTexData.shape = size;\n    newTexData.dtype = x.dtype;\n    let flatOffset = slice_util.computeFlatOffset(begin, x.strides);\n\n    if (xTexData.slice) {\n      // We are slicing an already sliced tensor, so we have to accumulate\n      // the offset.\n      flatOffset += xTexData.slice.flatOffset;\n    }\n\n    newTexData.slice = {\n      flatOffset,\n      // Point to the original dataId, which is used to do ref counting.\n      origDataId: xTexData.slice && xTexData.slice.origDataId || x.dataId\n    }; // Increase the ref count for that data bucket.\n\n    const refCount = this.dataRefCount.get(newTexData.slice.origDataId) || 1;\n    this.dataRefCount.set(newTexData.slice.origDataId, refCount + 1);\n    return t;\n  }\n\n  stridedSlice(x, begin, end, strides) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.stridedSlice(x, begin, end, strides);\n    }\n\n    const outShape = slice_util.computeOutShape(begin, end, strides);\n\n    if (outShape.some(axis => axis === 0)) {\n      return tensor([], outShape);\n    }\n\n    const program = new StridedSliceProgram(begin, strides, outShape);\n    return this.compileAndRun(program, [x]);\n  }\n\n  reverse(x, axis) {\n    const program = env().getBool('WEBGL_PACK_ARRAY_OPERATIONS') ? new ReversePackedProgram(x.shape, axis) : new ReverseProgram(x.shape, axis);\n    return this.compileAndRun(program, [x]);\n  }\n\n  concat(tensors, axis) {\n    if (tensors[0].dtype === 'complex64') {\n      const reals = tensors.map(t => real(t));\n      const imags = tensors.map(t => imag(t));\n      return complex(this.concat(reals, axis), this.concat(imags, axis));\n    }\n\n    if (this.shouldExecuteOnCPU(tensors)) {\n      return this.cpuBackend.concat(tensors, axis);\n    }\n\n    if (tensors.length === 1) {\n      return tensors[0];\n    }\n\n    if (tensors.length > env().getNumber('WEBGL_MAX_TEXTURES_IN_SHADER')) {\n      const midIndex = Math.floor(tensors.length / 2);\n      const leftSide = this.concat(tensors.slice(0, midIndex), axis);\n      const rightSide = this.concat(tensors.slice(midIndex), axis);\n      return this.concat([leftSide, rightSide], axis);\n    }\n\n    if (env().getBool('WEBGL_PACK_ARRAY_OPERATIONS') && tensors[0].rank > 1) {\n      const program = new ConcatPackedProgram(tensors.map(t => t.shape), axis);\n      return this.compileAndRun(program, tensors);\n    } // Any concat of n-dimensional tensors across any axis can be reduced to\n    // a concatenation of two-dimensional tensors across the axis 1 by first\n    // partitioning the axes of the original tensors into those less than the\n    // axis to be concatenated and the rest. Then reshape the tensors\n    // into a two-dimensional tensor by collapsing these two sets of axes and\n    // concatenate the resulting matrices across the axis 1, finally reshaping\n    // the result to have the proper shape.\n\n\n    const outShape = backend_util.computeOutShape(tensors.map(t => t.shape), axis);\n    const tensors2D = tensors.map(t => t.as2D(-1, util.sizeFromShape(t.shape.slice(axis))));\n    const program = new ConcatProgram(tensors2D.map(t => t.shape));\n    const res = this.compileAndRun(program, tensors2D);\n    return res.reshape(outShape);\n  }\n\n  neg(x) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.neg(x);\n    }\n\n    if (env().getBool('WEBGL_PACK_UNARY_OPERATIONS')) {\n      return this.packedUnaryOp(x, unary_op.NEG, x.dtype);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.NEG);\n    return this.compileAndRun(program, [x]);\n  }\n\n  batchMatMul(a, b, transposeA, transposeB) {\n    const outerShapeA = transposeA ? a.shape[2] : a.shape[1];\n    const outerShapeB = transposeB ? b.shape[1] : b.shape[2];\n    const sharedDim = transposeA ? a.shape[1] : a.shape[2];\n\n    const _a$shape = _slicedToArray(a.shape, 2),\n          batch = _a$shape[0]; // Since the matrices are vectors, it is faster to call mul().sum()\n    // because sum() is O(sqrt(N)) due to divide-and-conquer.\n\n\n    if ((outerShapeA === 1 || outerShapeB === 1) && sharedDim > MATMUL_SHARED_DIM_THRESHOLD) {\n      if (transposeA) {\n        a = transpose(a, [0, 2, 1]);\n      }\n\n      if (transposeB) {\n        b = transpose(b, [0, 2, 1]);\n      }\n\n      const a3D = outerShapeB === 1 ? a : a.as3D(batch, sharedDim, 1);\n      const axis = outerShapeB === 1 ? 2 : 1;\n      const b3D = outerShapeB === 1 ? b.as3D(batch, 1, sharedDim) : b;\n      return this.multiply(a3D, b3D).sum(axis, true\n      /* keepDims */\n      );\n    }\n\n    const dtype = upcastType(a.dtype, b.dtype);\n    const program = new MatMulPackedProgram(a.shape, [batch, outerShapeA, outerShapeB], transposeA, transposeB);\n    return this.compileAndRun(program, [a, b], dtype);\n  }\n\n  fusedBatchMatMul({\n    a,\n    b,\n    transposeA,\n    transposeB,\n    bias,\n    activation,\n    preluActivationWeights\n  }) {\n    const outerShapeA = transposeA ? a.shape[2] : a.shape[1];\n    const outerShapeB = transposeB ? b.shape[1] : b.shape[2];\n\n    const _a$shape2 = _slicedToArray(a.shape, 2),\n          batch = _a$shape2[0];\n\n    const dtype = upcastType(a.dtype, b.dtype);\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n    const program = new MatMulPackedProgram(a.shape, [batch, outerShapeA, outerShapeB], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights);\n    const inputs = [a, b];\n\n    if (bias) {\n      inputs.push(bias);\n    }\n\n    if (preluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n\n    return this.compileAndRun(program, inputs, dtype);\n  }\n\n  multiply(a, b) {\n    if (a.dtype === 'complex64') {\n      const aData = this.texData.get(a.dataId);\n      const bData = this.texData.get(b.dataId);\n      const realProgram = new BinaryOpComplexProgram(binaryop_complex_gpu.COMPLEX_MULTIPLY.REAL, a.shape, b.shape);\n      const imagProgram = new BinaryOpComplexProgram(binaryop_complex_gpu.COMPLEX_MULTIPLY.IMAG, a.shape, b.shape);\n      const inputs = [this.makeComplexComponentTensorInfo(a, aData.complexTensors.real), this.makeComplexComponentTensorInfo(a, aData.complexTensors.imag), this.makeComplexComponentTensorInfo(b, bData.complexTensors.real), this.makeComplexComponentTensorInfo(b, bData.complexTensors.imag)];\n      const real = this.compileAndRun(realProgram, inputs);\n      const imag = this.compileAndRun(imagProgram, inputs);\n      const complex = this.complex(real, imag);\n      real.dispose();\n      imag.dispose();\n      return complex;\n    }\n\n    if (this.shouldExecuteOnCPU([a, b])) {\n      return this.cpuBackend.multiply(a, b);\n    }\n\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_gpu.MUL, a.dtype);\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.MUL, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], a.dtype);\n  }\n\n  batchNorm(x, mean, variance, offset, scale, varianceEpsilon) {\n    const inputs = [x, mean, variance];\n    let offsetShape = null;\n\n    if (offset != null) {\n      offsetShape = offset.shape;\n      inputs.push(offset);\n    }\n\n    let scaleShape = null;\n\n    if (scale != null) {\n      scaleShape = scale.shape;\n      inputs.push(scale);\n    }\n\n    if (env().getBool('WEBGL_PACK_NORMALIZATION')) {\n      const batchNormPackedProgram = new BatchNormPackedProgram(x.shape, mean.shape, variance.shape, offsetShape, scaleShape, varianceEpsilon);\n      return this.compileAndRun(batchNormPackedProgram, inputs);\n    }\n\n    const batchNormProgram = new BatchNormProgram(x.shape, mean.shape, variance.shape, offsetShape, scaleShape, varianceEpsilon);\n    return this.compileAndRun(batchNormProgram, inputs);\n  }\n\n  localResponseNormalization4D(x, radius, bias, alpha, beta) {\n    const program = env().getBool('WEBGL_PACK_NORMALIZATION') ? new LRNPackedProgram(x.shape, radius, bias, alpha, beta) : new LRNProgram(x.shape, radius, bias, alpha, beta);\n    return this.compileAndRun(program, [x]);\n  }\n\n  LRNGrad(dy, inputImage, outputImage, depthRadius, bias, alpha, beta) {\n    const program = new LRNGradProgram(inputImage.shape, depthRadius, bias, alpha, beta);\n    return this.compileAndRun(program, [inputImage, outputImage, dy]);\n  }\n\n  tile(x, reps) {\n    if (x.dtype === 'string') {\n      const data = this.readSync(x.dataId);\n      const decodedData = data.map(d => util.decodeString(d));\n      const buf = buffer(x.shape, x.dtype, decodedData);\n      return tile(buf, reps);\n    }\n\n    const program = new TileProgram(x.shape, reps);\n    return this.compileAndRun(program, [x]);\n  }\n\n  pad(x, paddings, constantValue) {\n    const program = env().getBool('WEBGL_PACK_ARRAY_OPERATIONS') ? new PadPackedProgram(x.shape, paddings, constantValue) : new PadProgram(x.shape, paddings, constantValue);\n    return this.compileAndRun(program, [x]);\n  }\n\n  gather(x, indices, axis) {\n    if (this.shouldExecuteOnCPU([x, indices])) {\n      return this.cpuBackend.gather(x, indices, axis);\n    }\n\n    const program = new GatherProgram(x.shape, indices.size, axis);\n    return this.compileAndRun(program, [x, indices]);\n  }\n\n  batchToSpaceND(x, blockShape, crops) {\n    util.assert(x.rank <= 4, () => 'batchToSpaceND for rank > 4 with a WebGL backend not ' + 'implemented yet');\n    const prod = blockShape.reduce((a, b) => a * b);\n    const reshaped = backend_util.getReshaped(x.shape, blockShape, prod);\n    const permuted = backend_util.getPermuted(reshaped.length, blockShape.length);\n    const reshapedPermuted = backend_util.getReshapedPermuted(x.shape, blockShape, prod);\n    const sliceBeginCoords = backend_util.getSliceBeginCoords(crops, blockShape.length);\n    const sliceSize = backend_util.getSliceSize(reshapedPermuted, crops, blockShape.length);\n    return transpose(x.reshape(reshaped), permuted).reshape(reshapedPermuted).slice(sliceBeginCoords, sliceSize);\n  }\n\n  spaceToBatchND(x, blockShape, paddings) {\n    util.assert(x.rank <= 4, () => 'spaceToBatchND for rank > 4 with a WebGL backend not ' + 'implemented yet');\n    const prod = blockShape.reduce((a, b) => a * b);\n    const completePaddings = [[0, 0]];\n    completePaddings.push(...paddings);\n\n    for (let i = 1 + blockShape.length; i < x.shape.length; ++i) {\n      completePaddings.push([0, 0]);\n    }\n\n    const paddedX = x.pad(completePaddings);\n    const reshapedPaddedShape = backend_util.getReshaped(paddedX.shape, blockShape, prod, false);\n    const permutedReshapedPaddedPermutation = backend_util.getPermuted(reshapedPaddedShape.length, blockShape.length, false);\n    const flattenShape = backend_util.getReshapedPermuted(paddedX.shape, blockShape, prod, false);\n    const paddedXT = transpose(paddedX.reshape(reshapedPaddedShape), permutedReshapedPaddedPermutation);\n    return reshape(paddedXT, flattenShape);\n  }\n\n  reduce(x, reduceType, dtype) {\n    const batchSize = x.shape[0];\n    const inSize = x.shape[1];\n    const windowSize = backend_util.computeOptimalWindowSize(inSize);\n    const reduceInfo = {\n      windowSize,\n      inSize,\n      batchSize\n    };\n    const program = new ReduceProgram(reduceInfo, reduceType);\n    const output = this.compileAndRun(program, [x], dtype); // No need to run another GPGPU program.\n\n    if (output.shape[1] === 1) {\n      return output;\n    }\n\n    return this.reduce(output, reduceType, dtype);\n  }\n\n  argReduce(x, reduceType, bestIndicesA = null) {\n    let batchSize = x.shape[0];\n    let inSize = x.shape[1];\n\n    if (bestIndicesA != null) {\n      batchSize = bestIndicesA.shape[0];\n      inSize = bestIndicesA.shape[1];\n    }\n\n    const windowSize = backend_util.computeOptimalWindowSize(inSize);\n    const reduceInfo = {\n      windowSize,\n      inSize,\n      batchSize\n    };\n    const program = new ArgMinMaxProgram(reduceInfo, reduceType, bestIndicesA == null);\n    const inputs = [x];\n\n    if (bestIndicesA != null) {\n      inputs.push(bestIndicesA);\n    }\n\n    const output = this.compileAndRun(program, inputs, 'int32'); // No need to run another GPGPU program.\n\n    if (output.shape[1] === 1) {\n      return output;\n    }\n\n    return this.argReduce(x, reduceType, output);\n  }\n\n  argReducePacked(x, reduceType, bestIndicesA = null) {\n    const inShape = bestIndicesA != null ? bestIndicesA.shape : x.shape;\n    const inSize = inShape[inShape.length - 1];\n    const windowSize = backend_util.computeOptimalWindowSize(inSize);\n    const program = new ArgMinMaxPackedProgram(inShape, windowSize, reduceType, bestIndicesA == null);\n    const inputs = bestIndicesA == null ? [x] : [x, bestIndicesA];\n    const output = this.compileAndRun(program, inputs, 'int32');\n\n    if (output.rank === x.rank) {\n      return this.argReducePacked(x, reduceType, output);\n    }\n\n    return output;\n  }\n\n  sum(x, axes) {\n    backend_util.assertAxesAreInnerMostDims('sum', axes, x.rank);\n\n    const _backend_util$compute = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute2 = _slicedToArray(_backend_util$compute, 2),\n          outShape = _backend_util$compute2[0],\n          reduceShape = _backend_util$compute2[1];\n\n    const inSize = util.sizeFromShape(reduceShape);\n    const a2D = x.as2D(-1, inSize);\n    const outputDType = tf.sumOutType(x.dtype);\n    return this.reduce(a2D, 'sum', outputDType).reshape(outShape);\n  }\n\n  prod(x, axes) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.prod(x, axes);\n    }\n\n    const _backend_util$compute3 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute4 = _slicedToArray(_backend_util$compute3, 2),\n          outShape = _backend_util$compute4[0],\n          reduceShape = _backend_util$compute4[1];\n\n    const inSize = util.sizeFromShape(reduceShape);\n    const a2D = x.as2D(-1, inSize);\n    const outputDType = tf.sumOutType(x.dtype);\n    return this.reduce(a2D, 'prod', outputDType).reshape(outShape);\n  }\n\n  unsortedSegmentSum(x, segmentIds, numSegments) {\n    let axis = 0;\n    const permutation = backend_util.getAxesPermutation([axis], x.rank);\n    let permutedX = x;\n\n    if (permutation != null) {\n      permutedX = transpose(x, permutation);\n      axis = backend_util.getInnerMostAxes(1, x.rank)[0];\n    }\n\n    const outShape = segment_util.computeOutShape(permutedX.shape, axis, numSegments);\n    const inSize = util.sizeFromShape([permutedX.shape[axis]]);\n    const a2D = permutedX.as2D(-1, inSize);\n    const outputDType = tf.sumOutType(x.dtype);\n    let result = this.segOpCompute(a2D, 'unsortedSegmentSum', segmentIds, outputDType, numSegments).reshape(outShape);\n\n    if (permutation != null) {\n      result = transpose(result, backend_util.getUndoAxesPermutation(permutation));\n    }\n\n    return result;\n  }\n\n  segOpCompute(x, segOpType, segmentIds, dtype, numSegments) {\n    const batchSize = x.shape[0];\n    const inSize = x.shape[1];\n    const windowSize = segment_util.segOpComputeOptimalWindowSize(inSize, numSegments);\n    const segOpInfo = {\n      windowSize,\n      inSize,\n      batchSize,\n      numSegments\n    };\n    const program = new SegmentOpProgram(segOpInfo, segOpType);\n    const output = this.compileAndRun(program, [x, segmentIds], dtype); // No need to run another GPGPU program.\n\n    if (output.shape[1] === numSegments) {\n      return output;\n    }\n\n    segmentIds = range(0, numSegments).tile([inSize / windowSize]);\n    return this.segOpCompute(output, segOpType, segmentIds, dtype, numSegments);\n  }\n\n  argMinMaxReduce(x, axis, reduceType) {\n    const axes = [axis];\n    backend_util.assertAxesAreInnerMostDims('arg' + reduceType.charAt(0).toUpperCase() + reduceType.slice(1), axes, x.rank);\n\n    if (!env().getBool('WEBGL_PACK_REDUCE') || x.rank <= 2) {\n      const _backend_util$compute5 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n            _backend_util$compute6 = _slicedToArray(_backend_util$compute5, 2),\n            outShape = _backend_util$compute6[0],\n            reduceShape = _backend_util$compute6[1];\n\n      const inSize = util.sizeFromShape(reduceShape);\n      const a2D = x.as2D(-1, inSize);\n      return this.argReduce(a2D, reduceType).reshape(outShape);\n    }\n\n    return this.argReducePacked(x, reduceType);\n  }\n\n  argMin(x, axis) {\n    return this.argMinMaxReduce(x, axis, 'min');\n  }\n\n  argMax(x, axis) {\n    return this.argMinMaxReduce(x, axis, 'max');\n  }\n\n  cumsum(x, axis, exclusive, reverse) {\n    if (axis !== x.rank - 1) {\n      throw new Error(\"WebGL cumsum shader expects an inner-most axis=\".concat(x.rank - 1, \" \") + \"but got axis=\".concat(axis));\n    }\n\n    const size = x.shape[axis];\n    let result = x; // Use cumsum parallel algorithm, ref:\n    // https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda\n\n    for (let i = 0; i <= Math.ceil(Math.log2(size)) - 1; i++) {\n      const program = new CumSumProgram(x.shape, false, reverse);\n      const customSetup = program.getCustomSetupFunc(i);\n      const prevResult = result;\n      result = this.compileAndRun(program, [result], result.dtype, customSetup);\n      prevResult.dispose();\n    } // For exclusive cumsum, shift the end result in the direction of sum and\n    // add 0 to the front index.\n\n\n    if (exclusive) {\n      const program = new CumSumProgram(x.shape, exclusive, reverse);\n      const prevResult = result;\n      result = this.compileAndRun(program, [result]);\n      prevResult.dispose();\n    }\n\n    return result;\n  }\n\n  equal(a, b) {\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.EQUAL, 'bool');\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.EQUAL, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], 'bool');\n  }\n\n  notEqual(a, b) {\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.NOT_EQUAL, 'bool');\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.NOT_EQUAL, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], 'bool');\n  }\n\n  less(a, b) {\n    if (this.shouldExecuteOnCPU([a, b])) {\n      return this.cpuBackend.less(a, b);\n    }\n\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.LESS, 'bool');\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.LESS, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], 'bool');\n  }\n\n  lessEqual(a, b) {\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.LESS_EQUAL, 'bool');\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.LESS_EQUAL, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], 'bool');\n  }\n\n  greater(a, b) {\n    if (this.shouldExecuteOnCPU([a, b])) {\n      return this.cpuBackend.greater(a, b);\n    }\n\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.GREATER, 'bool');\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.GREATER, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], 'bool');\n  }\n\n  greaterEqual(a, b) {\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.GREATER_EQUAL, 'bool');\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.GREATER_EQUAL, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], 'bool');\n  }\n\n  logicalNot(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.LOGICAL_NOT);\n    return this.compileAndRun(program, [x]);\n  }\n\n  logicalAnd(a, b) {\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.LOGICAL_AND, 'bool');\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.LOGICAL_AND, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], 'bool');\n  }\n\n  logicalOr(a, b) {\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.LOGICAL_OR, 'bool');\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.LOGICAL_OR, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], 'bool');\n  }\n\n  select(condition, a, b) {\n    const program = new SelectProgram(condition.rank, a.shape, a.rank);\n    return this.compileAndRun(program, [condition, a, b], upcastType(a.dtype, b.dtype));\n  }\n\n  where(condition) {\n    backend_util.warn('tf.where() in webgl locks the UI thread. ' + 'Call tf.whereAsync() instead');\n    const condVals = condition.dataSync();\n    return whereImpl(condition.shape, condVals);\n  }\n\n  topk(x, k, sorted) {\n    const xVals = x.dataSync();\n    return topkImpl(xVals, x.shape, x.dtype, k, sorted);\n  }\n\n  min(x, axes) {\n    backend_util.assertAxesAreInnerMostDims('min', axes, x.rank);\n\n    const _backend_util$compute7 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute8 = _slicedToArray(_backend_util$compute7, 2),\n          outShape = _backend_util$compute8[0],\n          reduceShape = _backend_util$compute8[1];\n\n    const inSize = util.sizeFromShape(reduceShape);\n    const a2D = x.as2D(-1, inSize);\n    return this.reduce(a2D, 'min', a2D.dtype).reshape(outShape);\n  }\n\n  minimum(a, b) {\n    if (this.shouldExecuteOnCPU([a, b])) {\n      return this.cpuBackend.minimum(a, b);\n    }\n\n    const program = env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ? new BinaryOpPackedProgram(binaryop_packed_gpu.MIN, a.shape, b.shape) : new BinaryOpProgram(binaryop_gpu.MIN, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b]);\n  }\n\n  mod(a, b) {\n    const program = env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ? new BinaryOpPackedProgram(binaryop_packed_gpu.MOD, a.shape, b.shape) : new BinaryOpProgram(binaryop_gpu.MOD, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b]);\n  }\n\n  maximum(a, b) {\n    if (this.shouldExecuteOnCPU([a, b])) {\n      return this.cpuBackend.maximum(a, b);\n    }\n\n    const program = env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ? new BinaryOpPackedProgram(binaryop_packed_gpu.MAX, a.shape, b.shape) : new BinaryOpProgram(binaryop_gpu.MAX, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b]);\n  }\n\n  all(x, axes) {\n    backend_util.assertAxesAreInnerMostDims('all', axes, x.rank);\n\n    const _backend_util$compute9 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute10 = _slicedToArray(_backend_util$compute9, 2),\n          outShape = _backend_util$compute10[0],\n          reduceShape = _backend_util$compute10[1];\n\n    const inSize = util.sizeFromShape(reduceShape);\n    const a2D = x.as2D(-1, inSize);\n    return this.reduce(a2D, 'all', a2D.dtype).reshape(outShape);\n  }\n\n  any(x, axes) {\n    backend_util.assertAxesAreInnerMostDims('any', axes, x.rank);\n\n    const _backend_util$compute11 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute12 = _slicedToArray(_backend_util$compute11, 2),\n          outShape = _backend_util$compute12[0],\n          reduceShape = _backend_util$compute12[1];\n\n    const inSize = util.sizeFromShape(reduceShape);\n    const a2D = x.as2D(-1, inSize);\n    return this.reduce(a2D, 'any', a2D.dtype).reshape(outShape);\n  }\n\n  floorDiv(a, b) {\n    const op = binaryop_gpu.INT_DIV;\n    const outputDtype = 'int32';\n\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_packed_gpu.INT_DIV, outputDtype);\n    }\n\n    const program = new BinaryOpProgram(op, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], outputDtype);\n  }\n\n  add(a, b) {\n    if (a.dtype === 'complex64' && b.dtype === 'complex64') {\n      return this.complexSeparableBinaryOp(a, b, binaryop_gpu.ADD);\n    }\n\n    if (this.shouldExecuteOnCPU([a, b])) {\n      return this.cpuBackend.add(a, b);\n    }\n\n    const dtype = upcastType(a.dtype, b.dtype);\n\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_gpu.ADD, dtype);\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.ADD, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], dtype);\n  }\n\n  packedUnaryOp(x, op, dtype) {\n    const program = new UnaryOpPackedProgram(x.shape, op);\n    return this.compileAndRun(program, [x], dtype);\n  }\n\n  packedBinaryOp(a, b, op, dtype, checkOutOfBounds = false) {\n    const program = new BinaryOpPackedProgram(op, a.shape, b.shape, checkOutOfBounds);\n    return this.compileAndRun(program, [a, b], dtype);\n  }\n  /**\n   * Computes a complex binary operation that can be decomposed into a simple\n   * binary operation on both the real and imagary parts.\n   */\n\n\n  complexSeparableBinaryOp(a, b, op) {\n    const aData = this.texData.get(a.dataId);\n    const bData = this.texData.get(b.dataId);\n\n    const _map = [[aData.complexTensors.real, bData.complexTensors.real], [aData.complexTensors.imag, bData.complexTensors.imag]].map(complexParts => {\n      const _complexParts = _slicedToArray(complexParts, 2),\n            aPart = _complexParts[0],\n            bPart = _complexParts[1];\n\n      const aHandle = this.makeComplexComponentTensorInfo(a, aPart);\n      const bHandle = this.makeComplexComponentTensorInfo(b, bPart);\n      const program = new BinaryOpProgram(op, a.shape, b.shape);\n      return this.compileAndRun(program, [aHandle, bHandle], upcastType(aPart.dtype, bPart.dtype));\n    }),\n          _map2 = _slicedToArray(_map, 2),\n          real = _map2[0],\n          imag = _map2[1];\n\n    const complex = this.complex(real, imag);\n    real.dispose();\n    imag.dispose();\n    return complex;\n  } // Returns a TensorInfo with the complex shape and the dataId of the\n  // underlying part. We need to do this because a reshaped complex tensor is\n  // not reflected in its parts.\n\n\n  makeComplexComponentTensorInfo(complexTensor, complexPart) {\n    return {\n      dataId: complexPart.dataId,\n      dtype: complexPart.dtype,\n      shape: complexTensor.shape\n    };\n  }\n\n  addN(tensors) {\n    if (tensors.length === 1) {\n      return tensors[0];\n    } // Limit the number of uploaded textures for optimization.\n\n\n    if (tensors.length > env().get('WEBGL_MAX_TEXTURES_IN_SHADER')) {\n      const midIndex = Math.floor(tensors.length / 2);\n      const leftSide = this.addN(tensors.slice(0, midIndex));\n      const rightSide = this.addN(tensors.slice(midIndex));\n      return this.addN([leftSide, rightSide]);\n    }\n\n    const dtype = tensors.map(t => t.dtype).reduce((d1, d2) => upcastType(d1, d2));\n    const shapes = tensors.map(t => t.shape); // We can make sure shapes are identical in op level.\n\n    const usePackedOp = env().getBool('WEBGL_PACK');\n    const program = usePackedOp ? new AddNPackedProgram(tensors[0].shape, shapes) : new AddNProgram(tensors[0].shape, shapes);\n    return this.compileAndRun(program, tensors, dtype);\n  }\n\n  subtract(a, b) {\n    if (a.dtype === 'complex64' && b.dtype === 'complex64') {\n      return this.complexSeparableBinaryOp(a, b, binaryop_gpu.SUB);\n    }\n\n    if (this.shouldExecuteOnCPU([a, b])) {\n      return this.cpuBackend.subtract(a, b);\n    }\n\n    const dtype = upcastType(a.dtype, b.dtype);\n\n    if (env().getBool('WEBGL_PACK_BINARY_OPERATIONS')) {\n      return this.packedBinaryOp(a, b, binaryop_gpu.SUB, a.dtype);\n    }\n\n    const program = new BinaryOpProgram(binaryop_gpu.SUB, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b], dtype);\n  }\n\n  pow(a, b) {\n    const usePackedOp = env().getBool('WEBGL_PACK_BINARY_OPERATIONS');\n    const program = usePackedOp ? new BinaryOpPackedProgram(binaryop_packed_gpu.POW, a.shape, b.shape) : new BinaryOpProgram(binaryop_gpu.POW, a.shape, b.shape);\n    const dtype = upcastType(a.dtype, b.dtype);\n    return this.compileAndRun(program, [a, b], dtype);\n  }\n\n  ceil(x) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.ceil(x);\n    }\n\n    if (env().getBool('WEBGL_PACK_UNARY_OPERATIONS')) {\n      return this.packedUnaryOp(x, unary_op.CEIL, x.dtype);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.CEIL);\n    return this.compileAndRun(program, [x]);\n  }\n\n  floor(x) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.floor(x);\n    }\n\n    if (env().getBool('WEBGL_PACK_UNARY_OPERATIONS')) {\n      return this.packedUnaryOp(x, unary_op.FLOOR, x.dtype);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.FLOOR);\n    return this.compileAndRun(program, [x]);\n  }\n\n  sign(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.SIGN);\n    return this.compileAndRun(program, [x]);\n  }\n\n  isNaN(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.IS_NAN);\n    return this.compileAndRun(program, [x], 'bool');\n  }\n\n  isInf(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.IS_INF);\n    return this.compileAndRun(program, [x], 'bool');\n  }\n\n  isFinite(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.IS_FINITE);\n    return this.compileAndRun(program, [x], 'bool');\n  }\n\n  round(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.ROUND);\n    return this.compileAndRun(program, [x]);\n  }\n\n  exp(x) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.exp(x);\n    }\n\n    if (env().getBool('WEBGL_PACK_UNARY_OPERATIONS')) {\n      return this.packedUnaryOp(x, unary_op.EXP, x.dtype);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.EXP);\n    return this.compileAndRun(program, [x]);\n  }\n\n  expm1(x) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.expm1(x);\n    }\n\n    if (env().getBool('WEBGL_PACK_UNARY_OPERATIONS')) {\n      return this.packedUnaryOp(x, unary_op.EXPM1, x.dtype);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.EXPM1);\n    return this.compileAndRun(program, [x]);\n  }\n\n  softmax(logits, dim) {\n    const axes = util.parseAxisParam([dim], logits.shape); // TODO(annxingyuan): Call maxImpl rather than op as part of softmax kernel\n    // modularization.\n\n    const maxLogit = max(logits, axes);\n    const expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n    const a = this.subtract(logits, maxLogit.reshape(expandedShape));\n    const b = this.exp(a);\n    const sumExp = this.sum(b, axes).reshape(expandedShape); // TODO(annxingyuan): Call divImpl rather than op as part of softmax kernel\n    // modularization.\n\n    return div(b, sumExp);\n  }\n\n  log(x) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.log(x);\n    }\n\n    if (env().getBool('WEBGL_PACK_UNARY_OPERATIONS')) {\n      return this.packedUnaryOp(x, unary_packed_op.LOG, x.dtype);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.LOG);\n    return this.compileAndRun(program, [x]);\n  }\n\n  log1p(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.LOG1P);\n    return this.compileAndRun(program, [x]);\n  }\n\n  sqrt(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.SQRT);\n    return this.compileAndRun(program, [x]);\n  }\n\n  rsqrt(x) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.rsqrt(x);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.RSQRT);\n    return this.compileAndRun(program, [x]);\n  }\n\n  reciprocal(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.RECIPROCAL);\n    return this.compileAndRun(program, [x]);\n  }\n\n  relu(x) {\n    let program;\n\n    if (env().getBool('WEBGL_PACK')) {\n      program = new UnaryOpPackedProgram(x.shape, unary_packed_op.RELU);\n    } else {\n      program = new UnaryOpProgram(x.shape, unary_op.RELU);\n    }\n\n    return this.compileAndRun(program, [x]);\n  }\n\n  relu6(x) {\n    let program;\n\n    if (env().getBool('WEBGL_PACK')) {\n      program = new UnaryOpPackedProgram(x.shape, unary_packed_op.RELU6);\n    } else {\n      program = new UnaryOpProgram(x.shape, unary_op.RELU6);\n    }\n\n    return this.compileAndRun(program, [x]);\n  }\n\n  prelu(x, alpha) {\n    const program = env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ? new BinaryOpPackedProgram(binaryop_packed_gpu.PRELU, x.shape, alpha.shape) : new BinaryOpProgram(binaryop_gpu.PRELU, x.shape, alpha.shape);\n    return this.compileAndRun(program, [x, alpha]);\n  }\n\n  elu(x) {\n    if (env().getBool('WEBGL_PACK_UNARY_OPERATIONS')) {\n      return this.packedUnaryOp(x, unary_packed_op.ELU, x.dtype);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.ELU);\n    return this.compileAndRun(program, [x]);\n  }\n\n  eluDer(dy, y) {\n    const program = env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ? new BinaryOpPackedProgram(binaryop_packed_gpu.ELU_DER, dy.shape, y.shape) : new BinaryOpProgram(binaryop_gpu.ELU_DER, dy.shape, y.shape);\n    return this.compileAndRun(program, [dy, y]);\n  }\n\n  selu(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.SELU);\n    return this.compileAndRun(program, [x]);\n  }\n\n  int(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.TO_INT);\n    return this.compileAndRun(program, [x], 'int32');\n  }\n\n  clip(x, min, max) {\n    let program;\n\n    if (env().getBool('WEBGL_PACK_CLIP')) {\n      program = new ClipPackedProgram(x.shape);\n    } else {\n      program = new ClipProgram(x.shape);\n    }\n\n    const customSetup = program.getCustomSetupFunc(min, max);\n    return this.compileAndRun(program, [x], null, customSetup);\n  }\n\n  abs(x) {\n    if (this.shouldExecuteOnCPU([x])) {\n      return this.cpuBackend.abs(x);\n    }\n\n    if (env().getBool('WEBGL_PACK_UNARY_OPERATIONS')) {\n      return this.packedUnaryOp(x, unary_op.ABS, x.dtype);\n    }\n\n    const program = new UnaryOpProgram(x.shape, unary_op.ABS);\n    return this.compileAndRun(program, [x]);\n  }\n\n  complexAbs(x) {\n    const xData = this.texData.get(x.dataId);\n    const program = new ComplexAbsProgram(x.shape);\n    const inputs = [this.makeComplexComponentTensorInfo(x, xData.complexTensors.real), this.makeComplexComponentTensorInfo(x, xData.complexTensors.imag)];\n    return this.compileAndRun(program, inputs);\n  }\n\n  sigmoid(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.SIGMOID);\n    return this.compileAndRun(program, [x]);\n  }\n\n  softplus(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.SOFTPLUS);\n    return this.compileAndRun(program, [x]);\n  }\n\n  sin(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.SIN);\n    return this.compileAndRun(program, [x]);\n  }\n\n  cos(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.COS);\n    return this.compileAndRun(program, [x]);\n  }\n\n  tan(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.TAN);\n    return this.compileAndRun(program, [x]);\n  }\n\n  asin(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.ASIN);\n    return this.compileAndRun(program, [x]);\n  }\n\n  acos(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.ACOS);\n    return this.compileAndRun(program, [x]);\n  }\n\n  atan(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.ATAN);\n    return this.compileAndRun(program, [x]);\n  }\n\n  atan2(a, b) {\n    const program = env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ? new BinaryOpPackedProgram(binaryop_packed_gpu.ATAN2, a.shape, b.shape) : new BinaryOpProgram(binaryop_gpu.ATAN2, a.shape, b.shape);\n    return this.compileAndRun(program, [a, b]);\n  }\n\n  sinh(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.SINH);\n    return this.compileAndRun(program, [x]);\n  }\n\n  cosh(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.COSH);\n    return this.compileAndRun(program, [x]);\n  }\n\n  tanh(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.TANH);\n    return this.compileAndRun(program, [x]);\n  }\n\n  asinh(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.ASINH);\n    return this.compileAndRun(program, [x]);\n  }\n\n  acosh(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.ACOSH);\n    return this.compileAndRun(program, [x]);\n  }\n\n  atanh(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.ATANH);\n    return this.compileAndRun(program, [x]);\n  }\n\n  erf(x) {\n    const program = new UnaryOpProgram(x.shape, unary_op.ERF);\n    return this.compileAndRun(program, [x]);\n  }\n\n  step(x, alpha) {\n    const program = new UnaryOpProgram(x.shape, unary_op.STEP(alpha));\n    return this.compileAndRun(program, [x]);\n  }\n\n  conv2dByMatMul(x, filter, convInfo, bias, activation, preluActivationWeights) {\n    // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n    // result from 2D to 4D.\n    const xShape = x.shape;\n    const xTexData = this.texData.get(x.dataId);\n    const sharedMatMulDim = convInfo.inChannels;\n    const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n    const outerShapeFilter = convInfo.outChannels;\n    const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    const transposeA = false;\n    const transposeB = false; // TODO: Once reduction ops are packed, batchMatMul will always be packed\n    // and we can remove this condition.\n\n    const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n    const reshapeWillBeExpensive = xShape[2] % 2 !== 0 && !!xTexData.isPacked;\n\n    if (batchMatMulWillBeUnpacked || !env().getBool('WEBGL_LAZILY_UNPACK') || !env().getBool('WEBGL_PACK_BINARY_OPERATIONS') || !reshapeWillBeExpensive) {\n      const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] : xShape[0] * xShape[2] * xShape[3];\n      const xReshaped = this.reshape(x, [1, targetShape, convInfo.inChannels]);\n      const filterReshaped = this.reshape(filter, [1, convInfo.inChannels, convInfo.outChannels]);\n      return this.reshape(this.fusedBatchMatMul({\n        a: xReshaped,\n        b: filterReshaped,\n        transposeA,\n        transposeB,\n        bias,\n        activation,\n        preluActivationWeights\n      }), convInfo.outShape);\n    } // Following optimization is specific to packed |x| with odd row count\n    // (For example, in channelLast mode, 'row count' refers to x.shape[2]):\n    // we avoid expensive packed 2x2 reshape by padding row count to next,\n    // even number. When x.shape[2] is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for even x.shape[2] + 1. We make the odd-rows tensor to look like\n    // even-rows tensor before the operation and, after the batchMatMul,\n    // fix the even-rows result to have odd number of rows.\n\n\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * (xShape[2] + 1) : xShape[0] * xShape[2] * (xShape[3] + 1);\n    const xReshaped = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    }; // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing row count, after batchMatMul->...->compileProgram leads to\n    // invalid row count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even row count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => \"packed reshape \".concat(xTexData.shape, \" to \").concat(xReshaped.shape, \" isn't free\"));\n    const filterReshaped = this.reshape(filter, [1, convInfo.inChannels, convInfo.outChannels]);\n    const pointwiseConv = this.fusedBatchMatMul({\n      a: xReshaped,\n      b: filterReshaped,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights\n    });\n    const pointwiseConvTexData = this.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed'); // Restore the input shape to original.\n\n    xTexData.shape = originalXTexDataShape; // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n\n    pointwiseConvTexData.shape = convInfo.outShape;\n    return engine().makeTensorFromDataId(pointwiseConv.dataId, convInfo.outShape, pointwiseConv.dtype);\n  }\n\n  conv2dWithIm2Row(x, filter, convInfo, bias, activation, preluActivationWeights) {\n    // Rearranges conv2d input so each block to be convolved over forms the\n    // column of a new matrix with shape [filterWidth * filterHeight *\n    // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n    // output channel forms a row of a new matrix with shape [outChannels,\n    // filterWidth * filterHeight * inChannels]. The convolution is then\n    // computed by multiplying these matrices and reshaping the result.\n    const filterWidth = convInfo.filterWidth,\n          filterHeight = convInfo.filterHeight,\n          inChannels = convInfo.inChannels,\n          outWidth = convInfo.outWidth,\n          outHeight = convInfo.outHeight,\n          dataFormat = convInfo.dataFormat;\n    const isChannelsLast = dataFormat === 'channelsLast';\n    const sharedDim = filterWidth * filterHeight * inChannels;\n    const numCols = outHeight * outWidth;\n    const x2ColShape = [sharedDim, numCols];\n    const transposeA = true;\n    const transposeB = false;\n    const xSqueezed = x.squeeze([0]);\n    const w2Row = filter.reshape([1, sharedDim, -1]);\n    const im2ColProgram = new Im2ColPackedProgram(x2ColShape, xSqueezed.shape, convInfo);\n    const im2Col = this.compileAndRun(im2ColProgram, [xSqueezed]).reshape([1, x2ColShape[0], x2ColShape[1]]);\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n    const matmulProgram = new MatMulPackedProgram(im2Col.shape, [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights);\n    const inputs = [im2Col, w2Row];\n\n    if (bias) {\n      inputs.push(bias);\n    }\n\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n\n    const product = this.compileAndRun(matmulProgram, inputs);\n\n    if (isChannelsLast) {\n      return product.reshape([1, outHeight, outWidth, convInfo.outChannels]);\n    } else {\n      return product.reshape([1, convInfo.outChannels, outHeight, outWidth]);\n    }\n  }\n\n  fusedConv2d({\n    input,\n    filter,\n    convInfo,\n    bias,\n    activation,\n    preluActivationWeights\n  }) {\n    if (convInfo.filterHeight === 1 && convInfo.filterWidth === 1 && convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 && convInfo.strideHeight === 1 && convInfo.strideWidth === 1 && (convInfo.padInfo.type === 'SAME' || convInfo.padInfo.type === 'VALID')) {\n      return this.conv2dByMatMul(input, filter, convInfo, bias, activation, preluActivationWeights);\n    }\n\n    if (env().getBool('WEBGL_CONV_IM2COL') && input.shape[0] === 1) {\n      return this.conv2dWithIm2Row(input, filter, convInfo, bias, activation, preluActivationWeights);\n    }\n\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    const fusedActivation = activation ? mapActivationToShaderProgram(activation, false) : null;\n    const program = new Conv2DProgram(convInfo, hasBias, fusedActivation, hasPreluActivationWeights);\n    const inputs = [input, filter];\n\n    if (bias) {\n      inputs.push(bias);\n    }\n\n    if (preluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n\n    return this.compileAndRun(program, inputs);\n  }\n\n  conv2d(x, filter, convInfo) {\n    if (convInfo.filterHeight === 1 && convInfo.filterWidth === 1 && convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 && convInfo.strideHeight === 1 && convInfo.strideWidth === 1 && (convInfo.padInfo.type === 'SAME' || convInfo.padInfo.type === 'VALID')) {\n      return this.conv2dByMatMul(x, filter, convInfo);\n    }\n\n    if (env().getBool('WEBGL_CONV_IM2COL') && x.shape[0] === 1) {\n      return this.conv2dWithIm2Row(x, filter, convInfo);\n    }\n\n    const program = new Conv2DProgram(convInfo);\n    return this.compileAndRun(program, [x, filter]);\n  }\n\n  conv2dDerInput(dy, filter, convInfo) {\n    const program = new Conv2DDerInputProgram(convInfo);\n    return this.compileAndRun(program, [dy, filter]);\n  }\n\n  conv2dDerFilter(x, dy, convInfo) {\n    const program = new Conv2DDerFilterProgram(convInfo);\n    return this.compileAndRun(program, [x, dy]);\n  }\n\n  fusedDepthwiseConv2D({\n    input,\n    filter,\n    convInfo,\n    bias,\n    activation,\n    preluActivationWeights\n  }) {\n    const shouldPackDepthwiseConv = env().getBool('WEBGL_PACK_DEPTHWISECONV') && convInfo.strideWidth <= 2 && convInfo.outChannels / convInfo.inChannels === 1;\n    const fusedActivation = activation ? mapActivationToShaderProgram(activation, shouldPackDepthwiseConv) : null;\n    const inputs = [input, filter];\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n\n    if (hasBias) {\n      inputs.push(bias);\n    }\n\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n\n    let program;\n\n    if (shouldPackDepthwiseConv) {\n      program = new DepthwiseConvPacked2DProgram(convInfo, hasBias, fusedActivation, hasPreluActivationWeights);\n      return this.compileAndRun(program, inputs);\n    }\n\n    program = new DepthwiseConv2DProgram(convInfo, hasBias, fusedActivation, hasPreluActivationWeights);\n    return this.compileAndRun(program, inputs);\n  }\n\n  depthwiseConv2D(x, filter, convInfo) {\n    let program;\n\n    if (env().getBool('WEBGL_PACK_DEPTHWISECONV') && convInfo.strideWidth <= 2 && convInfo.outChannels / convInfo.inChannels === 1) {\n      program = new DepthwiseConvPacked2DProgram(convInfo);\n      return this.compileAndRun(program, [x, filter]);\n    }\n\n    program = new DepthwiseConv2DProgram(convInfo);\n    return this.compileAndRun(program, [x, filter]);\n  }\n\n  depthwiseConv2DDerInput(dy, filter, convInfo) {\n    const program = new DepthwiseConv2DDerInputProgram(convInfo);\n    return this.compileAndRun(program, [dy, filter]);\n  }\n\n  depthwiseConv2DDerFilter(x, dy, convInfo) {\n    const program = new DepthwiseConv2DDerFilterProgram(convInfo);\n    return this.compileAndRun(program, [x, dy]);\n  }\n\n  conv3d(x, filter, convInfo) {\n    const program = new Conv3DProgram(convInfo);\n    return this.compileAndRun(program, [x, filter]);\n  }\n\n  conv3dDerInput(dy, filter, convInfo) {\n    const program = new Conv3DDerInputProgram(convInfo);\n    return this.compileAndRun(program, [dy, filter]);\n  }\n\n  conv3dDerFilter(x, dy, convInfo) {\n    const program = new Conv3DDerFilterProgram(convInfo);\n    return this.compileAndRun(program, [x, dy]);\n  }\n\n  maxPool(x, convInfo) {\n    const program = new Pool2DProgram(convInfo, 'max', false);\n    return this.compileAndRun(program, [x]);\n  }\n\n  avgPool(x, convInfo) {\n    const program = new Pool2DProgram(convInfo, 'avg', false);\n    return this.compileAndRun(program, [x], 'float32');\n  }\n\n  maxPoolBackprop(dy, x, y, convInfo) {\n    const getPositions = true;\n    const maxPoolPositionsProgram = new Pool2DProgram(convInfo, 'max', getPositions);\n    const maxPoolPositions = this.compileAndRun(maxPoolPositionsProgram, [x]);\n    const maxPoolBackPropProgram = new MaxPool2DBackpropProgram(convInfo);\n    const result = this.compileAndRun(maxPoolBackPropProgram, [dy, maxPoolPositions], x.dtype);\n    maxPoolPositions.dispose();\n    return result;\n  }\n\n  avgPoolBackprop(dy, x, convInfo) {\n    const avgPoolBackpropProgram = new AvgPool2DBackpropProgram(convInfo);\n    return this.compileAndRun(avgPoolBackpropProgram, [dy], x.dtype);\n  }\n\n  cast(x, dtype) {\n    return backend_util.castTensor(x, dtype, this);\n  }\n\n  unstack(x, axis) {\n    const num = x.shape[axis];\n    const outShape = new Array(x.rank - 1);\n    let outIndex = 0;\n\n    for (let i = 0; i < x.rank; i++) {\n      if (i !== axis) {\n        outShape[outIndex++] = x.shape[i];\n      }\n    }\n\n    const begin = new Array(x.rank).fill(0);\n    const size = x.shape.slice();\n    size[axis] = 1;\n    const res = new Array(num);\n\n    for (let i = 0; i < res.length; i++) {\n      begin[axis] = i;\n      res[i] = this.slice(x, begin, size).reshape(outShape);\n    }\n\n    return res;\n  }\n\n  avgPool3d(x, convInfo) {\n    const program = new Pool3DProgram(convInfo, 'avg', false);\n    return this.compileAndRun(program, [x], 'float32');\n  }\n\n  avgPool3dBackprop(dy, x, convInfo) {\n    const avgPool3dBackpropProgram = new AvgPool3DBackpropProgram(convInfo);\n    return this.compileAndRun(avgPool3dBackpropProgram, [dy], x.dtype);\n  }\n\n  maxPool3d(x, convInfo) {\n    const program = new Pool3DProgram(convInfo, 'max', false);\n    return this.compileAndRun(program, [x], 'float32');\n  }\n\n  maxPool3dBackprop(dy, x, y, convInfo) {\n    const getPositions = true;\n    const maxPool3dPositionsProgram = new Pool3DProgram(convInfo, 'max', getPositions);\n    const maxPool3dPositions = this.compileAndRun(maxPool3dPositionsProgram, [x]);\n    const maxPool3dBackPropProgram = new MaxPool3DBackpropProgram(convInfo);\n    const result = this.compileAndRun(maxPool3dBackPropProgram, [dy, maxPool3dPositions], x.dtype);\n    maxPool3dPositions.dispose();\n    return result;\n  }\n\n  reshape(x, shape) {\n    const texData = this.texData.get(x.dataId);\n\n    if (texData.isPacked && !webgl_util.isReshapeFree(x.shape, shape) && !(texData.texture !== null && webgl_util.isReshapeFree(texData.shape, shape))) {\n      const info = this.packedReshape(x, shape);\n      return engine().makeTensorFromDataId(info.dataId, info.shape, info.dtype);\n    }\n\n    return backend_util.reshapeTensor(x, shape);\n  }\n\n  resizeBilinear(x, newHeight, newWidth, alignCorners) {\n    const program = env().getBool('WEBGL_PACK_IMAGE_OPERATIONS') ? new ResizeBilinearPackedProgram(x.shape, newHeight, newWidth, alignCorners) : new ResizeBilinearProgram(x.shape, newHeight, newWidth, alignCorners);\n    return this.compileAndRun(program, [x], 'float32');\n  }\n\n  resizeBilinearBackprop(dy, x, alignCorners) {\n    const program = new ResizeBilinearBackpropProgram(dy, x, alignCorners);\n    return this.compileAndRun(program, [dy]);\n  }\n\n  resizeNearestNeighbor(x, newHeight, newWidth, alignCorners) {\n    const program = new ResizeNearestNeighborProgram(x.shape, newHeight, newWidth, alignCorners);\n    return this.compileAndRun(program, [x]);\n  }\n\n  resizeNearestNeighborBackprop(dy, x, alignCorners) {\n    const program = new ResizeNearestNeigborBackpropProgram(dy, x, alignCorners);\n    return this.compileAndRun(program, [dy]);\n  }\n\n  multinomial(logits, normalized, numSamples, seed) {\n    const probs = normalized ? logits : softmax(logits);\n    const batchSize = probs.shape[0];\n    const numOutcomes = probs.shape[1];\n    const program = new MultinomialProgram(batchSize, numOutcomes, numSamples);\n    const customSetup = program.getCustomSetupFunc(seed);\n    return this.compileAndRun(program, [probs], 'int32', customSetup);\n  }\n\n  oneHot(indices, depth, onValue, offValue) {\n    const program = new OneHotProgram(indices.size, depth, onValue, offValue);\n    return this.compileAndRun(program, [indices]);\n  }\n\n  diag(x) {\n    const program = new DiagProgram(x.size);\n    return this.compileAndRun(program, [x]);\n  }\n\n  cropAndResize(image, boxes, boxIndex, cropSize, method, extrapolationValue) {\n    const program = new CropAndResizeProgram(image.shape, boxes.shape, cropSize, method, extrapolationValue);\n    return this.compileAndRun(program, [image, boxes, boxIndex], 'float32');\n  }\n\n  depthToSpace(x, blockSize, dataFormat) {\n    util.assert(blockSize > 1, () => \"blockSize should be > 1 for depthToSpace, but was: \".concat(blockSize));\n    const batchSize = x.shape[0];\n    const inputHeight = dataFormat === 'NHWC' ? x.shape[1] : x.shape[2];\n    const inputWidth = dataFormat === 'NHWC' ? x.shape[2] : x.shape[3];\n    const inputDepth = dataFormat === 'NHWC' ? x.shape[3] : x.shape[1];\n    const outputHeight = inputHeight * blockSize;\n    const outputWidth = inputWidth * blockSize;\n    const outputDepth = inputDepth / (blockSize * blockSize);\n    const outputShape = dataFormat === 'NHWC' ? [batchSize, outputHeight, outputWidth, outputDepth] : [batchSize, outputDepth, outputHeight, outputWidth];\n    const program = new DepthToSpaceProgram(outputShape, blockSize, dataFormat);\n    return this.compileAndRun(program, [x]);\n  }\n\n  split(x, sizeSplits, axis) {\n    return split(x, sizeSplits, axis);\n  }\n\n  scatterND(indices, updates, shape) {\n    const _backend_util$calcula = backend_util.calculateShapes(updates, indices, shape),\n          sliceRank = _backend_util$calcula.sliceRank,\n          numUpdates = _backend_util$calcula.numUpdates,\n          sliceSize = _backend_util$calcula.sliceSize,\n          strides = _backend_util$calcula.strides,\n          outputSize = _backend_util$calcula.outputSize;\n\n    const flattenShape = [outputSize / sliceSize, sliceSize];\n    const flattenIndices = indices.reshape([numUpdates, sliceRank]);\n    const flattenX = updates.reshape([numUpdates, sliceSize]);\n\n    if (outputSize === 0) {\n      return backend_util.reshapeTensor(tensor([]), shape);\n    }\n\n    const defaultValue = scalar(0);\n    const program = new ScatterProgram(numUpdates, sliceRank, flattenIndices.rank, flattenX.rank, strides, flattenShape);\n    const res = this.compileAndRun(program, [flattenX, flattenIndices, defaultValue]);\n    return res.reshape(shape);\n  }\n\n  sparseToDense(sparseIndices, sparseValues, outputShape, defaultValue) {\n    const _backend_util$calcula2 = backend_util.calculateShapes(sparseValues, sparseIndices, outputShape),\n          sliceRank = _backend_util$calcula2.sliceRank,\n          numUpdates = _backend_util$calcula2.numUpdates,\n          strides = _backend_util$calcula2.strides,\n          outputSize = _backend_util$calcula2.outputSize;\n\n    const sumDupeIndices = false;\n    const program = new ScatterProgram(numUpdates, sliceRank, sparseIndices.rank, sparseValues.rank, strides, [outputSize, 1], sumDupeIndices);\n    const res = this.compileAndRun(program, [sparseValues, sparseIndices, defaultValue]);\n    return res.reshape(outputShape);\n  }\n\n  fft(x) {\n    const inverse = false;\n    return this.fftImpl(x, inverse);\n  }\n\n  ifft(x) {\n    const inverse = true;\n    return this.fftImpl(x, inverse);\n  }\n\n  fftImpl(x, inverse) {\n    const xData = this.texData.get(x.dataId);\n    const realProgram = new FFTProgram(fft_gpu.COMPLEX_FFT.REAL, x.shape, inverse);\n    const imagProgram = new FFTProgram(fft_gpu.COMPLEX_FFT.IMAG, x.shape, inverse);\n    const inputs = [this.makeComplexComponentTensorInfo(x, xData.complexTensors.real), this.makeComplexComponentTensorInfo(x, xData.complexTensors.imag)];\n    const real = this.compileAndRun(realProgram, inputs);\n    const imag = this.compileAndRun(imagProgram, inputs);\n    const complex = this.complex(real, imag).as2D(x.shape[0], x.shape[1]);\n    real.dispose();\n    imag.dispose();\n    return complex;\n  }\n\n  gatherND(x, indices) {\n    const indicesShape = indices.shape;\n    const sliceRank = indicesShape[indicesShape.length - 1];\n\n    const _backend_util$prepare = backend_util.prepareAndValidate(x, indices),\n          _backend_util$prepare2 = _slicedToArray(_backend_util$prepare, 4),\n          resultShape = _backend_util$prepare2[0],\n          numSlices = _backend_util$prepare2[1],\n          sliceSize = _backend_util$prepare2[2],\n          strides = _backend_util$prepare2[3];\n\n    const flattenIndices = indices.reshape([numSlices, sliceRank]);\n    const flattenX = x.reshape([x.size / sliceSize, sliceSize]);\n    const program = new GatherNDProgram(sliceRank, strides, [numSlices, sliceSize]);\n    const res = this.compileAndRun(program, [flattenX, flattenIndices]);\n    return res.reshape(resultShape);\n  }\n\n  fill(shape, value, dtype) {\n    dtype = dtype || util.inferDtype(value);\n\n    if (dtype === 'string') {\n      // String type should be handled in CPU memory.\n      const values = util.getArrayFromDType(dtype, util.sizeFromShape(shape));\n      values.fill(value);\n      return engine().makeTensor(values, shape, dtype, this);\n    } else {\n      const program = new FillProgram(shape, value);\n      const customSetup = program.getCustomSetupFunc(value);\n      return this.compileAndRun(program, [], dtype, customSetup);\n    }\n  }\n\n  onesLike(x) {\n    if (x.dtype === 'string') {\n      throw new Error('onesLike is not supported under string dtype');\n    } else {\n      // TODO(cais, smilkov): Add WebGL shader for onesLike:\n      //   https://github.com/tensorflow/tfjs/issues/1293\n      return this.fill(x.shape, 1, x.dtype);\n    }\n  }\n\n  zerosLike(x) {\n    return this.fill(x.shape, x.dtype === 'string' ? '' : 0, x.dtype);\n  }\n\n  linspace(start, stop, num) {\n    // TODO: Use CPU implementation due to the precision problem in Safari.\n    return backend_util.linspaceImpl(start, stop, num);\n  }\n\n  makeTensorInfo(shape, dtype) {\n    const dataId = this.write(null\n    /* values */\n    , shape, dtype);\n    this.texData.get(dataId).usage = null;\n    return {\n      dataId,\n      shape,\n      dtype\n    };\n  }\n\n  makeOutput(shape, dtype) {\n    const _this$makeTensorInfo = this.makeTensorInfo(shape, dtype),\n          dataId = _this$makeTensorInfo.dataId;\n\n    return engine().makeTensorFromDataId(dataId, shape, dtype, this);\n  }\n\n  unpackTensor(input) {\n    const program = new UnpackProgram(input.shape);\n    return this.runWebGLProgram(program, [input], input.dtype);\n  }\n\n  packTensor(input) {\n    const program = new PackProgram(input.shape);\n    const preventEagerUnpackingOutput = true;\n    return this.runWebGLProgram(program, [input], input.dtype, null\n    /* customSetup */\n    , preventEagerUnpackingOutput);\n  }\n\n  packedReshape(input, afterShape) {\n    const input3DShape = [webgl_util.getBatchDim(input.shape), ...webgl_util.getRowsCols(input.shape)];\n    const input3D = {\n      dtype: input.dtype,\n      shape: input3DShape,\n      dataId: input.dataId\n    };\n    const afterShapeAs3D = [webgl_util.getBatchDim(afterShape), ...webgl_util.getRowsCols(afterShape)];\n    const program = new ReshapePackedProgram(afterShapeAs3D, input3DShape);\n    const preventEagerUnpackingOfOutput = true;\n    const output = this.runWebGLProgram(program, [input3D], input.dtype, null\n    /* customSetup */\n    , preventEagerUnpackingOfOutput);\n    return {\n      dataId: output.dataId,\n      shape: afterShape,\n      dtype: output.dtype\n    };\n  }\n\n  decode(dataId) {\n    const texData = this.texData.get(dataId);\n    const isPacked = texData.isPacked,\n          shape = texData.shape,\n          dtype = texData.dtype;\n    const shapeAs3D = webgl_util.getShapeAs3D(shape);\n    let program;\n\n    if (isPacked) {\n      program = new DecodeMatrixPackedProgram(shapeAs3D);\n    } else {\n      program = new DecodeMatrixProgram(shapeAs3D);\n    }\n\n    const preventEagerUnpackingOfOutput = true;\n    const out = this.runWebGLProgram(program, [{\n      shape: shapeAs3D,\n      dtype,\n      dataId\n    }], dtype, null\n    /* customSetup */\n    , preventEagerUnpackingOfOutput);\n    return {\n      dtype,\n      shape,\n      dataId: out.dataId\n    };\n  }\n\n  runWebGLProgram(program, inputs, outputDtype, customSetup, preventEagerUnpackingOfOutput = false) {\n    const output = this.makeTensorInfo(program.outputShape, outputDtype);\n    const outData = this.texData.get(output.dataId);\n\n    if (program.packedOutput) {\n      outData.isPacked = true;\n    }\n\n    if (program.outPackingScheme === tex_util.PackingScheme.DENSE) {\n      const texelShape = tex_util.getDenseTexShape(program.outputShape); // For a densely packed output, we explicitly set texShape\n      // so it doesn't get assigned later according to our typical packing\n      // scheme wherein a single texel can only contain values from adjacent\n      // rows/cols.\n\n      outData.texShape = texelShape.map(d => d * 2);\n    }\n\n    if (program.outTexUsage != null) {\n      outData.usage = program.outTexUsage;\n    }\n\n    if (util.sizeFromShape(output.shape) === 0) {\n      // Short-circuit the computation since the result is empty (has 0 in its\n      // shape).\n      outData.values = util.getTypedArrayFromDType(output.dtype, 0);\n      return output;\n    }\n\n    const dataToDispose = [];\n    const inputsData = inputs.map(input => {\n      if (input.dtype === 'complex64') {\n        throw new Error(\"GPGPUProgram does not support complex64 input. For complex64 \" + \"dtypes, please separate the program into real and imaginary \" + \"parts.\");\n      }\n\n      let texData = this.texData.get(input.dataId);\n\n      if (texData.texture == null) {\n        if (!program.packedInputs && util.sizeFromShape(input.shape) <= env().getNumber('WEBGL_SIZE_UPLOAD_UNIFORM')) {\n          // Upload small tensors that live on the CPU as uniforms, not as\n          // textures. Do this only when the environment supports 32bit floats\n          // due to problems when comparing 16bit floats with 32bit floats.\n          // TODO(https://github.com/tensorflow/tfjs/issues/821): Make it\n          // possible for packed shaders to sample from uniforms.\n          return {\n            shape: input.shape,\n            texData: null,\n            isUniform: true,\n            uniformValues: texData.values\n          };\n        } // This ensures that if a packed program's inputs have not yet been\n        // uploaded to the GPU, they get uploaded as packed right off the bat.\n\n\n        if (program.packedInputs) {\n          texData.isPacked = true;\n          texData.shape = input.shape;\n        }\n      } else if (!!texData.isPacked !== !!program.packedInputs) {\n        input = texData.isPacked ? this.unpackTensor(input) : this.packTensor(input);\n        dataToDispose.push(input);\n        texData = this.texData.get(input.dataId);\n      } else if (texData.isPacked && !webgl_util.isReshapeFree(texData.shape, input.shape)) {\n        // This is a special case where a texture exists for a tensor\n        // but the shapes are incompatible (due to packing constraints) because\n        // the tensor did not have a chance to go through the packed reshape\n        // shader. This only happens when we reshape the *same* tensor to form\n        // *distinct* inputs to an op, e.g. dotting a vector with itself. This\n        // case will disappear once packed uploading is the default.\n        const savedInput = input;\n        const targetShape = input.shape;\n        input.shape = texData.shape;\n        input = this.packedReshape(input, targetShape);\n        dataToDispose.push(input);\n        texData = this.texData.get(input.dataId);\n        savedInput.shape = targetShape;\n      }\n\n      this.uploadToGPU(input.dataId);\n      return {\n        shape: input.shape,\n        texData,\n        isUniform: false\n      };\n    });\n    this.uploadToGPU(output.dataId);\n    const outputData = {\n      shape: output.shape,\n      texData: outData,\n      isUniform: false\n    };\n    const key = gpgpu_math.makeShaderKey(program, inputsData, outputData);\n    const binary = this.getAndSaveBinary(key, () => {\n      return gpgpu_math.compileProgram(this.gpgpu, program, inputsData, outputData);\n    });\n    const shouldTimeProgram = this.activeTimers != null;\n    let query;\n\n    if (shouldTimeProgram) {\n      query = this.startTimer();\n    }\n\n    gpgpu_math.runProgram(this.gpgpu, binary, inputsData, outputData, customSetup);\n    dataToDispose.forEach(info => this.disposeData(info.dataId));\n\n    if (shouldTimeProgram) {\n      query = this.endTimer(query);\n      this.activeTimers.push({\n        name: program.constructor.name,\n        query: this.getQueryTime(query)\n      });\n    }\n\n    if (!env().getBool('WEBGL_LAZILY_UNPACK') && outData.isPacked && preventEagerUnpackingOfOutput === false) {\n      const unpacked = this.unpackTensor(output);\n      this.disposeData(output.dataId);\n      return unpacked;\n    }\n\n    return output;\n  }\n\n  compileAndRun(program, inputs, outputDtype, customSetup, preventEagerUnpackingOfOutput = false) {\n    outputDtype = outputDtype || inputs[0].dtype;\n    const outInfo = this.runWebGLProgram(program, inputs, outputDtype, customSetup, preventEagerUnpackingOfOutput);\n    return engine().makeTensorFromDataId(outInfo.dataId, outInfo.shape, outInfo.dtype);\n  }\n\n  getAndSaveBinary(key, getBinary) {\n    if (!(key in this.binaryCache)) {\n      this.binaryCache[key] = getBinary();\n    }\n\n    return this.binaryCache[key];\n  }\n\n  getTextureManager() {\n    return this.textureManager;\n  }\n\n  dispose() {\n    if (this.disposed) {\n      return;\n    } // Avoid disposing the compiled webgl programs during unit testing because\n    // it slows down test execution.\n\n\n    if (!env().getBool('IS_TEST')) {\n      const allKeys = Object.keys(this.binaryCache);\n      allKeys.forEach(key => {\n        this.gpgpu.deleteProgram(this.binaryCache[key].webGLProgram);\n        delete this.binaryCache[key];\n      });\n    }\n\n    this.textureManager.dispose();\n\n    if (this.canvas != null && typeof HTMLCanvasElement !== 'undefined' && this.canvas instanceof HTMLCanvasElement) {\n      this.canvas.remove();\n    } else {\n      this.canvas = null;\n    }\n\n    if (this.gpgpuCreatedLocally) {\n      this.gpgpu.program = null;\n      this.gpgpu.dispose();\n    }\n\n    this.disposed = true;\n  }\n\n  floatPrecision() {\n    if (this.floatPrecisionValue == null) {\n      this.floatPrecisionValue = tidy(() => {\n        if (!env().get('WEBGL_RENDER_FLOAT32_ENABLED')) {\n          // Momentarily switching DEBUG flag to false so we don't throw an\n          // error trying to upload a small value.\n          const debugFlag = env().getBool('DEBUG');\n          env().set('DEBUG', false);\n          const underflowCheckValue = this.abs(scalar(1e-8)).dataSync()[0];\n          env().set('DEBUG', debugFlag);\n\n          if (underflowCheckValue > 0) {\n            return 32;\n          }\n        }\n\n        return 16;\n      });\n    }\n\n    return this.floatPrecisionValue;\n  }\n  /** Returns the smallest representable number.  */\n\n\n  epsilon() {\n    return this.floatPrecision() === 32 ? EPSILON_FLOAT32 : EPSILON_FLOAT16;\n  }\n\n  uploadToGPU(dataId) {\n    const texData = this.texData.get(dataId);\n    const shape = texData.shape,\n          dtype = texData.dtype,\n          values = texData.values,\n          texture = texData.texture,\n          usage = texData.usage,\n          isPacked = texData.isPacked;\n\n    if (texture != null) {\n      // Array is already on GPU. No-op.\n      return;\n    }\n\n    const shouldTimeProgram = this.activeTimers != null;\n    let start;\n\n    if (shouldTimeProgram) {\n      start = util.now();\n    }\n\n    let texShape = texData.texShape;\n\n    if (texShape == null) {\n      texShape = webgl_util.getTextureShapeFromLogicalShape(shape, isPacked);\n      texData.texShape = texShape;\n    }\n\n    if (values != null) {\n      const shapeAs3D = webgl_util.getShapeAs3D(shape);\n      let program;\n      let width = texShape[1],\n          height = texShape[0];\n      const isByteArray = values instanceof Uint8Array;\n\n      if (isPacked) {\n        var _tex_util$getPackedMa = tex_util.getPackedMatrixTextureShapeWidthHeight(texShape[0], texShape[1]);\n\n        var _tex_util$getPackedMa2 = _slicedToArray(_tex_util$getPackedMa, 2);\n\n        width = _tex_util$getPackedMa2[0];\n        height = _tex_util$getPackedMa2[1];\n        program = new EncodeMatrixPackedProgram(shapeAs3D, [height, width], isByteArray);\n      } else {\n        program = new EncodeMatrixProgram(shapeAs3D, [height, width], isByteArray);\n      }\n\n      const tempDenseInputHandle = this.makeTensorInfo([height, width], dtype);\n\n      if (isByteArray) {\n        this.texData.get(tempDenseInputHandle.dataId).usage = TextureUsage.PIXELS;\n      } else {\n        this.texData.get(tempDenseInputHandle.dataId).usage = TextureUsage.UPLOAD;\n      }\n\n      this.gpgpu.uploadDenseMatrixToTexture(this.getTexture(tempDenseInputHandle.dataId), width, height, values); // We want the output to remain packed regardless of the value of\n      // WEBGL_PACK.\n\n      const preventEagerUnpacking = true;\n      const encodedOutputTarget = this.runWebGLProgram(program, [tempDenseInputHandle], dtype, null, preventEagerUnpacking); // Have the original texture assume the identity of the encoded output.\n\n      const outputTexData = this.texData.get(encodedOutputTarget.dataId);\n      texData.texture = outputTexData.texture;\n      texData.texShape = outputTexData.texShape;\n      texData.isPacked = outputTexData.isPacked;\n      texData.usage = outputTexData.usage;\n      this.disposeData(tempDenseInputHandle.dataId);\n      this.texData.delete(encodedOutputTarget.dataId); // Once uploaded, don't store the values on cpu.\n\n      texData.values = null;\n\n      if (shouldTimeProgram) {\n        this.uploadWaitMs += util.now() - start;\n      }\n    } else {\n      const newTexture = this.acquireTexture(texShape, usage, dtype, isPacked);\n      texData.texture = newTexture;\n    }\n  }\n\n  convertAndCacheOnCPU(dataId, float32Values) {\n    const texData = this.texData.get(dataId);\n    const dtype = texData.dtype;\n    this.releaseGPUData(dataId);\n\n    if (float32Values != null) {\n      texData.values = float32ToTypedArray(float32Values, dtype);\n    }\n\n    return texData.values;\n  }\n\n  acquireTexture(texShape, texType, dtype, isPacked) {\n    this.numBytesInGPU += this.computeBytes(texShape, dtype);\n\n    if (!this.warnedAboutMemory && this.numBytesInGPU > this.numMBBeforeWarning * 1024 * 1024) {\n      const mb = (this.numBytesInGPU / 1024 / 1024).toFixed(2);\n      this.warnedAboutMemory = true;\n      console.warn(\"High memory usage in GPU: \".concat(mb, \" MB, \") + \"most likely due to a memory leak\");\n    }\n\n    return this.textureManager.acquireTexture(texShape, texType, isPacked);\n  }\n\n  computeBytes(shape, dtype) {\n    return shape[0] * shape[1] * util.bytesPerElement(dtype);\n  }\n\n}\n\nfunction float32ToTypedArray(a, dtype) {\n  if (dtype === 'float32' || dtype === 'complex64') {\n    return a;\n  } else if (dtype === 'int32' || dtype === 'bool') {\n    const result = dtype === 'int32' ? new Int32Array(a.length) : new Uint8Array(a.length);\n\n    for (let i = 0; i < result.length; ++i) {\n      result[i] = Math.round(a[i]);\n    }\n\n    return result;\n  } else {\n    throw new Error(\"Unknown dtype \".concat(dtype));\n  }\n}","map":null,"metadata":{},"sourceType":"module"}