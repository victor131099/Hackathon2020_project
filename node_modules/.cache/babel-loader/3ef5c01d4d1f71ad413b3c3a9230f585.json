{"ast":null,"code":"import _slicedToArray from \"/home/victor/COVID-19-Coding-Fest/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\n\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n  if (Array.isArray(inputs)) {\n    if (initialState != null || constants != null) {\n      throw new ValueError('When inputs is an array, neither initialState or constants ' + 'should be provided');\n    }\n\n    if (numConstants != null) {\n      constants = inputs.slice(inputs.length - numConstants, inputs.length);\n      inputs = inputs.slice(0, inputs.length - numConstants);\n    }\n\n    if (inputs.length > 1) {\n      initialState = inputs.slice(1, inputs.length);\n    }\n\n    inputs = inputs[0];\n  }\n\n  function toListOrNull(x) {\n    if (x == null || Array.isArray(x)) {\n      return x;\n    } else {\n      return [x];\n    }\n  }\n\n  initialState = toListOrNull(initialState);\n  constants = toListOrNull(constants);\n  return {\n    inputs,\n    initialState,\n    constants\n  };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\n\nexport function rnn(stepFunction, inputs, initialStates, goBackwards = false, mask, constants, unroll = false, needPerStepOutputs = false) {\n  return tfc.tidy(() => {\n    const ndim = inputs.shape.length;\n\n    if (ndim < 3) {\n      throw new ValueError(\"Input should be at least 3D, but is \".concat(ndim, \"D.\"));\n    } // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n    // ...].\n\n\n    const axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n\n    if (constants != null) {\n      throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' + 'constants yet.');\n    } // Porting Note: the unroll option is ignored by the imperative backend.\n\n\n    if (unroll) {\n      console.warn('Backend rnn(): the unroll = true option is not applicable to the ' + 'imperative deeplearn.js backend.');\n    }\n\n    if (mask != null) {\n      mask = mask.asType('bool').asType('float32');\n\n      if (mask.rank === ndim - 1) {\n        mask = tfc.expandDims(mask, -1);\n      }\n\n      mask = tfc.transpose(mask, axes);\n    }\n\n    if (goBackwards) {\n      inputs = tfc.reverse(inputs, 0);\n\n      if (mask != null) {\n        mask = tfc.reverse(mask, 0);\n      }\n    } // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n    //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n    //   use the usual TypeScript control flow to iterate over the time steps in\n    //   the inputs.\n    // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n    // outputs.\n    //   This is not idiomatic in TypeScript. The info regarding whether we are\n    //   in a learning (i.e., training) phase for RNN is passed in a different\n    //   way.\n\n\n    const perStepOutputs = [];\n    let lastOutput;\n    let states = initialStates;\n    const timeSteps = inputs.shape[0];\n    const perStepInputs = tfc.unstack(inputs);\n    let perStepMasks;\n\n    if (mask != null) {\n      perStepMasks = tfc.unstack(mask);\n    }\n\n    for (let t = 0; t < timeSteps; ++t) {\n      const currentInput = perStepInputs[t];\n      const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n\n      if (mask == null) {\n        lastOutput = stepOutputs[0];\n        states = stepOutputs[1];\n      } else {\n        const maskedOutputs = tfc.tidy(() => {\n          const stepMask = perStepMasks[t];\n          const negStepMask = tfc.onesLike(stepMask).sub(stepMask); // TODO(cais): Would tfc.where() be better for performance?\n\n          const output = stepOutputs[0].mul(stepMask).add(states[0].mul(negStepMask));\n          const newStates = states.map((state, i) => {\n            return stepOutputs[1][i].mul(stepMask).add(state.mul(negStepMask));\n          });\n          return {\n            output,\n            newStates\n          };\n        });\n        lastOutput = maskedOutputs.output;\n        states = maskedOutputs.newStates;\n      }\n\n      if (needPerStepOutputs) {\n        perStepOutputs.push(lastOutput);\n      }\n    }\n\n    let outputs;\n\n    if (needPerStepOutputs) {\n      const axis = 1;\n      outputs = tfc.stack(perStepOutputs, axis);\n    }\n\n    return [lastOutput, outputs, states];\n  });\n}\nexport class RNN extends Layer {\n  constructor(args) {\n    super(args);\n    let cell;\n\n    if (args.cell == null) {\n      throw new ValueError('cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(args.cell)) {\n      cell = new StackedRNNCells({\n        cells: args.cell\n      });\n    } else {\n      cell = args.cell;\n    }\n\n    if (cell.stateSize == null) {\n      throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' + 'integers, one integer per RNN state).');\n    }\n\n    this.cell = cell;\n    this.returnSequences = args.returnSequences == null ? false : args.returnSequences;\n    this.returnState = args.returnState == null ? false : args.returnState;\n    this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n    this._stateful = args.stateful == null ? false : args.stateful;\n    this.unroll = args.unroll == null ? false : args.unroll;\n    this.supportsMasking = true;\n    this.inputSpec = [new InputSpec({\n      ndim: 3\n    })];\n    this.stateSpec = null;\n    this.states_ = null; // TODO(cais): Add constantsSpec and numConstants.\n\n    this.numConstants = null; // TODO(cais): Look into the use of initial_state in the kwargs of the\n    //   constructor.\n\n    this.keptStates = [];\n  } // Porting Note: This is the equivalent of `RNN.states` property getter in\n  //   PyKeras.\n\n\n  getStates() {\n    if (this.states_ == null) {\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      return math_utils.range(0, numStates).map(x => null);\n    } else {\n      return this.states_;\n    }\n  } // Porting Note: This is the equivalent of the `RNN.states` property setter in\n  //   PyKeras.\n\n\n  setStates(states) {\n    this.states_ = states;\n  }\n\n  computeOutputShape(inputShape) {\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape; // TODO(cais): Remove the casting once stacked RNN cells become supported.\n\n    let stateSize = this.cell.stateSize;\n\n    if (!Array.isArray(stateSize)) {\n      stateSize = [stateSize];\n    }\n\n    const outputDim = stateSize[0];\n    let outputShape;\n\n    if (this.returnSequences) {\n      outputShape = [inputShape[0], inputShape[1], outputDim];\n    } else {\n      outputShape = [inputShape[0], outputDim];\n    }\n\n    if (this.returnState) {\n      const stateShape = [];\n\n      for (const dim of stateSize) {\n        stateShape.push([inputShape[0], dim]);\n      }\n\n      return [outputShape].concat(stateShape);\n    } else {\n      return outputShape;\n    }\n  }\n\n  computeMask(inputs, mask) {\n    return tfc.tidy(() => {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n\n      const outputMask = this.returnSequences ? mask : null;\n\n      if (this.returnState) {\n        const stateMask = this.states.map(s => null);\n        return [outputMask].concat(stateMask);\n      } else {\n        return outputMask;\n      }\n    });\n  }\n  /**\n   * Get the current state tensors of the RNN.\n   *\n   * If the state hasn't been set, return an array of `null`s of the correct\n   * length.\n   */\n\n\n  get states() {\n    if (this.states_ == null) {\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      const output = [];\n\n      for (let i = 0; i < numStates; ++i) {\n        output.push(null);\n      }\n\n      return output;\n    } else {\n      return this.states_;\n    }\n  }\n\n  set states(s) {\n    this.states_ = s;\n  }\n\n  build(inputShape) {\n    // Note inputShape will be an Array of Shapes of initial states and\n    // constants if these are passed in apply().\n    const constantShape = null;\n\n    if (this.numConstants != null) {\n      throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n    }\n\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    const batchSize = this.stateful ? inputShape[0] : null;\n    const inputDim = inputShape[inputShape.length - 1];\n    this.inputSpec[0] = new InputSpec({\n      shape: [batchSize, null, inputDim]\n    }); // Allow cell (if RNNCell Layer) to build before we set or validate\n    // stateSpec.\n\n    const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (constantShape != null) {\n      throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n    } else {\n      this.cell.build(stepInputShape);\n    } // Set or validate stateSpec.\n\n\n    let stateSize;\n\n    if (Array.isArray(this.cell.stateSize)) {\n      stateSize = this.cell.stateSize;\n    } else {\n      stateSize = [this.cell.stateSize];\n    }\n\n    if (this.stateSpec != null) {\n      if (!util.arraysEqual(this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]), stateSize)) {\n        throw new ValueError(\"An initialState was passed that is not compatible with \" + \"cell.stateSize. Received stateSpec=\".concat(this.stateSpec, \"; \") + \"However cell.stateSize is \".concat(this.cell.stateSize));\n      }\n    } else {\n      this.stateSpec = stateSize.map(dim => new InputSpec({\n        shape: [null, dim]\n      }));\n    }\n\n    if (this.stateful) {\n      this.resetStates();\n    }\n  }\n  /**\n   * Reset the state tensors of the RNN.\n   *\n   * If the `states` argument is `undefined` or `null`, will set the\n   * state tensor(s) of the RNN to all-zero tensors of the appropriate\n   * shape(s).\n   *\n   * If `states` is provided, will set the state tensors of the RNN to its\n   * value.\n   *\n   * @param states Optional externally-provided initial states.\n   * @param training Whether this call is done during training. For stateful\n   *   RNNs, this affects whether the old states are kept or discarded. In\n   *   particular, if `training` is `true`, the old states will be kept so\n   *   that subsequent backpropgataion through time (BPTT) may work properly.\n   *   Else, the old states will be discarded.\n   */\n\n\n  resetStates(states, training = false) {\n    tidy(() => {\n      if (!this.stateful) {\n        throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n      }\n\n      const batchSize = this.inputSpec[0].shape[0];\n\n      if (batchSize == null) {\n        throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' + 'the batch size of your input tensors: \\n' + '- If using a Sequential model, specify the batch size by ' + 'passing a `batchInputShape` option to your first layer.\\n' + '- If using the functional API, specify the batch size by ' + 'passing a `batchShape` option to your Input layer.');\n      } // Initialize state if null.\n\n\n      if (this.states_ == null) {\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ = this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n        }\n      } else if (states == null) {\n        // Dispose old state tensors.\n        tfc.dispose(this.states_); // For stateful RNNs, fully dispose kept old states.\n\n        if (this.keptStates != null) {\n          tfc.dispose(this.keptStates);\n          this.keptStates = [];\n        }\n\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ = this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n        }\n      } else {\n        if (!Array.isArray(states)) {\n          states = [states];\n        }\n\n        if (states.length !== this.states_.length) {\n          throw new ValueError(\"Layer \".concat(this.name, \" expects \").concat(this.states_.length, \" state(s), \") + \"but it received \".concat(states.length, \" state value(s). Input \") + \"received: \".concat(states));\n        }\n\n        if (training === true) {\n          // Store old state tensors for complete disposal later, i.e., during\n          // the next no-arg call to this method. We do not dispose the old\n          // states immediately because that BPTT (among other things) require\n          // them.\n          this.keptStates.push(this.states_.slice());\n        } else {\n          tfc.dispose(this.states_);\n        }\n\n        for (let index = 0; index < this.states_.length; ++index) {\n          const value = states[index];\n          const dim = Array.isArray(this.cell.stateSize) ? this.cell.stateSize[index] : this.cell.stateSize;\n          const expectedShape = [batchSize, dim];\n\n          if (!util.arraysEqual(value.shape, expectedShape)) {\n            throw new ValueError(\"State \".concat(index, \" is incompatible with layer \").concat(this.name, \": \") + \"expected shape=\".concat(expectedShape, \", received shape=\").concat(value.shape));\n          }\n\n          this.states_[index] = value;\n        }\n      }\n\n      this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n    });\n  }\n\n  apply(inputs, kwargs) {\n    // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants; // If any of `initial_state` or `constants` are specified and are\n    // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n    // the input_spec to include them.\n\n    let additionalInputs = [];\n    let additionalSpecs = [];\n\n    if (initialState != null) {\n      kwargs['initialState'] = initialState;\n      additionalInputs = additionalInputs.concat(initialState);\n      this.stateSpec = [];\n\n      for (const state of initialState) {\n        this.stateSpec.push(new InputSpec({\n          shape: state.shape\n        }));\n      } // TODO(cais): Use the following instead.\n      // this.stateSpec = initialState.map(state => new InputSpec({shape:\n      // state.shape}));\n\n\n      additionalSpecs = additionalSpecs.concat(this.stateSpec);\n    }\n\n    if (constants != null) {\n      kwargs['constants'] = constants;\n      additionalInputs = additionalInputs.concat(constants); // TODO(cais): Add this.constantsSpec.\n\n      this.numConstants = constants.length;\n    }\n\n    const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n    if (isTensor) {\n      // Compute full input spec, including state and constants.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call with temporarily replaced inputSpec.\n\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  } // tslint:disable-next-line:no-any\n\n\n  call(inputs, kwargs) {\n    // Input shape: `[samples, time (padded with zeros), input_dim]`.\n    // Note that the .build() method of subclasses **must** define\n    // this.inputSpec and this.stateSpec owith complete input shapes.\n    return tidy(() => {\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      let initialState = kwargs == null ? null : kwargs['initialState'];\n      inputs = getExactlyOneTensor(inputs);\n\n      if (initialState == null) {\n        if (this.stateful) {\n          initialState = this.states_;\n        } else {\n          initialState = this.getInitialState(inputs);\n        }\n      }\n\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n\n      if (initialState.length !== numStates) {\n        throw new ValueError(\"RNN Layer has \".concat(numStates, \" state(s) but was passed \") + \"\".concat(initialState.length, \" initial state(s).\"));\n      }\n\n      if (this.unroll) {\n        console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n      }\n\n      const cellCallKwargs = {\n        training\n      }; // TODO(cais): Add support for constants.\n\n      const step = (inputs, states) => {\n        // `inputs` and `states` are concatenated to form a single `Array` of\n        // `tf.Tensor`s as the input to `cell.call()`.\n        const outputs = this.cell.call([inputs].concat(states), cellCallKwargs); // Marshall the return value into output and new states.\n\n        return [outputs[0], outputs.slice(1)];\n      }; // TODO(cais): Add support for constants.\n\n\n      const rnnOutputs = rnn(step, inputs, initialState, this.goBackwards, mask, null, this.unroll, this.returnSequences);\n      const lastOutput = rnnOutputs[0];\n      const outputs = rnnOutputs[1];\n      const states = rnnOutputs[2];\n\n      if (this.stateful) {\n        this.resetStates(states, training);\n      }\n\n      const output = this.returnSequences ? outputs : lastOutput; // TODO(cais): Porperty set learning phase flag.\n\n      if (this.returnState) {\n        return [output].concat(states);\n      } else {\n        return output;\n      }\n    });\n  }\n\n  getInitialState(inputs) {\n    return tidy(() => {\n      // Build an all-zero tensor of shape [samples, outputDim].\n      // [Samples, timeSteps, inputDim].\n      let initialState = tfc.zeros(inputs.shape); // [Samples].\n\n      initialState = tfc.sum(initialState, [1, 2]);\n      initialState = K.expandDims(initialState); // [Samples, 1].\n\n      if (Array.isArray(this.cell.stateSize)) {\n        return this.cell.stateSize.map(dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n      } else {\n        return this.cell.stateSize > 1 ? [K.tile(initialState, [1, this.cell.stateSize])] : [initialState];\n      }\n    });\n  }\n\n  get trainableWeights() {\n    if (!this.trainable) {\n      return [];\n    } // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n\n\n    return this.cell.trainableWeights;\n  }\n\n  get nonTrainableWeights() {\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    if (!this.trainable) {\n      return this.cell.weights;\n    }\n\n    return this.cell.nonTrainableWeights;\n  }\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.cell != null) {\n      this.cell.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig() {\n    const config = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll\n    };\n\n    if (this.numConstants != null) {\n      config['numConstants'] = this.numConstants;\n    }\n\n    const cellConfig = this.cell.getConfig();\n    config['cell'] = {\n      'className': this.cell.getClassName(),\n      'config': cellConfig\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config, customObjects = {}) {\n    const cellConfig = config['cell'];\n    const cell = deserialize(cellConfig, customObjects);\n    return new cls(Object.assign(config, {\n      cell\n    }));\n  }\n\n}\n/** @nocollapse */\n\nRNN.className = 'RNN';\nserialization.registerClass(RNN);\n/**\n * An RNNCell layer.\n */\n// Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n\n/** @doc {heading: 'Layers', subheading: 'Classes'} */\n\nexport class RNNCell extends Layer {}\nexport class SimpleRNNCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    this.units = args.units;\n    assertPositiveInteger(this.units, \"units\");\n    this.activation = getActivation(args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape); // TODO(cais): Use regularizer.\n\n    this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n\n    this.built = true;\n  } // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n  //   `inputs` and `states`. Here, the two tensors are combined into an\n  //   `Tensor[]` Array as the first input argument.\n  //   Similarly, PyKeras' equivalent of this method returns two values:\n  //    `output` and `[output]`. Here the two are combined into one length-2\n  //    `Tensor[]`, consisting of `output` repeated.\n\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n\n      if (inputs.length !== 2) {\n        throw new ValueError(\"SimpleRNNCell expects 2 input Tensors, got \".concat(inputs.length, \".\"));\n      }\n\n      let prevOutput = inputs[1];\n      inputs = inputs[0];\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask(() => tfc.onesLike(inputs), this.dropout, training);\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask(() => tfc.onesLike(prevOutput), this.recurrentDropout, training);\n      }\n\n      let h;\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n\n      if (dpMask != null) {\n        h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n      } else {\n        h = K.dot(inputs, this.kernel.read());\n      }\n\n      if (this.bias != null) {\n        h = K.biasAdd(h, this.bias.read());\n      }\n\n      if (recDpMask != null) {\n        prevOutput = tfc.mul(prevOutput, recDpMask);\n      }\n\n      let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n\n      if (this.activation != null) {\n        output = this.activation.apply(output);\n      } // TODO(cais): Properly set learning phase on output tensor?\n\n\n      return [output, output];\n    });\n  }\n\n  getConfig() {\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport class SimpleRNN extends RNN {\n  constructor(args) {\n    args.cell = new SimpleRNNCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  } // TODO(cais): Research possibility of refactoring out the tedious all\n  //   the getters that delegate to `this.cell` below.\n\n\n  get units() {\n    return this.cell.units;\n  }\n\n  get activation() {\n    return this.cell.activation;\n  }\n\n  get useBias() {\n    return this.cell.useBias;\n  }\n\n  get kernelInitializer() {\n    return this.cell.kernelInitializer;\n  }\n\n  get recurrentInitializer() {\n    return this.cell.recurrentInitializer;\n  }\n\n  get biasInitializer() {\n    return this.cell.biasInitializer;\n  }\n\n  get kernelRegularizer() {\n    return this.cell.kernelRegularizer;\n  }\n\n  get recurrentRegularizer() {\n    return this.cell.recurrentRegularizer;\n  }\n\n  get biasRegularizer() {\n    return this.cell.biasRegularizer;\n  }\n\n  get kernelConstraint() {\n    return this.cell.kernelConstraint;\n  }\n\n  get recurrentConstraint() {\n    return this.cell.recurrentConstraint;\n  }\n\n  get biasConstraint() {\n    return this.cell.biasConstraint;\n  }\n\n  get dropout() {\n    return this.cell.dropout;\n  }\n\n  get recurrentDropout() {\n    return this.cell.recurrentDropout;\n  }\n\n  getConfig() {\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n    const baseConfig = super.getConfig();\n    delete baseConfig['cell'];\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport class GRUCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n\n    if (args.resetAfter) {\n      throw new ValueError(\"GRUCell does not support reset_after parameter set to true.\");\n    }\n\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION : args.activation);\n    this.recurrentActivation = getActivation(args.recurrentActivation === undefined ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.implementation = args.implementation;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    } // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n\n\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n\n      if (inputs.length !== 2) {\n        throw new ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \" + \"\".concat(inputs.length, \".\"));\n      }\n\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      let hTMinus1 = inputs[1]; // Previous memory state.\n\n      inputs = inputs[0]; // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2, regardless of the actual value of\n      // config.implementation.\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask(() => tfc.onesLike(inputs), this.dropout, training, 3);\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask(() => tfc.onesLike(hTMinus1), this.recurrentDropout, training, 3);\n      }\n\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n      let z;\n      let r;\n      let hh;\n\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n\n      let matrixX = K.dot(inputs, this.kernel.read());\n\n      if (this.useBias) {\n        matrixX = K.biasAdd(matrixX, this.bias.read());\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n\n      const recurrentKernelValue = this.recurrentKernel.read();\n\n      const _tfc$split = tfc.split(recurrentKernelValue, [2 * this.units, this.units], recurrentKernelValue.rank - 1),\n            _tfc$split2 = _slicedToArray(_tfc$split, 2),\n            rk1 = _tfc$split2[0],\n            rk2 = _tfc$split2[1];\n\n      const matrixInner = K.dot(hTMinus1, rk1);\n\n      const _tfc$split3 = tfc.split(matrixX, 3, matrixX.rank - 1),\n            _tfc$split4 = _slicedToArray(_tfc$split3, 3),\n            xZ = _tfc$split4[0],\n            xR = _tfc$split4[1],\n            xH = _tfc$split4[2];\n\n      const _tfc$split5 = tfc.split(matrixInner, 2, matrixInner.rank - 1),\n            _tfc$split6 = _slicedToArray(_tfc$split5, 2),\n            recurrentZ = _tfc$split6[0],\n            recurrentR = _tfc$split6[1];\n\n      z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n      r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n      const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n      hh = this.activation.apply(tfc.add(xH, recurrentH));\n      const h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh)); // TODO(cais): Add use_learning_phase flag properly.\n\n      return [h, h];\n    });\n  }\n\n  getConfig() {\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: false\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport class GRU extends RNN {\n  constructor(args) {\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    args.cell = new GRUCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n\n  get units() {\n    return this.cell.units;\n  }\n\n  get activation() {\n    return this.cell.activation;\n  }\n\n  get recurrentActivation() {\n    return this.cell.recurrentActivation;\n  }\n\n  get useBias() {\n    return this.cell.useBias;\n  }\n\n  get kernelInitializer() {\n    return this.cell.kernelInitializer;\n  }\n\n  get recurrentInitializer() {\n    return this.cell.recurrentInitializer;\n  }\n\n  get biasInitializer() {\n    return this.cell.biasInitializer;\n  }\n\n  get kernelRegularizer() {\n    return this.cell.kernelRegularizer;\n  }\n\n  get recurrentRegularizer() {\n    return this.cell.recurrentRegularizer;\n  }\n\n  get biasRegularizer() {\n    return this.cell.biasRegularizer;\n  }\n\n  get kernelConstraint() {\n    return this.cell.kernelConstraint;\n  }\n\n  get recurrentConstraint() {\n    return this.cell.recurrentConstraint;\n  }\n\n  get biasConstraint() {\n    return this.cell.biasConstraint;\n  }\n\n  get dropout() {\n    return this.cell.dropout;\n  }\n\n  get recurrentDropout() {\n    return this.cell.recurrentDropout;\n  }\n\n  get implementation() {\n    return this.cell.implementation;\n  }\n\n  getConfig() {\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: false\n    };\n    const baseConfig = super.getConfig();\n    delete baseConfig['cell'];\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport class LSTMCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION : args.activation);\n    this.recurrentActivation = getActivation(args.recurrentActivation === undefined ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.unitForgetBias = args.unitForgetBias;\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.implementation = args.implementation;\n    this.stateSize = [this.units, this.units];\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    var _a;\n\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n    let biasInitializer;\n\n    if (this.useBias) {\n      if (this.unitForgetBias) {\n        const capturedBiasInit = this.biasInitializer;\n        const capturedUnits = this.units;\n        biasInitializer = new (_a = class CustomInit extends Initializer {\n          apply(shape, dtype) {\n            // TODO(cais): More informative variable names?\n            const bI = capturedBiasInit.apply([capturedUnits]);\n            const bF = new Ones().apply([capturedUnits]);\n            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n            return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n          }\n\n        },\n        /** @nocollapse */\n        _a.className = 'CustomInit', _a)();\n      } else {\n        biasInitializer = this.biasInitializer;\n      }\n\n      this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    } // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n\n\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      inputs = inputs;\n\n      if (inputs.length !== 3) {\n        throw new ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \" + \"\".concat(inputs.length, \".\"));\n      }\n\n      let hTMinus1 = inputs[1]; // Previous memory state.\n\n      const cTMinus1 = inputs[2]; // Previous carry state.\n\n      inputs = inputs[0];\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask(() => tfc.onesLike(inputs), this.dropout, training, 4);\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask(() => tfc.onesLike(hTMinus1), this.recurrentDropout, training, 4);\n      }\n\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask; // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2 regardless of the actual value of\n      // config.implementation.\n\n      let i;\n      let f;\n      let c;\n      let o;\n\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n\n      let z = K.dot(inputs, this.kernel.read());\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n\n      z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n\n      if (this.useBias) {\n        z = K.biasAdd(z, this.bias.read());\n      }\n\n      const _tfc$split7 = tfc.split(z, 4, z.rank - 1),\n            _tfc$split8 = _slicedToArray(_tfc$split7, 4),\n            z0 = _tfc$split8[0],\n            z1 = _tfc$split8[1],\n            z2 = _tfc$split8[2],\n            z3 = _tfc$split8[3];\n\n      i = this.recurrentActivation.apply(z0);\n      f = this.recurrentActivation.apply(z1);\n      c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n      o = this.recurrentActivation.apply(z3);\n      const h = tfc.mul(o, this.activation.apply(c)); // TODO(cais): Add use_learning_phase flag properly.\n\n      return [h, h, c];\n    });\n  }\n\n  getConfig() {\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport class LSTM extends RNN {\n  constructor(args) {\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    args.cell = new LSTMCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n\n  get units() {\n    return this.cell.units;\n  }\n\n  get activation() {\n    return this.cell.activation;\n  }\n\n  get recurrentActivation() {\n    return this.cell.recurrentActivation;\n  }\n\n  get useBias() {\n    return this.cell.useBias;\n  }\n\n  get kernelInitializer() {\n    return this.cell.kernelInitializer;\n  }\n\n  get recurrentInitializer() {\n    return this.cell.recurrentInitializer;\n  }\n\n  get biasInitializer() {\n    return this.cell.biasInitializer;\n  }\n\n  get unitForgetBias() {\n    return this.cell.unitForgetBias;\n  }\n\n  get kernelRegularizer() {\n    return this.cell.kernelRegularizer;\n  }\n\n  get recurrentRegularizer() {\n    return this.cell.recurrentRegularizer;\n  }\n\n  get biasRegularizer() {\n    return this.cell.biasRegularizer;\n  }\n\n  get kernelConstraint() {\n    return this.cell.kernelConstraint;\n  }\n\n  get recurrentConstraint() {\n    return this.cell.recurrentConstraint;\n  }\n\n  get biasConstraint() {\n    return this.cell.biasConstraint;\n  }\n\n  get dropout() {\n    return this.cell.dropout;\n  }\n\n  get recurrentDropout() {\n    return this.cell.recurrentDropout;\n  }\n\n  get implementation() {\n    return this.cell.implementation;\n  }\n\n  getConfig() {\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n    const baseConfig = super.getConfig();\n    delete baseConfig['cell'];\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport class StackedRNNCells extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.cells = args.cells;\n  }\n\n  get stateSize() {\n    // States are a flat list in reverse order of the cell stack.\n    // This allows perserving the requirement `stack.statesize[0] ===\n    // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n    // assuming one LSTM has states `[h, c]`.\n    const stateSize = [];\n\n    for (const cell of this.cells.slice().reverse()) {\n      if (Array.isArray(cell.stateSize)) {\n        stateSize.push(...cell.stateSize);\n      } else {\n        stateSize.push(cell.stateSize);\n      }\n    }\n\n    return stateSize;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n      let states = inputs.slice(1); // Recover per-cell states.\n\n      const nestedStates = [];\n\n      for (const cell of this.cells.slice().reverse()) {\n        if (Array.isArray(cell.stateSize)) {\n          nestedStates.push(states.splice(0, cell.stateSize.length));\n        } else {\n          nestedStates.push(states.splice(0, 1));\n        }\n      }\n\n      nestedStates.reverse(); // Call the cells in order and store the returned states.\n\n      const newNestedStates = [];\n      let callInputs;\n\n      for (let i = 0; i < this.cells.length; ++i) {\n        const cell = this.cells[i];\n        states = nestedStates[i]; // TODO(cais): Take care of constants.\n\n        if (i === 0) {\n          callInputs = [inputs[0]].concat(states);\n        } else {\n          callInputs = [callInputs[0]].concat(states);\n        }\n\n        callInputs = cell.call(callInputs, kwargs);\n        newNestedStates.push(callInputs.slice(1));\n      } // Format the new states as a flat list in reverse cell order.\n\n\n      states = [];\n\n      for (const cellStates of newNestedStates.slice().reverse()) {\n        states.push(...cellStates);\n      }\n\n      return [callInputs[0]].concat(states);\n    });\n  }\n\n  build(inputShape) {\n    if (isArrayOfShapes(inputShape)) {\n      // TODO(cais): Take care of input constants.\n      // const constantShape = inputShape.slice(1);\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    let outputDim;\n    this.cells.forEach((cell, i) => {\n      nameScope(\"RNNCell_\".concat(i), () => {\n        // TODO(cais): Take care of input constants.\n        cell.build(inputShape);\n\n        if (Array.isArray(cell.stateSize)) {\n          outputDim = cell.stateSize[0];\n        } else {\n          outputDim = cell.stateSize;\n        }\n\n        inputShape = [inputShape[0], outputDim];\n      });\n    });\n    this.built = true;\n  }\n\n  getConfig() {\n    const cellConfigs = [];\n\n    for (const cell of this.cells) {\n      cellConfigs.push({\n        'className': cell.getClassName(),\n        'config': cell.getConfig()\n      });\n    }\n\n    const config = {\n      'cells': cellConfigs\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config, customObjects = {}) {\n    const cells = [];\n\n    for (const cellConfig of config['cells']) {\n      cells.push(deserialize(cellConfig, customObjects));\n    }\n\n    return new cls({\n      cells\n    });\n  }\n\n  get trainableWeights() {\n    if (!this.trainable) {\n      return [];\n    }\n\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.trainableWeights);\n    }\n\n    return weights;\n  }\n\n  get nonTrainableWeights() {\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.nonTrainableWeights);\n    }\n\n    if (!this.trainable) {\n      const trainableWeights = [];\n\n      for (const cell of this.cells) {\n        trainableWeights.push(...cell.trainableWeights);\n      }\n\n      return trainableWeights.concat(weights);\n    }\n\n    return weights;\n  }\n  /**\n   * Retrieve the weights of a the model.\n   *\n   * @returns A flat `Array` of `tf.Tensor`s.\n   */\n\n\n  getWeights() {\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.weights);\n    }\n\n    return batchGetValue(weights);\n  }\n  /**\n   * Set the weights of the model.\n   *\n   * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n   *     the output of `getWeights()`.\n   */\n\n\n  setWeights(weights) {\n    const tuples = [];\n\n    for (const cell of this.cells) {\n      const numParams = cell.weights.length;\n      const inputWeights = weights.splice(numParams);\n\n      for (let i = 0; i < cell.weights.length; ++i) {\n        tuples.push([cell.weights[i], inputWeights[i]]);\n      }\n    }\n\n    batchSetValue(tuples);\n  }\n\n}\n/** @nocollapse */\n\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\n\nfunction generateDropoutMask(ones, rate, training = null, count = 1) {\n  function droppedInputs() {\n    return K.dropout(ones(), rate);\n  }\n\n  if (count > 1) {\n    const mask = [];\n\n    for (let i = 0; i < count; i++) {\n      mask.push(K.inTrainPhase(droppedInputs, ones, training));\n    }\n\n    return mask.map(m => tfc.keep(m.clone()));\n  } else {\n    return tfc.keep(K.inTrainPhase(droppedInputs, ones, training).clone());\n  }\n}","map":null,"metadata":{},"sourceType":"module"}