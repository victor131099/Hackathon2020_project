{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\n\nexport class Wrapper extends Layer {\n  constructor(args) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  build(inputShape) {\n    this.built = true;\n  } // TODO(cais): Implement activityRegularizer getter.\n\n\n  get trainable() {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  get trainableWeights() {\n    return this.layer.trainableWeights;\n  } // TODO(cais): Implement setter for trainableWeights.\n\n\n  get nonTrainableWeights() {\n    return this.layer.nonTrainableWeights;\n  } // TODO(cais): Implement setter for nonTrainableWeights.\n\n\n  get updates() {\n    // tslint:disable-next-line:no-any\n    return this.layer._updates;\n  } // TODO(cais): Implement getUpdatesFor().\n\n\n  get losses() {\n    return this.layer.losses;\n  } // TODO(cais): Implement getLossesFor().\n\n\n  getWeights() {\n    return this.layer.getWeights();\n  }\n\n  setWeights(weights) {\n    this.layer.setWeights(weights);\n  }\n\n  getConfig() {\n    const config = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig()\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config, customObjects = {}) {\n    const layerConfig = config['layer'];\n    const layer = deserialize(layerConfig, customObjects);\n    delete config['layer'];\n    const newConfig = {\n      layer\n    };\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n\n}\nexport class TimeDistributed extends Wrapper {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n\n    if (inputShape.length < 3) {\n      throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" + \"input shape \".concat(JSON.stringify(inputShape)));\n    }\n\n    this.inputSpec = [{\n      shape: inputShape\n    }];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n\n    super.build(inputShape);\n  }\n\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape = this.layer.computeOutputShape(childInputShape);\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs); // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n\n      const step = (inputs, states) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n\n      const rnnOutputs = rnn(step, inputs, [], false\n      /* goBackwards */\n      , null\n      /* mask */\n      , null\n      /* constants */\n      , false\n      /* unroll */\n      , true\n      /* needPerStepOutputs */\n      );\n      const y = rnnOutputs[1]; // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n\n      return y;\n    });\n  }\n\n}\n/** @nocollapse */\n\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n  constructor(args) {\n    super(args); // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n\n    const layerConfig = args.layer.getConfig();\n    const forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    const backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict);\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n    this.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  get trainable() {\n    return this._trainable;\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  getWeights() {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }\n\n  setWeights(weights) {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  computeOutputShape(inputShape) {\n    let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes];\n    }\n\n    layerShapes = layerShapes;\n    let outputShape;\n    let outputShapes;\n    let stateShape;\n\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n\n    outputShape = outputShape;\n\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  apply(inputs, kwargs) {\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = inputs.slice(1);\n      inputs = inputs[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) && constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n\n    const additionalInputs = [];\n    const additionalSpecs = [];\n\n    if (initialState != null) {\n      const numStates = initialState.length;\n\n      if (numStates % 2 > 0) {\n        throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n      }\n\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = initialState.map(state => new InputSpec({\n        shape: state.shape\n      }));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n\n    if (constants != null) {\n      throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n      let y;\n      let yRev;\n\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: forwardState\n        }));\n        yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: backwardState\n        }));\n      }\n\n      let states;\n\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat(yRev.slice(1));\n        } else {}\n\n        y = y[0];\n        yRev = yRev[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev, 1);\n      }\n\n      let output;\n\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y, yRev]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y, yRev);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y, yRev));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y, yRev);\n      } else if (this.mergeMode == null) {\n        output = [y, yRev];\n      } // TODO(cais): Properly set learning phase.\n\n\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return output.concat(states);\n        }\n\n        return [output].concat(states);\n      }\n\n      return output;\n    });\n  }\n\n  resetStates(states) {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  build(inputShape) {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  computeMask(inputs, mask) {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n\n    let outputMask;\n\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask = states.map(state => null);\n\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  get trainableWeights() {\n    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights() {\n    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n  } // TODO(cais): Implement constraints().\n\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig() {\n    const config = {\n      'mergeMode': this.mergeMode\n    }; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    const rnnLayer = deserialize(config['layer']);\n    delete config['layer']; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants \" + \"present is not supported yet.\");\n    } // tslint:disable-next-line:no-any\n\n\n    const newConfig = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n\n}\n/** @nocollapse */\n\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":null,"metadata":{},"sourceType":"module"}