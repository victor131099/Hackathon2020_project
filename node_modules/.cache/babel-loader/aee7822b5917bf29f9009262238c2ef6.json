{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source: keras/engine/topology.py */\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getNextUniqueTensorId, getUid } from '../backend/state';\nimport { getScopedTensorName, getUniqueTensorName, nameScope } from '../common';\nimport { AttributeError, NotImplementedError, RuntimeError, ValueError } from '../errors';\nimport { getInitializer } from '../initializers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as types_utils from '../utils/types_utils';\nimport * as variable_utils from '../utils/variable_utils';\nimport { batchGetValue, batchSetValue, LayerVariable } from '../variables';\n/**\n * Specifies the ndim, dtype and shape of every input to a layer.\n *\n * Every layer should expose (if appropriate) an `inputSpec` attribute:\n * a list of instances of InputSpec (one per input tensor).\n *\n * A null entry in a shape is compatible with any dimension,\n * a null shape is compatible with any shape.\n */\n\nexport class InputSpec {\n  constructor(args) {\n    this.dtype = args.dtype;\n    this.shape = args.shape;\n    /*\n      TODO(michaelterry): Could throw error if ndim and shape are both defined\n        (then backport).\n    */\n\n    if (args.shape != null) {\n      this.ndim = args.shape.length;\n    } else {\n      this.ndim = args.ndim;\n    }\n\n    this.maxNDim = args.maxNDim;\n    this.minNDim = args.minNDim;\n    this.axes = args.axes || {};\n  }\n\n}\n/**\n * `tf.SymbolicTensor` is a placeholder for a Tensor without any concrete value.\n *\n * They are most often encountered when building a graph of `Layer`s for a\n * a `tf.LayersModel` and the input data's shape, but not values are known.\n */\n\n/** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\nexport class SymbolicTensor {\n  /**\n   *\n   * @param dtype\n   * @param shape\n   * @param sourceLayer The Layer that produced this symbolic tensor.\n   * @param inputs The inputs passed to sourceLayer's __call__() method.\n   * @param nodeIndex\n   * @param tensorIndex\n   * @param callArgs The keyword arguments passed to the __call__() method.\n   * @param name\n   * @param outputTensorIndex The index of this tensor in the list of outputs\n   *   returned by apply().\n   */\n  constructor(dtype, shape, sourceLayer, inputs, callArgs, name, outputTensorIndex) {\n    this.dtype = dtype;\n    this.shape = shape;\n    this.sourceLayer = sourceLayer;\n    this.inputs = inputs;\n    this.callArgs = callArgs;\n    this.outputTensorIndex = outputTensorIndex;\n    this.id = getNextUniqueTensorId();\n\n    if (name != null) {\n      this.originalName = getScopedTensorName(name);\n      this.name = getUniqueTensorName(this.originalName);\n    }\n\n    this.rank = shape.length;\n  }\n\n}\nlet _nextNodeID = 0;\n/**\n * A `Node` describes the connectivity between two layers.\n *\n * Each time a layer is connected to some new input,\n * a node is added to `layer.inboundNodes`.\n *\n * Each time the output of a layer is used by another layer,\n * a node is added to `layer.outboundNodes`.\n *\n * `nodeIndices` and `tensorIndices` are basically fine-grained coordinates\n * describing the origin of the `inputTensors`, verifying the following:\n *\n * `inputTensors[i] ==\n * inboundLayers[i].inboundNodes[nodeIndices[i]].outputTensors[\n *   tensorIndices[i]]`\n *\n * A node from layer A to layer B is added to:\n *     A.outboundNodes\n *     B.inboundNodes\n */\n\nexport class Node {\n  constructor(args, // TODO(michaelterry): Define actual type for this.\n  callArgs) {\n    this.callArgs = callArgs;\n    this.id = _nextNodeID++;\n    /*\n      Layer instance (NOT a list).\n      this is the layer that takes a list of input tensors\n      and turns them into a list of output tensors.\n      the current node will be added to\n      the inboundNodes of outboundLayer.\n    */\n\n    this.outboundLayer = args.outboundLayer;\n    /*\n        The following 3 properties describe where\n        the input tensors come from: which layers,\n        and for each layer, which node and which\n        tensor output of each node.\n    */\n    // List of layer instances.\n\n    this.inboundLayers = args.inboundLayers; // List of integers, 1:1 mapping with inboundLayers.\n\n    this.nodeIndices = args.nodeIndices; // List of integers, 1:1 mapping with inboundLayers.\n\n    this.tensorIndices = args.tensorIndices;\n    /*\n        Following 2 properties:\n        tensor inputs and outputs of outboundLayer.\n    */\n    // List of tensors. 1:1 mapping with inboundLayers.\n\n    this.inputTensors = args.inputTensors; // List of tensors, created by outboundLayer.call().\n\n    this.outputTensors = args.outputTensors;\n    /*\n        Following 2 properties: input and output masks.\n        List of tensors, 1:1 mapping with inputTensor.\n    */\n\n    this.inputMasks = args.inputMasks; // List of tensors, created by outboundLayer.computeMask().\n\n    this.outputMasks = args.outputMasks; // Following 2 properties: input and output shapes.\n    // List of shape tuples, shapes of inputTensors.\n\n    this.inputShapes = args.inputShapes; // List of shape tuples, shapes of outputTensors.\n\n    this.outputShapes = args.outputShapes; // Add nodes to all layers involved.\n\n    for (const layer of args.inboundLayers) {\n      if (layer != null) {\n        layer.outboundNodes.push(this);\n      }\n    }\n\n    args.outboundLayer.inboundNodes.push(this);\n  }\n\n  getConfig() {\n    const inboundNames = [];\n\n    for (const layer of this.inboundLayers) {\n      if (layer != null) {\n        inboundNames.push(layer.name);\n      } else {\n        inboundNames.push(null);\n      }\n    }\n\n    return {\n      outboundLayer: this.outboundLayer ? this.outboundLayer.name : null,\n      inboundLayers: inboundNames,\n      nodeIndices: this.nodeIndices,\n      tensorIndices: this.tensorIndices\n    };\n  }\n\n}\nlet _nextLayerID = 0;\n/**\n * A layer is a grouping of operations and weights that can be composed to\n * create a `tf.LayersModel`.\n *\n * Layers are constructed by using the functions under the\n * [tf.layers](#Layers-Basic) namespace.\n */\n\n/** @doc {heading: 'Layers', subheading: 'Classes', namespace: 'layers'} */\n\nexport class Layer extends serialization.Serializable {\n  constructor(args = {}) {\n    super();\n    this._callHook = null;\n    this._addedWeightNames = []; // Porting Notes: PyKeras does not have this property in this base Layer\n    //   class. Instead lets Layer subclass set it dynamically and checks the\n    //   value with `hasattr`. In tfjs-layers, we let this be a member of this\n    //   base class.\n\n    this._stateful = false;\n    this.id = _nextLayerID++;\n    this.activityRegularizer = null;\n    this.inputSpec = null;\n    this.supportsMasking = false; // These properties will be set upon call of this.build()\n\n    this._trainableWeights = [];\n    this._nonTrainableWeights = [];\n    this._losses = [];\n    this._updates = [];\n    this._built = false;\n    /*\n      These lists will be filled via successive calls\n      to this.addInboundNode().\n     */\n\n    this.inboundNodes = [];\n    this.outboundNodes = [];\n    let name = args.name;\n\n    if (!name) {\n      const prefix = this.getClassName();\n      name = generic_utils.toSnakeCase(prefix) + '_' + getUid(prefix);\n    }\n\n    this.name = name;\n    this.trainable_ = args.trainable == null ? true : args.trainable;\n\n    if (args.inputShape != null || args.batchInputShape != null) {\n      /*\n        In this case we will later create an input layer\n        to insert before the current layer\n       */\n      let batchInputShape;\n\n      if (args.batchInputShape != null) {\n        batchInputShape = args.batchInputShape;\n      } else if (args.inputShape != null) {\n        let batchSize = null;\n\n        if (args.batchSize != null) {\n          batchSize = args.batchSize;\n        }\n\n        batchInputShape = [batchSize].concat(args.inputShape);\n      }\n\n      this.batchInputShape = batchInputShape; // Set dtype.\n\n      let dtype = args.dtype;\n\n      if (dtype == null) {\n        dtype = args.inputDType;\n      }\n\n      if (dtype == null) {\n        dtype = 'float32';\n      }\n\n      this.dtype = dtype;\n    }\n\n    if (args.weights != null) {\n      this.initialWeights = args.weights;\n    } else {\n      this.initialWeights = null;\n    } // The value of `_refCount` is initialized to null. When the layer is used\n    // in a symbolic way for the first time, it will be set to 1.\n\n\n    this._refCount = null;\n    this.fastWeightInitDuringBuild = false;\n  }\n  /**\n   * Converts a layer and its index to a unique (immutable type) name.\n   * This function is used internally with `this.containerNodes`.\n   * @param layer The layer.\n   * @param nodeIndex The layer's position (e.g. via enumerate) in a list of\n   *   nodes.\n   *\n   * @returns The unique name.\n   */\n\n\n  static nodeKey(layer, nodeIndex) {\n    return layer.name + '_ib-' + nodeIndex.toString();\n  }\n  /**\n   * Returns this.inboundNode at index nodeIndex.\n   *\n   * Porting note: This is a replacement for _get_node_attribute_at_index()\n   * @param nodeIndex\n   * @param attrName The name of the attribute related to request for this node.\n   */\n\n\n  getNodeAtIndex(nodeIndex, attrName) {\n    if (this.inboundNodes.length === 0) {\n      throw new RuntimeError('The layer has never been called ' + \"and thus has no defined \".concat(attrName, \".\"));\n    }\n\n    if (this.inboundNodes.length <= nodeIndex) {\n      throw new ValueError(\"Asked to get \".concat(attrName, \" at node \").concat(nodeIndex, \", \") + \"but the layer has only \".concat(this.inboundNodes.length, \" inbound nodes.\"));\n    }\n\n    return this.inboundNodes[nodeIndex];\n  }\n  /**\n   * Retrieves the input tensor(s) of a layer at a given node.\n   *\n   * @param nodeIndex Integer, index of the node from which to retrieve the\n   *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer\n   *   was called.\n   *\n   * @return A tensor (or list of tensors if the layer has multiple inputs).\n   */\n\n\n  getInputAt(nodeIndex) {\n    return generic_utils.singletonOrArray(this.getNodeAtIndex(nodeIndex, 'input').inputTensors);\n  }\n  /**\n   * Retrieves the output tensor(s) of a layer at a given node.\n   *\n   * @param nodeIndex Integer, index of the node from which to retrieve the\n   *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer\n   *   was called.\n   *\n   * @return A tensor (or list of tensors if the layer has multiple outputs).\n   */\n\n\n  getOutputAt(nodeIndex) {\n    return generic_utils.singletonOrArray(this.getNodeAtIndex(nodeIndex, 'output').outputTensors);\n  } // Properties\n\n  /**\n   * Retrieves the input tensor(s) of a layer.\n   *\n   * Only applicable if the layer has exactly one inbound node,\n   * i.e. if it is connected to one incoming layer.\n   *\n   * @return Input tensor or list of input tensors.\n   *\n   * @exception AttributeError if the layer is connected to more than one\n   *   incoming layers.\n   */\n\n\n  get input() {\n    if (this.inboundNodes.length > 1) {\n      throw new AttributeError(\"Layer \".concat(this.name) + ' has multiple inbound nodes, ' + 'hence the notion of \"layer input\" ' + 'is ill-defined. ' + 'Use `getInputAt(nodeIndex)` instead.');\n    } else if (this.inboundNodes.length === 0) {\n      throw new AttributeError(\"Layer \".concat(this.name) + ' is not connected, no input to return.');\n    }\n\n    return generic_utils.singletonOrArray(this.getNodeAtIndex(0, 'input').inputTensors);\n  }\n  /**\n   * Retrieves the output tensor(s) of a layer.\n   *\n   * Only applicable if the layer has exactly one inbound node,\n   * i.e. if it is connected to one incoming layer.\n   *\n   * @return Output tensor or list of output tensors.\n   *\n   * @exception AttributeError if the layer is connected to more than one\n   *   incoming layers.\n   */\n\n\n  get output() {\n    if (this.inboundNodes.length === 0) {\n      throw new AttributeError(\"Layer \".concat(this.name) + ' has no inbound nodes.');\n    }\n\n    if (this.inboundNodes.length > 1) {\n      throw new AttributeError(\"Layer \".concat(this.name) + ' has multiple inbound nodes, ' + 'hence the notion of \"layer output\" ' + 'is ill-defined. ' + 'Use `getOutputAt(nodeIndex)` instead.');\n    }\n\n    return generic_utils.singletonOrArray(this.getNodeAtIndex(0, 'output').outputTensors);\n  }\n\n  get losses() {\n    return this._losses;\n  }\n  /**\n   * Retrieves the Layer's current loss values.\n   *\n   * Used for regularizers during training.\n   */\n\n\n  calculateLosses() {\n    // Porting Node: This is an augmentation to Layer.loss in PyKeras.\n    //   In PyKeras, Layer.loss returns symbolic tensors. Here a concrete\n    //   Tensor (specifically Scalar) values are returned. This is due to the\n    //   imperative backend.\n    return this.losses.map(lossFn => lossFn());\n  }\n\n  get updates() {\n    return this._updates;\n  }\n\n  get built() {\n    return this._built;\n  }\n\n  set built(built) {\n    this._built = built;\n  }\n\n  get trainable() {\n    return this.trainable_;\n  }\n\n  set trainable(trainable) {\n    this._trainableWeights.forEach(w => w.trainable = trainable);\n\n    this.trainable_ = trainable;\n  }\n\n  get trainableWeights() {\n    if (this.trainable_) {\n      return this._trainableWeights.filter(w => w.trainable);\n    } else {\n      return [];\n    }\n  }\n\n  set trainableWeights(weights) {\n    this._trainableWeights = weights;\n  }\n\n  get nonTrainableWeights() {\n    if (this.trainable) {\n      return this._trainableWeights.filter(w => !w.trainable).concat(this._nonTrainableWeights);\n    } else {\n      return this._trainableWeights.concat(this._nonTrainableWeights);\n    }\n  }\n\n  set nonTrainableWeights(weights) {\n    this._nonTrainableWeights = weights;\n  }\n  /**\n   * The concatenation of the lists trainableWeights and nonTrainableWeights\n   * (in this order).\n   */\n\n\n  get weights() {\n    return this.trainableWeights.concat(this.nonTrainableWeights);\n  }\n\n  get stateful() {\n    return this._stateful;\n  }\n  /**\n   * Reset the states of the layer.\n   *\n   * This method of the base Layer class is essentially a no-op.\n   * Subclasses that are stateful (e.g., stateful RNNs) should override this\n   * method.\n   */\n\n\n  resetStates() {\n    if (!this.stateful) {\n      throw new Error('Cannot call the resetStates() method of a non-stateful Layer ' + 'object.');\n    }\n  }\n  /**\n   * Checks compatibility between the layer and provided inputs.\n   *\n   * This checks that the tensor(s) `input`\n   * verify the input assumptions of the layer\n   * (if any). If not, exceptions are raised.\n   *\n   * @param inputs Input tensor or list of input tensors.\n   *\n   * @exception ValueError in case of mismatch between\n   *   the provided inputs and the expectations of the layer.\n   */\n\n\n  assertInputCompatibility(inputs) {\n    inputs = generic_utils.toList(inputs);\n\n    if (this.inputSpec == null || this.inputSpec.length === 0) {\n      return;\n    }\n\n    const inputSpec = generic_utils.toList(this.inputSpec);\n\n    if (inputs.length !== inputSpec.length) {\n      throw new ValueError(\"Layer \".concat(this.name, \" expects \").concat(inputSpec.length, \" inputs, \") + \"but it received \".concat(inputs.length, \" input tensors. \") + \"Input received: \".concat(inputs));\n    }\n\n    for (let inputIndex = 0; inputIndex < inputs.length; inputIndex++) {\n      const x = inputs[inputIndex];\n      const spec = inputSpec[inputIndex];\n\n      if (spec == null) {\n        continue;\n      } // Check ndim.\n\n\n      const ndim = x.rank;\n\n      if (spec.ndim != null) {\n        if (ndim !== spec.ndim) {\n          throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \").concat(this.name, \": \") + \"expected ndim=\".concat(spec.ndim, \", found ndim=\").concat(ndim));\n        }\n      }\n\n      if (spec.maxNDim != null) {\n        if (ndim > spec.maxNDim) {\n          throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \").concat(this.name) + \": expected max_ndim=\".concat(spec.maxNDim, \", found ndim=\").concat(ndim));\n        }\n      }\n\n      if (spec.minNDim != null) {\n        if (ndim < spec.minNDim) {\n          throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \").concat(this.name) + \": expected min_ndim=\".concat(spec.minNDim, \", found ndim=\").concat(ndim, \".\"));\n        }\n      } // Check dtype.\n\n\n      if (spec.dtype != null) {\n        if (x.dtype !== spec.dtype) {\n          throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \").concat(this.name, \" \") + \": expected dtype=\".concat(spec.dtype, \", found dtype=\").concat(x.dtype, \".\"));\n        }\n      } // Check specific shape axes.\n\n\n      if (spec.axes) {\n        const xShape = x.shape;\n\n        for (const key in spec.axes) {\n          const axis = Number(key);\n          const value = spec.axes[key]; // Perform Python-style slicing in case axis < 0;\n          // TODO(cais): Use https://github.com/alvivi/typescript-underscore to\n          // ensure type safety through Underscore calls.\n\n          const xShapeAtAxis = axis >= 0 ? xShape[axis] : xShape[xShape.length + axis];\n\n          if (value != null && [value, null].indexOf(xShapeAtAxis) === -1) {\n            throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \") + \"\".concat(this.name, \": expected axis \").concat(axis, \" of input shape to \") + \"have value \".concat(value, \" but got shape \").concat(xShape, \".\"));\n          }\n        }\n      } // Check shape.\n\n\n      if (spec.shape != null) {\n        for (let i = 0; i < spec.shape.length; ++i) {\n          const specDim = spec.shape[i];\n          const dim = x.shape[i];\n\n          if (specDim != null && dim != null) {\n            if (specDim !== dim) {\n              throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \") + \"\".concat(this.name, \": expected shape=\").concat(spec.shape, \", \") + \"found shape=\".concat(x.shape, \".\"));\n            }\n          }\n        }\n      }\n    }\n  }\n  /**\n   * This is where the layer's logic lives.\n   *\n   * @param inputs Input tensor, or list/tuple of input tensors.\n   * @param kwargs Additional keyword arguments.\n   *\n   * @return A tensor or list/tuple of tensors.\n   */\n\n\n  call(inputs, kwargs) {\n    return inputs;\n  }\n\n  invokeCallHook(inputs, kwargs) {\n    if (this._callHook != null) {\n      this._callHook(inputs, kwargs);\n    }\n  }\n  /**\n   * Set call hook.\n   * This is currently used for testing only.\n   * @param callHook\n   */\n\n\n  setCallHook(callHook) {\n    this._callHook = callHook;\n  }\n  /**\n   * Clear call hook.\n   * This is currently used for testing only.\n   */\n\n\n  clearCallHook() {\n    this._callHook = null;\n  }\n  /**\n   * Builds or executes a `Layer's logic.\n   *\n   * When called with `tf.Tensor`(s), execute the `Layer`s computation and\n   * return Tensor(s). For example:\n   *\n   * ```js\n   * const denseLayer = tf.layers.dense({\n   *   units: 1,\n   *   kernelInitializer: 'zeros',\n   *   useBias: false\n   * });\n   *\n   * // Invoke the layer's apply() method with a `tf.Tensor` (with concrete\n   * // numeric values).\n   * const input = tf.ones([2, 2]);\n   * const output = denseLayer.apply(input);\n   *\n   * // The output's value is expected to be [[0], [0]], due to the fact that\n   * // the dense layer has a kernel initialized to all-zeros and does not have\n   * // a bias.\n   * output.print();\n   * ```\n   *\n   * When called with `tf.SymbolicTensor`(s), this will prepare the layer for\n   * future execution.  This entails internal book-keeping on shapes of\n   * expected Tensors, wiring layers together, and initializing weights.\n   *\n   * Calling `apply` with `tf.SymbolicTensor`s are typically used during the\n   * building of non-`tf.Sequential` models. For example:\n   *\n   * ```js\n   * const flattenLayer = tf.layers.flatten();\n   * const denseLayer = tf.layers.dense({units: 1});\n   *\n   * // Use tf.layers.input() to obtain a SymbolicTensor as input to apply().\n   * const input = tf.input({shape: [2, 2]});\n   * const output1 = flattenLayer.apply(input);\n   *\n   * // output1.shape is [null, 4]. The first dimension is the undetermined\n   * // batch size. The second dimension comes from flattening the [2, 2]\n   * // shape.\n   * console.log(JSON.stringify(output1.shape));\n   *\n   * // The output SymbolicTensor of the flatten layer can be used to call\n   * // the apply() of the dense layer:\n   * const output2 = denseLayer.apply(output1);\n   *\n   * // output2.shape is [null, 1]. The first dimension is the undetermined\n   * // batch size. The second dimension matches the number of units of the\n   * // dense layer.\n   * console.log(JSON.stringify(output2.shape));\n   *\n   * // The input and output and be used to construct a model that consists\n   * // of the flatten and dense layers.\n   * const model = tf.model({inputs: input, outputs: output2});\n   * ```\n   *\n   * @param inputs a `tf.Tensor` or `tf.SymbolicTensor` or an Array of them.\n   * @param kwargs Additional keyword arguments to be passed to `call()`.\n   *\n   * @return Output of the layer's `call` method.\n   *\n   * @exception ValueError error in case the layer is missing shape information\n   *   for its `build` call.\n   */\n  // Porting Note: This is a replacement for __call__() in Python.\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  apply(inputs, kwargs) {\n    kwargs = kwargs || {};\n    this.assertNotDisposed(); // Ensure inputs are all the same type.\n\n    const inputsList = generic_utils.toList(inputs);\n    let allAreSymbolic = true;\n\n    for (const input of inputsList) {\n      if (!(input instanceof SymbolicTensor)) {\n        allAreSymbolic = false;\n        break;\n      }\n    }\n\n    let noneAreSymbolic = true;\n\n    for (const input of inputsList) {\n      if (input instanceof SymbolicTensor) {\n        noneAreSymbolic = false;\n        break;\n      }\n    }\n\n    if (allAreSymbolic === noneAreSymbolic) {\n      throw new ValueError('Arguments to apply() must be all ' + 'SymbolicTensors or all Tensors');\n    } // TODO(michaelterry): nameScope() may not be necessary.\n\n\n    return nameScope(this.name, () => {\n      // Handle laying building (weight creating, input spec locking).\n      if (!this.built) {\n        /*\n          Throw exceptions in case the input is not compatible\n          with the inputSpec specified in the layer constructor.\n         */\n        this.assertInputCompatibility(inputs); // Collect input shapes to build layer.\n\n        const inputShapes = [];\n\n        for (const xElem of generic_utils.toList(inputs)) {\n          inputShapes.push(xElem.shape);\n        }\n\n        this.build(generic_utils.singletonOrArray(inputShapes));\n        this.built = true; // Load weights that were specified at layer instantiation.\n\n        if (this.initialWeights) {\n          this.setWeights(this.initialWeights);\n        }\n\n        if (this._refCount === null && noneAreSymbolic) {\n          // The first use of this layer is a non-symbolic call, set ref count\n          // to 1 so the Layer can be properly disposed if its dispose() method\n          // is called.\n          this._refCount = 1;\n        }\n      }\n      /*\n        Throw exceptions in case the input is not compatible\n        with the inputSpec set at build time.\n      */\n\n\n      this.assertInputCompatibility(inputs); // Handle mask propagation.\n      // TODO(michaelterry): Mask propagation not currently implemented.\n      // Actually call the layer, collecting output(s), mask(s), and shape(s).\n\n      if (noneAreSymbolic) {\n        let output = this.call(inputs, kwargs); // TODO(michaelterry): Compute the outputMask\n        // If the layer returns tensors from its inputs, unmodified,\n        // we copy them to avoid loss of tensor metadata.\n\n        const outputList = generic_utils.toList(output);\n        const outputListCopy = []; // TODO(michaelterry): This copying may not be necessary given our eager\n        // backend.\n\n        for (let x of outputList) {\n          if (inputsList.indexOf(x) !== -1) {\n            x = x.clone();\n          }\n\n          outputListCopy.push(x);\n        }\n\n        output = generic_utils.singletonOrArray(outputListCopy);\n\n        if (this.activityRegularizer != null) {\n          throw new NotImplementedError('Layer invocation in the presence of activity ' + 'regularizer(s) is not supported yet.');\n        } // TODO(michaelterry): Call addInboundNode()?\n\n\n        return output;\n      } else {\n        const inputShape = collectInputShape(inputs);\n        const outputShape = this.computeOutputShape(inputShape);\n        let output;\n        const outputDType = guessOutputDType(inputs);\n        this.warnOnIncompatibleInputShape(Array.isArray(inputs) ? inputShape[0] : inputShape);\n\n        if (outputShape != null && outputShape.length > 0 && Array.isArray(outputShape[0])) {\n          // We have multiple output shapes. Create multiple output tensors.\n          output = outputShape.map((shape, index) => new SymbolicTensor(outputDType, shape, this, generic_utils.toList(inputs), kwargs, this.name, index));\n        } else {\n          output = new SymbolicTensor(outputDType, outputShape, this, generic_utils.toList(inputs), kwargs, this.name);\n        }\n        /*\n          Add an inbound node to the layer, so that it keeps track\n          of the call and of all new variables created during the call.\n          This also updates the layer history of the output tensor(s).\n          If the input tensor(s) had no previous history,\n          this does nothing.\n        */\n\n\n        this.addInboundNode(inputs, output, null, null, inputShape, outputShape, kwargs);\n        this._refCount++;\n\n        if (this.activityRegularizer != null) {\n          throw new NotImplementedError('Layer invocation in the presence of activity ' + 'regularizer(s) is not supported yet.');\n        }\n\n        return output;\n      }\n    });\n  }\n  /**\n   * Check compatibility between input shape and this layer's batchInputShape.\n   *\n   * Print warning if any incompatibility is found.\n   *\n   * @param inputShape Input shape to be checked.\n   */\n\n\n  warnOnIncompatibleInputShape(inputShape) {\n    if (this.batchInputShape == null) {\n      return;\n    } else if (inputShape.length !== this.batchInputShape.length) {\n      console.warn(\"The rank of the input tensor provided (shape: \" + \"\".concat(JSON.stringify(inputShape), \") does not match that of the \") + \"batchInputShape (\".concat(JSON.stringify(this.batchInputShape), \") \") + \"of the layer \".concat(this.name));\n    } else {\n      let dimMismatch = false;\n      this.batchInputShape.forEach((dimension, i) => {\n        if (dimension != null && inputShape[i] != null && inputShape[i] !== dimension) {\n          dimMismatch = true;\n        }\n      });\n\n      if (dimMismatch) {\n        console.warn(\"The shape of the input tensor \" + \"(\".concat(JSON.stringify(inputShape), \") does not \") + \"match the expectation of layer \".concat(this.name, \": \") + \"\".concat(JSON.stringify(this.batchInputShape)));\n      }\n    }\n  }\n  /**\n   * Retrieves the output shape(s) of a layer.\n   *\n   * Only applicable if the layer has only one inbound node, or if all inbound\n   * nodes have the same output shape.\n   *\n   * @returns Output shape or shapes.\n   * @throws AttributeError: if the layer is connected to more than one incoming\n   *   nodes.\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  get outputShape() {\n    if (this.inboundNodes == null || this.inboundNodes.length === 0) {\n      throw new AttributeError(\"The layer \".concat(this.name, \" has never been called and thus has no \") + \"defined output shape.\");\n    }\n\n    const allOutputShapes = [];\n\n    for (const node of this.inboundNodes) {\n      const shapeString = JSON.stringify(node.outputShapes);\n\n      if (allOutputShapes.indexOf(shapeString) === -1) {\n        allOutputShapes.push(shapeString);\n      }\n    }\n\n    if (allOutputShapes.length === 1) {\n      const outputShapes = this.inboundNodes[0].outputShapes;\n\n      if (Array.isArray(outputShapes) && Array.isArray(outputShapes[0]) && outputShapes.length === 1) {\n        return outputShapes[0];\n      } else {\n        return outputShapes;\n      }\n    } else {\n      throw new AttributeError(\"The layer \".concat(this.name, \" has multiple inbound nodes with different \") + \"output shapes. Hence the notion of \\\"output shape\\\" is ill-defined \" + \"for the layer.\"); // TODO(cais): Implement getOutputShapeAt().\n    }\n  }\n  /**\n   * Counts the total number of numbers (e.g., float32, int32) in the\n   * weights.\n   *\n   * @returns An integer count.\n   * @throws RuntimeError: If the layer is not built yet (in which case its\n   *   weights are not defined yet.)\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  countParams() {\n    if (!this.built) {\n      throw new RuntimeError(\"You tried to call countParams() on \".concat(this.name, \", \") + \"but the layer is not built yet. Build it first by calling \" + \"build(batchInputShape).\");\n    }\n\n    return variable_utils.countParamsInWeights(this.weights);\n  }\n  /**\n   * Creates the layer weights.\n   *\n   * Must be implemented on all layers that have weights.\n   *\n   * Called when apply() is called to construct the weights.\n   *\n   * @param inputShape A `Shape` or array of `Shape` (unused).\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  build(inputShape) {\n    this.built = true;\n  }\n  /**\n   * Returns the current values of the weights of the layer.\n   *\n   * @param trainableOnly Whether to get the values of only trainable weights.\n   * @returns Weight values as an `Array` of `tf.Tensor`s.\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  getWeights(trainableOnly = false) {\n    return batchGetValue(trainableOnly ? this.trainableWeights : this.weights);\n  }\n  /**\n   * Sets the weights of the layer, from Tensors.\n   *\n   * @param weights a list of Tensors. The number of arrays and their shape\n   *   must match number of the dimensions of the weights of the layer (i.e.\n   *   it should match the output of `getWeights`).\n   *\n   * @exception ValueError If the provided weights list does not match the\n   *   layer's specifications.\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  setWeights(weights) {\n    tidy(() => {\n      const params = this.weights;\n\n      if (params.length !== weights.length) {\n        // TODO(cais): Restore the following and use `providedWeights`, instead\n        // of `weights` in the error message, once the deeplearn.js bug is\n        // fixed: https://github.com/PAIR-code/deeplearnjs/issues/498 const\n        // providedWeights = JSON.stringify(weights).substr(0, 50);\n        throw new ValueError(\"You called setWeights(weights) on layer \\\"\".concat(this.name, \"\\\" \") + \"with a weight list of length \".concat(weights.length, \", \") + \"but the layer was expecting \".concat(params.length, \" weights. \") + \"Provided weights: \".concat(weights, \"...\"));\n      }\n\n      if (params.length === 0) {\n        return;\n      }\n\n      const weightValueTuples = [];\n      const paramValues = batchGetValue(params);\n\n      for (let i = 0; i < paramValues.length; ++i) {\n        const pv = paramValues[i];\n        const p = params[i];\n        const w = weights[i];\n\n        if (!util.arraysEqual(pv.shape, w.shape)) {\n          throw new ValueError(\"Layer weight shape \".concat(pv.shape, \" \") + \"not compatible with provided weight shape \".concat(w.shape));\n        }\n\n        weightValueTuples.push([p, w]);\n      }\n\n      batchSetValue(weightValueTuples);\n    });\n  }\n  /**\n   * Adds a weight variable to the layer.\n   *\n   * @param name Name of the new weight variable.\n   * @param shape The shape of the weight.\n   * @param dtype The dtype of the weight.\n   * @param initializer An initializer instance.\n   * @param regularizer A regularizer instance.\n   * @param trainable Whether the weight should be trained via backprop or not\n   *   (assuming that the layer itself is also trainable).\n   * @param constraint An optional trainable.\n   * @return The created weight variable.\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  addWeight(name, shape, dtype, initializer, regularizer, trainable, constraint) {\n    // Reject duplicate weight names.\n    if (this._addedWeightNames.indexOf(name) !== -1) {\n      throw new ValueError(\"Duplicate weight name \".concat(name, \" for layer \").concat(this.name));\n    }\n\n    this._addedWeightNames.push(name);\n\n    if (dtype == null) {\n      dtype = 'float32';\n    }\n\n    if (this.fastWeightInitDuringBuild) {\n      initializer = getInitializer('zeros');\n    }\n\n    const initValue = initializer.apply(shape, dtype);\n    const weight = new LayerVariable(initValue, dtype, name, trainable, constraint);\n    initValue.dispose(); // Request backend not to dispose the weights of the model on scope() exit.\n\n    if (regularizer != null) {\n      this.addLoss(() => regularizer.apply(weight.read()));\n    }\n\n    if (trainable == null) {\n      trainable = true;\n    }\n\n    if (trainable) {\n      this._trainableWeights.push(weight);\n    } else {\n      this._nonTrainableWeights.push(weight);\n    }\n\n    return weight;\n  }\n  /**\n   * Set the fast-weight-initialization flag.\n   *\n   * In cases where the initialized weight values will be immediately\n   * overwritten by loaded weight values during model loading, setting\n   * the flag to `true` saves unnecessary calls to potentially expensive\n   * initializers and speeds up the loading process.\n   *\n   * @param value Target value of the flag.\n   */\n\n\n  setFastWeightInitDuringBuild(value) {\n    this.fastWeightInitDuringBuild = value;\n  }\n  /**\n   * Add losses to the layer.\n   *\n   * The loss may potentionally be conditional on some inputs tensors,\n   * for instance activity losses are conditional on the layer's inputs.\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  addLoss(losses) {\n    if (losses == null || Array.isArray(losses) && losses.length === 0) {\n      return;\n    } // Update this.losses\n\n\n    losses = generic_utils.toList(losses);\n\n    if (this._losses !== undefined && this._losses !== null) {\n      this.losses.push(...losses);\n    }\n  }\n  /**\n   * Computes the output shape of the layer.\n   *\n   * Assumes that the layer will be built to match that input shape provided.\n   *\n   * @param inputShape A shape (tuple of integers) or a list of shape tuples\n   *   (one per output tensor of the layer). Shape tuples can include null for\n   *   free dimensions, instead of an integer.\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  /**\n   * Computes an output mask tensor.\n   *\n   * @param inputs Tensor or list of tensors.\n   * @param mask Tensor or list of tensors.\n   *\n   * @return null or a tensor (or list of tensors, one per output tensor of the\n   * layer).\n   */\n\n\n  computeMask(inputs, mask) {\n    if (!this.supportsMasking) {\n      if (mask != null) {\n        if (Array.isArray(mask)) {\n          mask.forEach(maskElement => {\n            if (maskElement != null) {\n              throw new TypeError(\"Layer \".concat(this.name, \" does not support masking, \") + 'but was passed an inputMask.');\n            }\n          });\n        } else {\n          throw new TypeError(\"Layer \".concat(this.name, \" does not support masking, \") + 'but was passed an inputMask.');\n        }\n      } // masking not explicitly supported: return null as mask\n\n\n      return null;\n    } // if masking is explictly supported, by default\n    // carry over the input mask\n\n\n    return mask;\n  }\n  /**\n   * Internal method to create an inbound node for the layer.\n   *\n   * @param inputTensors List of input tensors.\n   * @param outputTensors List of output tensors.\n   * @param inputMasks List of input masks (a mask can be a tensor, or null).\n   * @param outputMasks List of output masks (a mask can be a tensor, or null).\n   * @param inputShapes List of input shape tuples.\n   * @param outputShapes List of output shape tuples.\n   * @param kwargs Dictionary of keyword arguments that were passed to the\n   *   `call` method of the layer at the call that created the node.\n   */\n\n\n  addInboundNode(inputTensors, outputTensors, inputMasks, outputMasks, inputShapes, outputShapes, kwargs = null) {\n    const inputTensorList = generic_utils.toList(inputTensors);\n    outputTensors = generic_utils.toList(outputTensors);\n    inputMasks = generic_utils.toList(inputMasks);\n    outputMasks = generic_utils.toList(outputMasks);\n    inputShapes = types_utils.normalizeShapeList(inputShapes);\n    outputShapes = types_utils.normalizeShapeList(outputShapes); // Collect input tensor(s) coordinates.\n\n    const inboundLayers = [];\n    const nodeIndices = [];\n    const tensorIndices = [];\n\n    for (const x of inputTensorList) {\n      /*\n       * TODO(michaelterry): Keras adds this value to tensors; it's not\n       * clear whether we'll use this or not.\n       */\n      inboundLayers.push(x.sourceLayer);\n      nodeIndices.push(x.nodeIndex);\n      tensorIndices.push(x.tensorIndex);\n    } // Create node, add it to inbound nodes.\n    // (This call has side effects.)\n    // tslint:disable-next-line:no-unused-expression\n\n\n    new Node({\n      outboundLayer: this,\n      inboundLayers,\n      nodeIndices,\n      tensorIndices,\n      inputTensors: inputTensorList,\n      outputTensors,\n      inputMasks,\n      outputMasks,\n      inputShapes,\n      outputShapes\n    }, kwargs); // Update tensor history\n\n    for (let i = 0; i < outputTensors.length; i++) {\n      // TODO(michaelterry: _uses_learning_phase not tracked.\n      outputTensors[i].sourceLayer = this;\n      outputTensors[i].nodeIndex = this.inboundNodes.length - 1;\n      outputTensors[i].tensorIndex = i;\n    }\n  }\n  /**\n   * Returns the config of the layer.\n   *\n   * A layer config is a TS dictionary (serializable)\n   * containing the configuration of a layer.\n   * The same layer can be reinstantiated later\n   * (without its trained weights) from this configuration.\n   *\n   * The config of a layer does not include connectivity\n   * information, nor the layer class name.  These are handled\n   * by 'Container' (one layer of abstraction above).\n   *\n   * Porting Note: The TS dictionary follows TS naming standrds for\n   * keys, and uses tfjs-layers type-safe Enums.  Serialization methods\n   * should use a helper function to convert to the pythonic storage\n   * standard. (see serialization_utils.convertTsToPythonic)\n   *\n   * @returns TS dictionary of configuration.\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  getConfig() {\n    const config = {\n      name: this.name,\n      trainable: this.trainable\n    };\n\n    if (this.batchInputShape != null) {\n      config['batchInputShape'] = this.batchInputShape;\n    }\n\n    if (this.dtype != null) {\n      config['dtype'] = this.dtype;\n    }\n\n    return config;\n  }\n  /**\n   * Dispose the weight variables that this Layer instance holds.\n   *\n   * @returns {number} Number of disposed variables.\n   */\n\n\n  disposeWeights() {\n    this.weights.forEach(weight => weight.dispose());\n    return this.weights.length;\n  }\n\n  assertNotDisposed() {\n    if (this._refCount === 0) {\n      throw new Error(\"Layer '\".concat(this.name, \"' is already disposed.\"));\n    }\n  }\n  /**\n   * Attempt to dispose layer's weights.\n   *\n   * This method decrease the reference count of the Layer object by 1.\n   *\n   * A Layer is reference-counted. Its reference count is incremented by 1\n   * the first item its `apply()` method is called and when it becomes a part\n   * of a new `Node` (through calling the `apply()`) method on a\n   * `tf.SymbolicTensor`).\n   *\n   * If the reference count of a Layer becomes 0, all the weights will be\n   * disposed and the underlying memory (e.g., the textures allocated in WebGL)\n   * will be freed.\n   *\n   * Note: If the reference count is greater than 0 after the decrement, the\n   * weights of the Layer will *not* be disposed.\n   *\n   * After a Layer is disposed, it cannot be used in calls such as `apply()`,\n   * `getWeights()` or `setWeights()` anymore.\n   *\n   * @returns A DisposeResult Object with the following fields:\n   *   - refCountAfterDispose: The reference count of the Container after this\n   *     `dispose()` call.\n   *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed\n   *     during this `dispose()` call.\n   * @throws {Error} If the layer is not built yet, or if the layer has already\n   *   been disposed.\n   */\n\n  /** @doc {heading: 'Models', 'subheading': 'Classes'} */\n\n\n  dispose() {\n    if (!this.built) {\n      throw new Error(\"Cannot dispose Layer \".concat(this.name, \" because it has not been \") + \"built yet.\");\n    }\n\n    if (this._refCount === null) {\n      throw new Error(\"Cannot dispose Layer \".concat(this.name, \" because it has not been used \") + \"yet.\");\n    }\n\n    this.assertNotDisposed();\n    let numDisposedVariables = 0;\n\n    if (--this._refCount === 0) {\n      numDisposedVariables = this.disposeWeights();\n    }\n\n    return {\n      refCountAfterDispose: this._refCount,\n      numDisposedVariables\n    };\n  }\n\n}\n/**\n * Collects the input shape(s) of a list of `tf.Tensor`s or\n * `tf.SymbolicTensor`s.\n *\n * TODO(michaelterry): Update PyKeras docs (backport).\n *\n * @param inputTensors List of input tensors (or single input tensor).\n *\n * @return List of shape tuples (or single tuple), one tuple per input.\n */\n\nfunction collectInputShape(inputTensors) {\n  inputTensors = generic_utils.toList(inputTensors);\n  const shapes = [];\n\n  for (const x of inputTensors) {\n    shapes.push(x.shape);\n  }\n\n  return generic_utils.singletonOrArray(shapes);\n}\n/**\n * Guesses output dtype based on inputs.\n *\n * At present, just returns 'float32' for any input.\n *\n * @param inputTensors List of input tensors (or single input tensor).\n *\n * @return The guessed DType. At present, always returns 'float32'.\n */\n\n\nfunction guessOutputDType(inputTensors) {\n  return 'float32';\n}\n/**\n * Returns the list of input tensors necessary to compute `tensor`.\n *\n * Output will always be a list of tensors (potentially with 1 element).\n *\n * @param tensor The tensor to start from.\n * @param layer Origin layer of the tensor.\n * @param nodeIndex Origin node index of the tensor.\n *\n * @return Array of input tensors.\n */\n\n\nexport function getSourceInputs(tensor, layer, nodeIndex) {\n  if (layer == null || nodeIndex != null && nodeIndex > 0) {\n    layer = tensor.sourceLayer;\n    nodeIndex = tensor.nodeIndex;\n  }\n\n  if (layer.inboundNodes.length === 0) {\n    return [tensor];\n  } else {\n    const node = layer.inboundNodes[nodeIndex];\n\n    if (node.inboundLayers.length === 0) {\n      return node.inputTensors;\n    } else {\n      const sourceTensors = [];\n\n      for (let i = 0; i < node.inboundLayers.length; i++) {\n        const x = node.inputTensors[i];\n        const layer = node.inboundLayers[i];\n        const nodeIndex = node.nodeIndices[i];\n        const previousSources = getSourceInputs(x, layer, nodeIndex); // Avoid input redundancy.\n\n        for (const x of previousSources) {\n          if (sourceTensors.indexOf(x) === -1) {\n            sourceTensors.push(x);\n          }\n        }\n      }\n\n      return sourceTensors;\n    }\n  }\n}","map":null,"metadata":{},"sourceType":"module"}