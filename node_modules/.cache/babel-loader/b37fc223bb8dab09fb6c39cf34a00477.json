{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport { eye, linalg, mul, ones, randomUniform, scalar, serialization, tidy, truncatedNormal, zeros } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { checkDataFormat } from './common';\nimport { NotImplementedError, ValueError } from './errors';\nimport { VALID_DISTRIBUTION_VALUES, VALID_FAN_MODE_VALUES } from './keras_format/initializer_config';\nimport { checkStringTypeUnionValue, deserializeKerasObject, serializeKerasObject } from './utils/generic_utils';\nimport { arrayProd } from './utils/math_utils';\nexport function checkFanMode(value) {\n  checkStringTypeUnionValue(VALID_FAN_MODE_VALUES, 'FanMode', value);\n}\nexport function checkDistribution(value) {\n  checkStringTypeUnionValue(VALID_DISTRIBUTION_VALUES, 'Distribution', value);\n}\n/**\n * Initializer base class.\n *\n * @doc {\n *   heading: 'Initializers', subheading: 'Classes', namespace: 'initializers'}\n */\n\nexport class Initializer extends serialization.Serializable {\n  fromConfigUsesCustomObjects() {\n    return false;\n  }\n\n  getConfig() {\n    return {};\n  }\n\n}\nexport class Zeros extends Initializer {\n  apply(shape, dtype) {\n    return zeros(shape, dtype);\n  }\n\n}\n/** @nocollapse */\n\nZeros.className = 'Zeros';\nserialization.registerClass(Zeros);\nexport class Ones extends Initializer {\n  apply(shape, dtype) {\n    return ones(shape, dtype);\n  }\n\n}\n/** @nocollapse */\n\nOnes.className = 'Ones';\nserialization.registerClass(Ones);\nexport class Constant extends Initializer {\n  constructor(args) {\n    super();\n\n    if (typeof args !== 'object') {\n      throw new ValueError(\"Expected argument of type ConstantConfig but got \".concat(args));\n    }\n\n    if (args.value === undefined) {\n      throw new ValueError(\"config must have value set but got \".concat(args));\n    }\n\n    this.value = args.value;\n  }\n\n  apply(shape, dtype) {\n    return tidy(() => mul(scalar(this.value), ones(shape, dtype)));\n  }\n\n  getConfig() {\n    return {\n      value: this.value\n    };\n  }\n\n}\n/** @nocollapse */\n\nConstant.className = 'Constant';\nserialization.registerClass(Constant);\nexport class RandomUniform extends Initializer {\n  constructor(args) {\n    super();\n    this.DEFAULT_MINVAL = -0.05;\n    this.DEFAULT_MAXVAL = 0.05;\n    this.minval = args.minval || this.DEFAULT_MINVAL;\n    this.maxval = args.maxval || this.DEFAULT_MAXVAL;\n    this.seed = args.seed;\n  }\n\n  apply(shape, dtype) {\n    return randomUniform(shape, this.minval, this.maxval, dtype);\n  }\n\n  getConfig() {\n    return {\n      minval: this.minval,\n      maxval: this.maxval,\n      seed: this.seed\n    };\n  }\n\n}\n/** @nocollapse */\n\nRandomUniform.className = 'RandomUniform';\nserialization.registerClass(RandomUniform);\nexport class RandomNormal extends Initializer {\n  constructor(args) {\n    super();\n    this.DEFAULT_MEAN = 0.;\n    this.DEFAULT_STDDEV = 0.05;\n    this.mean = args.mean || this.DEFAULT_MEAN;\n    this.stddev = args.stddev || this.DEFAULT_STDDEV;\n    this.seed = args.seed;\n  }\n\n  apply(shape, dtype) {\n    dtype = dtype || 'float32';\n\n    if (dtype !== 'float32' && dtype !== 'int32') {\n      throw new NotImplementedError(\"randomNormal does not support dType \".concat(dtype, \".\"));\n    }\n\n    return K.randomNormal(shape, this.mean, this.stddev, dtype, this.seed);\n  }\n\n  getConfig() {\n    return {\n      mean: this.mean,\n      stddev: this.stddev,\n      seed: this.seed\n    };\n  }\n\n}\n/** @nocollapse */\n\nRandomNormal.className = 'RandomNormal';\nserialization.registerClass(RandomNormal);\nexport class TruncatedNormal extends Initializer {\n  constructor(args) {\n    super();\n    this.DEFAULT_MEAN = 0.;\n    this.DEFAULT_STDDEV = 0.05;\n    this.mean = args.mean || this.DEFAULT_MEAN;\n    this.stddev = args.stddev || this.DEFAULT_STDDEV;\n    this.seed = args.seed;\n  }\n\n  apply(shape, dtype) {\n    dtype = dtype || 'float32';\n\n    if (dtype !== 'float32' && dtype !== 'int32') {\n      throw new NotImplementedError(\"truncatedNormal does not support dType \".concat(dtype, \".\"));\n    }\n\n    return truncatedNormal(shape, this.mean, this.stddev, dtype, this.seed);\n  }\n\n  getConfig() {\n    return {\n      mean: this.mean,\n      stddev: this.stddev,\n      seed: this.seed\n    };\n  }\n\n}\n/** @nocollapse */\n\nTruncatedNormal.className = 'TruncatedNormal';\nserialization.registerClass(TruncatedNormal);\nexport class Identity extends Initializer {\n  constructor(args) {\n    super();\n    this.gain = args.gain != null ? args.gain : 1.0;\n  }\n\n  apply(shape, dtype) {\n    return tidy(() => {\n      if (shape.length !== 2 || shape[0] !== shape[1]) {\n        throw new ValueError('Identity matrix initializer can only be used for' + ' 2D square matrices.');\n      } else {\n        return mul(this.gain, eye(shape[0]));\n      }\n    });\n  }\n\n  getConfig() {\n    return {\n      gain: this.gain\n    };\n  }\n\n}\n/** @nocollapse */\n\nIdentity.className = 'Identity';\nserialization.registerClass(Identity);\n/**\n * Computes the number of input and output units for a weight shape.\n * @param shape Shape of weight.\n * @param dataFormat data format to use for convolution kernels.\n *   Note that all kernels in Keras are standardized on the\n *   CHANNEL_LAST ordering (even when inputs are set to CHANNEL_FIRST).\n * @return An length-2 array: fanIn, fanOut.\n */\n\nfunction computeFans(shape, dataFormat = 'channelsLast') {\n  let fanIn;\n  let fanOut;\n  checkDataFormat(dataFormat);\n\n  if (shape.length === 2) {\n    fanIn = shape[0];\n    fanOut = shape[1];\n  } else if ([3, 4, 5].indexOf(shape.length) !== -1) {\n    if (dataFormat === 'channelsFirst') {\n      const receptiveFieldSize = arrayProd(shape, 2);\n      fanIn = shape[1] * receptiveFieldSize;\n      fanOut = shape[0] * receptiveFieldSize;\n    } else if (dataFormat === 'channelsLast') {\n      const receptiveFieldSize = arrayProd(shape, 0, shape.length - 2);\n      fanIn = shape[shape.length - 2] * receptiveFieldSize;\n      fanOut = shape[shape.length - 1] * receptiveFieldSize;\n    }\n  } else {\n    const shapeProd = arrayProd(shape);\n    fanIn = Math.sqrt(shapeProd);\n    fanOut = Math.sqrt(shapeProd);\n  }\n\n  return [fanIn, fanOut];\n}\n\nexport class VarianceScaling extends Initializer {\n  /**\n   * Constructor of VarianceScaling.\n   * @throws ValueError for invalid value in scale.\n   */\n  constructor(args) {\n    super();\n\n    if (args.scale < 0.0) {\n      throw new ValueError(\"scale must be a positive float. Got: \".concat(args.scale));\n    }\n\n    this.scale = args.scale == null ? 1.0 : args.scale;\n    this.mode = args.mode == null ? 'fanIn' : args.mode;\n    checkFanMode(this.mode);\n    this.distribution = args.distribution == null ? 'normal' : args.distribution;\n    checkDistribution(this.distribution);\n    this.seed = args.seed;\n  }\n\n  apply(shape, dtype) {\n    const fans = computeFans(shape);\n    const fanIn = fans[0];\n    const fanOut = fans[1];\n    let scale = this.scale;\n\n    if (this.mode === 'fanIn') {\n      scale /= Math.max(1, fanIn);\n    } else if (this.mode === 'fanOut') {\n      scale /= Math.max(1, fanOut);\n    } else {\n      scale /= Math.max(1, (fanIn + fanOut) / 2);\n    }\n\n    if (this.distribution === 'normal') {\n      const stddev = Math.sqrt(scale);\n      dtype = dtype || 'float32';\n\n      if (dtype !== 'float32' && dtype !== 'int32') {\n        throw new NotImplementedError(\"\".concat(this.getClassName(), \" does not support dType \").concat(dtype, \".\"));\n      }\n\n      return truncatedNormal(shape, 0, stddev, dtype, this.seed);\n    } else {\n      const limit = Math.sqrt(3 * scale);\n      return randomUniform(shape, -limit, limit, dtype);\n    }\n  }\n\n  getConfig() {\n    return {\n      scale: this.scale,\n      mode: this.mode,\n      distribution: this.distribution,\n      seed: this.seed\n    };\n  }\n\n}\n/** @nocollapse */\n\nVarianceScaling.className = 'VarianceScaling';\nserialization.registerClass(VarianceScaling);\nexport class GlorotUniform extends VarianceScaling {\n  /**\n   * Constructor of GlorotUniform\n   * @param scale\n   * @param mode\n   * @param distribution\n   * @param seed\n   */\n  constructor(args) {\n    super({\n      scale: 1.0,\n      mode: 'fanAvg',\n      distribution: 'uniform',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  getClassName() {\n    // In Python Keras, GlorotUniform is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n\n}\n/** @nocollapse */\n\nGlorotUniform.className = 'GlorotUniform';\nserialization.registerClass(GlorotUniform);\nexport class GlorotNormal extends VarianceScaling {\n  /**\n   * Constructor of GlorotNormal.\n   * @param scale\n   * @param mode\n   * @param distribution\n   * @param seed\n   */\n  constructor(args) {\n    super({\n      scale: 1.0,\n      mode: 'fanAvg',\n      distribution: 'normal',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  getClassName() {\n    // In Python Keras, GlorotNormal is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n\n}\n/** @nocollapse */\n\nGlorotNormal.className = 'GlorotNormal';\nserialization.registerClass(GlorotNormal);\nexport class HeNormal extends VarianceScaling {\n  constructor(args) {\n    super({\n      scale: 2.0,\n      mode: 'fanIn',\n      distribution: 'normal',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  getClassName() {\n    // In Python Keras, HeNormal is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n\n}\n/** @nocollapse */\n\nHeNormal.className = 'HeNormal';\nserialization.registerClass(HeNormal);\nexport class HeUniform extends VarianceScaling {\n  constructor(args) {\n    super({\n      scale: 2.0,\n      mode: 'fanIn',\n      distribution: 'uniform',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  getClassName() {\n    // In Python Keras, HeUniform is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n\n}\n/** @nocollapse */\n\nHeUniform.className = 'HeUniform';\nserialization.registerClass(HeUniform);\nexport class LeCunNormal extends VarianceScaling {\n  constructor(args) {\n    super({\n      scale: 1.0,\n      mode: 'fanIn',\n      distribution: 'normal',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  getClassName() {\n    // In Python Keras, LeCunNormal is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n\n}\n/** @nocollapse */\n\nLeCunNormal.className = 'LeCunNormal';\nserialization.registerClass(LeCunNormal);\nexport class LeCunUniform extends VarianceScaling {\n  constructor(args) {\n    super({\n      scale: 1.0,\n      mode: 'fanIn',\n      distribution: 'uniform',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  getClassName() {\n    // In Python Keras, LeCunUniform is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n\n}\n/** @nocollapse */\n\nLeCunUniform.className = 'LeCunNormal';\nserialization.registerClass(LeCunUniform);\nexport class Orthogonal extends Initializer {\n  constructor(args) {\n    super();\n    this.DEFAULT_GAIN = 1;\n    this.gain = args.gain == null ? this.DEFAULT_GAIN : args.gain;\n    this.seed = args.seed;\n\n    if (this.seed != null) {\n      throw new NotImplementedError('Random seed is not implemented for Orthogonal Initializer yet.');\n    }\n  }\n\n  apply(shape, dtype) {\n    return tidy(() => {\n      if (shape.length !== 2) {\n        throw new NotImplementedError('The Orthogonal Initializer does not support non-2D shapes yet.');\n      }\n\n      if (shape[0] * shape[1] > 2000) {\n        console.warn(\"Orthogonal initializer is being called on a matrix with more \" + \"than 2000 (\".concat(shape[0] * shape[1], \") elements: \") + \"Slowness may result.\");\n      } // TODO(cais): Add seed support.\n\n\n      const normalizedShape = shape[0] > shape[1] ? [shape[1], shape[0]] : shape;\n      const a = K.randomNormal(normalizedShape, 0, 1, 'float32');\n      let q = linalg.gramSchmidt(a);\n\n      if (shape[0] > shape[1]) {\n        q = q.transpose();\n      }\n\n      return mul(this.gain, q);\n    });\n  }\n\n  getConfig() {\n    return {\n      gain: this.gain,\n      seed: this.seed\n    };\n  }\n\n}\n/** @nocollapse */\n\nOrthogonal.className = 'Orthogonal';\nserialization.registerClass(Orthogonal); // Maps the JavaScript-like identifier keys to the corresponding registry\n// symbols.\n\nexport const INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = {\n  'constant': 'Constant',\n  'glorotNormal': 'GlorotNormal',\n  'glorotUniform': 'GlorotUniform',\n  'heNormal': 'HeNormal',\n  'heUniform': 'HeUniform',\n  'identity': 'Identity',\n  'leCunNormal': 'LeCunNormal',\n  'leCunUniform': 'LeCunUniform',\n  'ones': 'Ones',\n  'orthogonal': 'Orthogonal',\n  'randomNormal': 'RandomNormal',\n  'randomUniform': 'RandomUniform',\n  'truncatedNormal': 'TruncatedNormal',\n  'varianceScaling': 'VarianceScaling',\n  'zeros': 'Zeros'\n};\n\nfunction deserializeInitializer(config, customObjects = {}) {\n  return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'initializer');\n}\n\nexport function serializeInitializer(initializer) {\n  return serializeKerasObject(initializer);\n}\nexport function getInitializer(identifier) {\n  if (typeof identifier === 'string') {\n    const className = identifier in INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[identifier] : identifier;\n    /* We have four 'helper' classes for common initializers that\n    all get serialized as 'VarianceScaling' and shouldn't go through\n    the deserializeInitializer pathway. */\n\n    if (className === 'GlorotNormal') {\n      return new GlorotNormal();\n    } else if (className === 'GlorotUniform') {\n      return new GlorotUniform();\n    } else if (className === 'HeNormal') {\n      return new HeNormal();\n    } else if (className === 'HeUniform') {\n      return new HeUniform();\n    } else if (className === 'LeCunNormal') {\n      return new LeCunNormal();\n    } else if (className === 'LeCunUniform') {\n      return new LeCunUniform();\n    } else {\n      const config = {};\n      config['className'] = className;\n      config['config'] = {};\n      return deserializeInitializer(config);\n    }\n  } else if (identifier instanceof Initializer) {\n    return identifier;\n  } else {\n    return deserializeInitializer(identifier);\n  }\n}","map":null,"metadata":{},"sourceType":"module"}