{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Merge Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { l2Normalize } from '../losses';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as mathUtils from '../utils/math_utils';\nimport { getExactlyOneShape } from '../utils/types_utils';\n/**\n * Generic Merge layer for element-wise merge functions.\n *\n * Used to implement `Sum`, `Average`, `Concatenate`, etc.\n */\n\nexport class Merge extends Layer {\n  constructor(args) {\n    super(args || {});\n    this.supportsMasking = true;\n  }\n  /**\n   * Logic for merging multiple tensors, to be overridden by subclasses.\n   * @param inputs\n   */\n\n\n  mergeFunction(inputs) {\n    throw new NotImplementedError();\n  }\n  /**\n   * Computes the shape of the result of an elementwise operation.\n   *\n   * @param shape1: Shape of the first tensor.\n   * @param shape2: Shape of the second tensor.\n   * @returns Expected output shape when an elementwise operation is carried\n   *   out on 2 tensors with shapes `shape1` and `shape2`.\n   * @throws ValueError: If `shape1` and `shape2` are not compatible for\n   *   element-wise operations.\n   */\n\n\n  computeElementwiseOpOutputShape(shape1, shape2) {\n    if (shape1 == null || shape2 == null) {\n      return null;\n    } else if (shape1.length < shape2.length) {\n      return this.computeElementwiseOpOutputShape(shape2, shape1);\n    } else if (shape2.length === 0) {\n      return shape1;\n    }\n\n    const outputShape = shape1.slice(0, shape1.length - shape2.length);\n\n    for (let k = 0; k < shape2.length; ++k) {\n      const i = shape1[shape1.length - shape2.length + k];\n      const j = shape2[k];\n\n      if (i == null || j == null || i < 0 || j < 0) {\n        outputShape.push(null);\n      } else if (i === 1) {\n        outputShape.push(j);\n      } else if (j === 1) {\n        outputShape.push(i);\n      } else {\n        if (i !== j) {\n          throw new ValueError('Operands could not be broadcast together with shapes ' + JSON.stringify(shape1) + ' ' + JSON.stringify(shape2));\n        }\n\n        outputShape.push(i);\n      }\n    }\n\n    return outputShape;\n  }\n\n  build(inputShape) {\n    // Used purely for shape validation.\n    if (Array.isArray(inputShape) && !Array.isArray(inputShape[0])) {\n      // Make sure that inputShape is an Array of shape.\n      inputShape = [getExactlyOneShape(inputShape)];\n    }\n\n    inputShape = inputShape;\n\n    if (inputShape.length < 2) {\n      throw new ValueError('A merge layer should be called on an Array of at least 2 inputs.' + \" Got \".concat(inputShape.length, \" input(s).\"));\n    } // Make sure that there is at most one unique batch size among the input\n    // shapes.\n\n\n    let batchSizes = [];\n\n    for (const shape of inputShape) {\n      if (shape != null && shape[0] !== null) {\n        batchSizes.push(shape[0]);\n      }\n    }\n\n    batchSizes = generic_utils.unique(batchSizes);\n\n    if (batchSizes.length > 1) {\n      throw new ValueError(\"Can not merge tensors with different batch sizes. \" + \"Got tensors with shapes: \".concat(JSON.stringify(inputShape), \".\"));\n    }\n\n    let outputShape = inputShape[0] == null ? null : inputShape[0].slice(1);\n\n    for (let i = 1; i < inputShape.length; ++i) {\n      const shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n      outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n    } // If the inputs have different ranks, we have to reshape them to make them\n    // broadcastable.\n\n\n    const allRanks = inputShape.map(shape => shape.length);\n\n    if (inputShape.indexOf(null) === -1 && generic_utils.unique(allRanks).length === 1) {\n      this.reshapeRequired = false;\n    } else {\n      this.reshapeRequired = true;\n    }\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n\n      if (this.reshapeRequired) {\n        const reshapedInputs = [];\n        const inputDims = inputs.map(input => input.rank);\n\n        if (inputDims.indexOf(null) === -1) {\n          // If ranks of all inputs are available, we simply expand each of them\n          // at axis=1 until all of them have the same rank.\n          const maxNDim = mathUtils.max(inputDims);\n\n          for (let x of inputs) {\n            const xNDim = x.rank;\n\n            for (let k = 0; k < maxNDim - xNDim; ++k) {\n              x = K.expandDims(x, 1);\n            }\n\n            reshapedInputs.push(x);\n          }\n\n          return this.mergeFunction(reshapedInputs);\n        } else {\n          // Transpose all inputs so that batch size is the last dimension.\n          // [batchSize, dim1, dim2, ...] -> [dim1, dim2, ..., batchSize]\n          let transposed = false;\n\n          for (const x of inputs) {\n            const xNDim = x.rank;\n\n            if (xNDim == null) {\n              const xShape = x.shape;\n              const batchSize = xShape[0];\n              const newShape = xShape.slice(1).concat([batchSize]);\n              let xTransposed = x.reshape([batchSize].concat(mathUtils.arrayProd(xShape.slice(1))));\n              xTransposed = tfc.transpose(xTransposed, [1, 0]);\n              xTransposed = xTransposed.reshape(newShape);\n              reshapedInputs.push(xTransposed);\n              transposed = true;\n            } else if (xNDim > 1) {\n              const dims = mathUtils.range(1, xNDim).concat([0]);\n              reshapedInputs.push(tfc.transpose(x, dims));\n              transposed = true;\n            } else {\n              // We don't transpose inputs if they are 1D vectors or scalars.\n              reshapedInputs.push(x);\n            }\n          }\n\n          let y = this.mergeFunction(reshapedInputs);\n          const yNDim = y.rank;\n\n          if (transposed) {\n            // If inputs have been transposed, we have to transpose the output\n            // too.\n            if (yNDim == null) {\n              const yShape = y.shape;\n              const yNDim = yShape.length;\n              const batchSize = yShape[yNDim - 1];\n              const newShape = [batchSize].concat(yShape.slice(0, yShape.length - 1));\n              y = tfc.transpose(y.reshape([-1, batchSize]), [1, 0]).reshape(newShape);\n            } else if (yNDim > 1) {\n              const dims = [yNDim - 1].concat(mathUtils.range(0, yNDim - 1));\n              y = tfc.transpose(y, dims);\n            }\n          }\n\n          return y;\n        }\n      } else {\n        return this.mergeFunction(inputs);\n      }\n    });\n  }\n\n  computeOutputShape(inputShape) {\n    inputShape = inputShape;\n    let outputShape;\n\n    if (inputShape[0] == null) {\n      outputShape = null;\n    } else {\n      outputShape = inputShape[0].slice(1);\n    }\n\n    for (let i = 1; i < inputShape.length; ++i) {\n      const shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n      outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n    }\n\n    let batchSizes = [];\n\n    for (const shape of inputShape) {\n      if (shape != null && shape[0] !== null) {\n        batchSizes.push(shape[0]);\n      }\n    }\n\n    batchSizes = generic_utils.unique(batchSizes);\n\n    if (batchSizes.length === 1) {\n      outputShape = batchSizes.concat(outputShape);\n    } else {\n      outputShape = [null].concat(outputShape);\n    }\n\n    return outputShape;\n  }\n\n  computeMask(inputs, mask) {\n    return tfc.tidy(() => {\n      if (mask == null) {\n        return null;\n      }\n\n      if (!Array.isArray(mask)) {\n        throw new ValueError('`mask` should be an Array');\n      }\n\n      if (!Array.isArray(inputs)) {\n        throw new ValueError('`inputs` should be an Array');\n      }\n\n      if (mask.length !== inputs.length) {\n        throw new ValueError(\"The Array 'inputs' and 'mask' are expected to have the same \" + \"length, but have different lengths \" + \"(\".concat(inputs.length, \" vs \").concat(mask.length, \")\"));\n      }\n\n      if (mask.every(m => m == null)) {\n        return null;\n      }\n\n      mask = mask.map(m => m == null ? m : tfc.expandDims(m, 0));\n      let output = mask[0];\n\n      for (let i = 1; i < mask.length - 1; ++i) {\n        output = tfc.logicalAnd(output, mask[i]);\n      }\n\n      return output;\n    });\n  }\n\n}\nexport class Add extends Merge {\n  constructor(args) {\n    super(args);\n  }\n\n  mergeFunction(inputs) {\n    return tidy(() => {\n      let output = inputs[0].clone();\n\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.add(output, inputs[i]);\n      }\n\n      return output;\n    });\n  }\n\n}\n/** @nocollapse */\n\nAdd.className = 'Add';\nserialization.registerClass(Add);\n/**\n * Calculate the element-wise sum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Add` layer, by using no input argument\n *    or a single configuration argument. The resultant `Add` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const addLayer = tf.layers.add();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = addLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.add([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.add([input1, input2]).print();\n * // Gives [[11, 22], [33, 44]].\n *\n */\n\nexport function add(config) {\n  if (Array.isArray(config)) {\n    const layer = new Add({});\n    return layer.apply(config);\n  } else {\n    return new Add(config);\n  }\n}\nexport class Multiply extends Merge {\n  constructor(args) {\n    super(args);\n  }\n\n  mergeFunction(inputs) {\n    return tidy(() => {\n      let output = inputs[0].clone();\n\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.mul(output, inputs[i]);\n      }\n\n      return output;\n    });\n  }\n\n}\n/** @nocollapse */\n\nMultiply.className = 'Multiply';\nserialization.registerClass(Multiply);\n/**\n * Calculate the element-wise product of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Multiply` layer, by using no input argument\n *    or a single configuration argument. The resultant `Multiply` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const multiplyLayer = tf.layers.multiply();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = multiplyLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.multiply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.multiply([input1, input2]).print();\n * // Gives [[10, 40], [90, 160]].\n *\n */\n\nexport function multiply(config) {\n  if (Array.isArray(config)) {\n    const layer = new Multiply({});\n    return layer.apply(config);\n  } else {\n    return new Multiply(config);\n  }\n}\nexport class Average extends Merge {\n  constructor(args) {\n    super(args);\n  }\n\n  mergeFunction(inputs) {\n    return tidy(() => {\n      let output = inputs[0].clone();\n\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.add(output, inputs[i]);\n      }\n\n      return tfc.mul(1 / inputs.length, output);\n    });\n  }\n\n}\n/** @nocollapse */\n\nAverage.className = 'Average';\nserialization.registerClass(Average);\n/**\n * Calculate the element-wise arithmetic mean of inputs, which all have the same\n * shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Average` layer, by using no input argument\n *    or a single configuration argument. The resultant `Average` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const averageLayer = tf.layers.average();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = averageLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.average([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.average([input1, input2]).print();\n * // Gives [[5.5, 11], [16.5, 22]].\n *\n */\n\nexport function average(config) {\n  if (Array.isArray(config)) {\n    const layer = new Average({});\n    return layer.apply(config);\n  } else {\n    return new Average(config);\n  }\n}\nexport class Maximum extends Merge {\n  constructor(args) {\n    super(args);\n  }\n\n  mergeFunction(inputs) {\n    return tidy(() => {\n      let output = inputs[0];\n\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.maximum(output, inputs[i]);\n      }\n\n      return output;\n    });\n  }\n\n}\n/** @nocollapse */\n\nMaximum.className = 'Maximum';\nserialization.registerClass(Maximum);\n/**\n * Calculate the element-wise maximum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Maximum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Maximum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const maximumLayer = tf.layers.maximum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = maximumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.maximum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.maximum([input1, input2]).print();\n * // Gives [[10, 20], [30, 40]].\n *\n */\n\nexport function maximum(config) {\n  if (Array.isArray(config)) {\n    const layer = new Maximum({});\n    return layer.apply(config);\n  } else {\n    return new Maximum(config);\n  }\n}\nexport class Minimum extends Merge {\n  constructor(args) {\n    super(args);\n  }\n\n  mergeFunction(inputs) {\n    return tidy(() => {\n      let output = inputs[0];\n\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.minimum(output, inputs[i]);\n      }\n\n      return output;\n    });\n  }\n\n}\n/** @nocollapse */\n\nMinimum.className = 'Minimum';\nserialization.registerClass(Minimum);\n/**\n * Calculate the element-wise minimum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Minimum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Minimum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const minimumLayer = tf.layers.minimum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = minimumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.minimum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.minimum([input1, input2]).print();\n * // Gives [[1, 2], [3, 4]].\n *\n */\n\nexport function minimum(config) {\n  if (Array.isArray(config)) {\n    const layer = new Minimum({});\n    return layer.apply(config);\n  } else {\n    return new Minimum(config);\n  }\n}\nexport class Concatenate extends Merge {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_AXIS = -1;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n    this.supportsMasking = true;\n    this.reshapeRequired = false;\n  }\n\n  build(inputShape) {\n    // Used purely for shape validation.]\n    if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0])) || inputShape.length === 1) {\n      throw new ValueError('A `Concatenate` layer should be called on a list of at least 2 ' + 'inputs');\n    }\n\n    inputShape = inputShape;\n    let allNoneShape = true;\n\n    for (const shape of inputShape) {\n      if (shape != null) {\n        allNoneShape = false;\n        break;\n      }\n    }\n\n    if (allNoneShape) {\n      return;\n    }\n\n    const shapeSet = [];\n\n    for (let i = 0; i < inputShape.length; ++i) {\n      const shapeWithoutConcatAxis = inputShape[i].slice();\n      shapeWithoutConcatAxis.splice(this.axis, 1);\n      let exists = false;\n\n      for (const shape of shapeSet) {\n        if (util.arraysEqual(shape, shapeWithoutConcatAxis)) {\n          exists = true;\n          break;\n        }\n      }\n\n      if (!exists) {\n        shapeSet.push(shapeWithoutConcatAxis);\n      }\n    }\n\n    if (shapeSet.length > 1) {\n      throw new ValueError('A `Concatenate` layer requires inputs with matching shapes ' + 'except for the concat axis. Got input shapes: ' + JSON.stringify(inputShape));\n    }\n  }\n\n  mergeFunction(inputs) {\n    return tidy(() => {\n      return K.concatenate(inputs, this.axis);\n    });\n  }\n\n  computeOutputShape(inputShape) {\n    if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0]))) {\n      throw new ValueError('A `Concatenate` layer should be called on a list of inputs.');\n    }\n\n    const inputShapes = inputShape;\n    const outputShape = inputShapes[0].slice();\n    const axis = this.axis < 0 ? outputShape.length + this.axis : this.axis; // Porting Note: the line above is because TypeScript doesn't support\n    //   negative indices.\n\n    for (const shape of inputShapes.slice(1)) {\n      if (outputShape[axis] == null || shape[axis] == null) {\n        outputShape[axis] = null;\n        break;\n      }\n\n      outputShape[axis] += shape[axis];\n    }\n\n    return outputShape;\n  }\n\n  computeMask(inputs, mask) {\n    if (mask == null) {\n      return null;\n    }\n\n    if (!Array.isArray(mask)) {\n      throw new ValueError('`mask` should be an array for Concatenate');\n    }\n\n    if (!Array.isArray(inputs)) {\n      throw new ValueError('`inputs` should be an array for Concatenate');\n    }\n\n    if (mask.length !== inputs.length) {\n      throw new ValueError(\"Mismatch in the length of mask (\".concat(mask.length, \") \") + \"and the legnth of inputs (\".concat(inputs.length, \")\"));\n    }\n\n    return tfc.tidy(() => {\n      let allNullMasks = true;\n      mask.forEach(m => {\n        if (m != null) {\n          allNullMasks = false;\n          return;\n        }\n      });\n\n      if (allNullMasks) {\n        return null;\n      }\n\n      const outputMasks = [];\n\n      for (let i = 0; i < inputs.length; ++i) {\n        if (mask[i] == null) {\n          // Input is unmasked. Append all 1's to masks.\n          outputMasks.push(tfc.onesLike(inputs[i]).asType('bool'));\n        } else if (mask[i].rank < inputs[i].rank) {\n          // Mask is smaller than the input, expand it.\n          outputMasks.push(tfc.expandDims(mask[i], -1));\n        } else {\n          outputMasks.push(mask[i]);\n        }\n      }\n\n      const concatenatedMasks = tfc.concat(outputMasks, this.axis);\n      return tfc.all(concatenatedMasks, -1, false);\n    });\n  }\n\n  getConfig() {\n    const config = {\n      'axis': this.axis\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nConcatenate.className = 'Concatenate';\nserialization.registerClass(Concatenate);\n/**\n * Concatenate an `Array` of inputs.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Concatenate` layer, by using no input argument\n *    or a single configuration argument. The resultant `Concatenate` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const concatLayer = tf.layers.concatenate();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = concatLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 7], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = tf.layers.concatenate([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([[1, 2], [3, 4]], [2, 2]);\n * const input2 = tf.tensor2d([[10, 20], [30, 40]], [2, 2]);\n * tf.layers.concatenate([input1, input2]).print();\n * // Gives [[1, 2, 10, 20], [3, 4, 30, 40]].\n *\n */\n\nexport function concatenate(config) {\n  if (Array.isArray(config)) {\n    const layer = new Concatenate({});\n    return layer.apply(config);\n  } else {\n    return new Concatenate(config);\n  }\n}\n/**\n * Interpretable potentially negative axis index.\n *\n * For example, given axis = -1, and dim = 3, this function will return 2.\n *\n * @param axis The axis index, may be a positive, zero or negative integer.\n * @param dim Total number of dimensions, a positive integer.\n * @returns A non-negative axis index equivalent to the input `axis`.\n */\n\nfunction interpretAxis(axis, dim) {\n  while (axis < 0) {\n    axis += dim;\n  }\n\n  return axis;\n}\n\nfunction batchDot(x, y, axes) {\n  if (x.shape.length > 3 || y.shape.length > 3) {\n    throw new NotImplementedError('batchDot is not implemented for tensors of 4D or higher rank yet');\n  }\n\n  tfc.util.assert(x.shape.length >= 2, () => \"batchDot requires the rank of x to be >= 2, \" + \"but got \".concat(x.shape.length));\n  tfc.util.assert(x.shape.length >= 2, () => \"batchDot requires the rank of y to be >= 2, \" + \"but got \".concat(y.shape.length));\n\n  if (typeof axes === 'number') {\n    axes = [axes, axes];\n  }\n\n  if (x.dtype === 'complex64' || y.dtype === 'complex64') {\n    throw new NotImplementedError('batchDot is not implemented for complex64-type Tensors yet.');\n  }\n\n  const xNDim = x.shape.length;\n  const yNDim = y.shape.length;\n\n  if (axes == null) {\n    // Behave like batchMatmul by default.\n    axes = [xNDim - 1, yNDim - 2];\n  }\n\n  const axesArray = axes;\n  return tfc.tidy(() => {\n    let diff;\n\n    if (xNDim > yNDim) {\n      diff = xNDim - yNDim;\n      const diffShape = [];\n\n      for (let i = 0; i < diff; ++i) {\n        diffShape.push(1);\n      }\n\n      y = y.reshape(y.shape.concat(diffShape));\n    } else if (yNDim > xNDim) {\n      diff = yNDim - xNDim;\n      const diffShape = [];\n\n      for (let i = 0; i < diff; ++i) {\n        diffShape.push(1);\n      }\n\n      x = x.reshape(x.shape.concat(diffShape));\n    } else {\n      diff = 0;\n    }\n\n    let out;\n\n    if (x.shape.length === 2 && y.shape.length === 2) {\n      if (axesArray[0] === axesArray[1]) {\n        out = x.mul(y).sum(axesArray[0]);\n      } else {\n        out = x.transpose([1, 0]).mul(y).sum(axesArray[1]);\n      }\n    } else {\n      const adjX = axesArray[0] !== x.shape.length - 1;\n      const adjY = axesArray[1] === y.shape.length - 1;\n      out = x.matMul(y, adjX, adjY);\n    }\n\n    if (diff > 0) {\n      let idx;\n\n      if (xNDim > yNDim) {\n        idx = xNDim + yNDim - 3;\n      } else {\n        idx = xNDim - 1;\n      }\n\n      const squeezeAxes = [];\n\n      for (let i = idx; i < idx + diff; ++i) {\n        squeezeAxes.push(i);\n      }\n\n      out = out.squeeze(squeezeAxes);\n    }\n\n    if (out.shape.length === 1) {\n      out = out.expandDims(1);\n    }\n\n    return out;\n  });\n}\n\nexport class Dot extends Merge {\n  constructor(args) {\n    super(args);\n    this.axes = args.axes;\n    this.normalize = args.normalize == null ? false : args.normalize;\n    this.supportsMasking = true;\n    this.reshapeRequired = false;\n  }\n\n  build(inputShape) {\n    tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 && Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), () => 'A `Dot` layer should be called on a list of exactly 2 inputs.');\n    const shape1 = inputShape[0];\n    const shape2 = inputShape[1];\n\n    if (shape1.length > 3 || shape2.length > 3) {\n      throw new NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n    }\n\n    const axes = this.interpretAxes(shape1, shape2);\n\n    if (shape1[axes[0]] !== shape2[axes[1]]) {\n      throw new ValueError(\"Dimension incompatibility: \" + \"\".concat(shape1[axes[0]], \" !== \").concat(shape2[axes[1]]));\n    }\n  }\n\n  mergeFunction(inputs) {\n    if (inputs.length !== 2) {\n      throw new ValueError('A `Dot` layer must be called on exactly 2 inputs, ' + \"but received \".concat(inputs.length, \" input(s).\"));\n    }\n\n    let x1 = inputs[0];\n    let x2 = inputs[1];\n    let axes;\n\n    if (!Array.isArray(this.axes)) {\n      axes = [interpretAxis(this.axes, x1.shape.length), interpretAxis(this.axes, x2.shape.length)];\n    } else {\n      axes = this.axes.map((axis, i) => interpretAxis(axis, inputs[i].shape.length));\n    }\n\n    if (this.normalize) {\n      x1 = l2Normalize(x1, axes[0]);\n      x2 = l2Normalize(x2, axes[1]);\n    }\n\n    return batchDot(x1, x2, axes);\n  }\n\n  interpretAxes(shape1, shape2) {\n    let axes;\n\n    if (!Array.isArray(this.axes)) {\n      // `this.axes` is a single integer.\n      axes = [interpretAxis(this.axes, shape1.length), interpretAxis(this.axes, shape2.length)];\n    } else {\n      // `this.axes` is an Array of integers.\n      axes = this.axes;\n    }\n\n    return axes;\n  }\n\n  computeOutputShape(inputShape) {\n    tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 && Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), () => 'A `Dot` layer should be called on a list of exactly 2 inputs.');\n    const shape1 = inputShape[0].slice();\n    const shape2 = inputShape[1].slice();\n\n    if (shape1.length > 3 || shape2.length > 3) {\n      throw new NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n    }\n\n    const axes = this.interpretAxes(shape1, shape2);\n    shape1.splice(axes[0], 1);\n    shape2.splice(axes[1], 1);\n    shape2.splice(0, 1);\n    const outputShape = shape1.concat(shape2);\n\n    if (outputShape.length === 1) {\n      outputShape.push(1);\n    }\n\n    return outputShape;\n  }\n\n  computeMask(inputs, mask) {\n    return null;\n  }\n\n  getConfig() {\n    const config = {\n      'axes': this.axes,\n      'normalize': this.normalize\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nDot.className = 'Dot';\nserialization.registerClass(Dot); // TODO(cais): Add functional interfaces for the merge layers.","map":null,"metadata":{},"sourceType":"module"}