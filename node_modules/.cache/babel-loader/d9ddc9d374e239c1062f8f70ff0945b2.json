{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport { env, tensor, util } from '@tensorflow/tfjs-core';\nimport { LazyIterator } from './lazy_iterator';\n/**\n * Provide a stream of tensors from microphone audio stream. The tensors are\n * representing audio data as frequency-domain spectrogram generated with\n * browser's native FFT. Tensors representing time-domain waveform is available\n * based on configuration. Only works in browser environment.\n */\n\nexport class MicrophoneIterator extends LazyIterator {\n  constructor(microphoneConfig) {\n    super();\n    this.microphoneConfig = microphoneConfig;\n    this.isClosed = false;\n    this.fftSize = microphoneConfig.fftSize || 1024;\n    const fftSizeLog2 = Math.log2(this.fftSize);\n\n    if (this.fftSize < 0 || fftSizeLog2 < 4 || fftSizeLog2 > 14 || !Number.isInteger(fftSizeLog2)) {\n      throw new Error(\"Invalid fftSize: it must be a power of 2 between \" + \"2 to 4 and 2 to 14, but got \".concat(this.fftSize));\n    }\n\n    this.numFrames = microphoneConfig.numFramesPerSpectrogram || 43;\n    this.sampleRateHz = microphoneConfig.sampleRateHz;\n    this.columnTruncateLength = microphoneConfig.columnTruncateLength || this.fftSize;\n    this.audioTrackConstraints = microphoneConfig.audioTrackConstraints;\n    this.smoothingTimeConstant = microphoneConfig.smoothingTimeConstant || 0;\n    this.includeSpectrogram = microphoneConfig.includeSpectrogram === false ? false : true;\n    this.includeWaveform = microphoneConfig.includeWaveform === true ? true : false;\n\n    if (!this.includeSpectrogram && !this.includeWaveform) {\n      throw new Error('Both includeSpectrogram and includeWaveform are false. ' + 'At least one type of data should be returned.');\n    }\n  }\n\n  summary() {\n    return \"microphone\";\n  } // Construct a MicrophoneIterator and start the audio stream.\n\n\n  static async create(microphoneConfig = {}) {\n    if (env().get('IS_NODE')) {\n      throw new Error('microphone API is only supported in browser environment.');\n    }\n\n    const microphoneIterator = new MicrophoneIterator(microphoneConfig); // Call async function start() to initialize the audio stream.\n\n    await microphoneIterator.start();\n    return microphoneIterator;\n  } // Start the audio stream and FFT.\n\n\n  async start() {\n    try {\n      this.stream = await navigator.mediaDevices.getUserMedia({\n        audio: this.audioTrackConstraints == null ? true : this.audioTrackConstraints,\n        video: false\n      });\n    } catch (e) {\n      throw new Error(\"Error thrown while initializing video stream: \".concat(e.message));\n    }\n\n    if (!this.stream) {\n      throw new Error('Could not obtain audio from microphone.');\n    }\n\n    const ctxConstructor = // tslint:disable-next-line:no-any\n    window.AudioContext || window.webkitAudioContext;\n    this.audioContext = new ctxConstructor();\n\n    if (!this.sampleRateHz) {\n      // If sample rate is not provided, use the available sample rate on\n      // device.\n      this.sampleRateHz = this.audioContext.sampleRate;\n    } else if (this.audioContext.sampleRate !== this.sampleRateHz) {\n      throw new Error(\"Mismatch in sampling rate: \" + \"Expected: \".concat(this.sampleRateHz, \"; \") + \"Actual: \".concat(this.audioContext.sampleRate));\n    }\n\n    const streamSource = this.audioContext.createMediaStreamSource(this.stream);\n    this.analyser = this.audioContext.createAnalyser();\n    this.analyser.fftSize = this.fftSize * 2;\n    this.analyser.smoothingTimeConstant = this.smoothingTimeConstant;\n    streamSource.connect(this.analyser);\n    this.freqData = new Float32Array(this.fftSize);\n    this.timeData = new Float32Array(this.fftSize);\n    return;\n  }\n\n  async next() {\n    if (this.isClosed) {\n      return {\n        value: null,\n        done: true\n      };\n    }\n\n    let spectrogramTensor;\n    let waveformTensor;\n    const audioDataQueue = await this.getAudioData();\n\n    if (this.includeSpectrogram) {\n      const freqData = this.flattenQueue(audioDataQueue.freqDataQueue);\n      spectrogramTensor = this.getTensorFromAudioDataArray(freqData, [this.numFrames, this.columnTruncateLength, 1]);\n    }\n\n    if (this.includeWaveform) {\n      const timeData = this.flattenQueue(audioDataQueue.timeDataQueue);\n      waveformTensor = this.getTensorFromAudioDataArray(timeData, [this.numFrames * this.fftSize, 1]);\n    }\n\n    return {\n      value: {\n        'spectrogram': spectrogramTensor,\n        'waveform': waveformTensor\n      },\n      done: false\n    };\n  } // Capture one result from the audio stream, and extract the value from\n  // iterator.next() result.\n\n\n  async capture() {\n    return (await this.next()).value;\n  }\n\n  async getAudioData() {\n    const freqDataQueue = [];\n    const timeDataQueue = [];\n    let currentFrames = 0;\n    return new Promise(resolve => {\n      const intervalID = setInterval(() => {\n        if (this.includeSpectrogram) {\n          this.analyser.getFloatFrequencyData(this.freqData); // If the audio stream is initializing, return empty queue.\n\n          if (this.freqData[0] === -Infinity) {\n            resolve({\n              freqDataQueue,\n              timeDataQueue\n            });\n          }\n\n          freqDataQueue.push(this.freqData.slice(0, this.columnTruncateLength));\n        }\n\n        if (this.includeWaveform) {\n          this.analyser.getFloatTimeDomainData(this.timeData);\n          timeDataQueue.push(this.timeData.slice());\n        } // Clean interval and return when all frames have been collected\n\n\n        if (++currentFrames === this.numFrames) {\n          clearInterval(intervalID);\n          resolve({\n            freqDataQueue,\n            timeDataQueue\n          });\n        }\n      }, this.fftSize / this.sampleRateHz * 1e3);\n    });\n  } // Stop the audio stream and pause the iterator.\n\n\n  stop() {\n    if (!this.isClosed) {\n      this.isClosed = true;\n      this.analyser.disconnect();\n      this.audioContext.close();\n\n      if (this.stream != null && this.stream.getTracks().length > 0) {\n        this.stream.getTracks()[0].stop();\n      }\n    }\n  } // Override toArray() function to prevent collecting.\n\n\n  toArray() {\n    throw new Error('Can not convert infinite audio stream to array.');\n  } // Return audio sampling rate in Hz\n\n\n  getSampleRate() {\n    return this.sampleRateHz;\n  }\n\n  flattenQueue(queue) {\n    const frameSize = queue[0].length;\n    const freqData = new Float32Array(queue.length * frameSize);\n    queue.forEach((data, i) => freqData.set(data, i * frameSize));\n    return freqData;\n  }\n\n  getTensorFromAudioDataArray(freqData, shape) {\n    const vals = new Float32Array(util.sizeFromShape(shape)); // If the data is less than the output shape, the rest is padded with zeros.\n\n    vals.set(freqData, vals.length - freqData.length);\n    return tensor(vals, shape);\n  }\n\n}","map":null,"metadata":{},"sourceType":"module"}