{"ast":null,"code":"import _slicedToArray from \"/home/victor/COVID-19-Coding-Fest/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\n\n/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport { engine, env } from '@tensorflow/tfjs-core';\nimport { backend_util, buffer, slice_util, util } from '@tensorflow/tfjs-core';\nimport { DataStorage, KernelBackend, max, reshape, TensorBuffer, upcastType } from '@tensorflow/tfjs-core';\nimport { kernel_impls } from '@tensorflow/tfjs-core';\nconst nonMaxSuppressionV3Impl = kernel_impls.nonMaxSuppressionV3Impl;\nconst split = kernel_impls.split;\nconst tile = kernel_impls.tile;\nconst topkImpl = kernel_impls.topkImpl;\nconst whereImpl = kernel_impls.whereImpl;\nimport * as seedrandom from 'seedrandom';\nimport { assertNotComplex } from './cpu_util';\nimport { maxPoolPositions, pool } from './utils/pool_utils';\n\nfunction mapActivation(backend, x, activation, preluActivationWeights) {\n  if (activation === 'linear') {\n    return backend.linear(x);\n  } else if (activation === 'relu') {\n    return backend.relu(x);\n  } else if (activation === 'elu') {\n    return backend.elu(x);\n  } else if (activation === 'relu6') {\n    return backend.relu6(x);\n  } else if (activation === 'prelu') {\n    return backend.prelu(x, preluActivationWeights);\n  }\n\n  throw new Error(\"Activation \".concat(activation, \" has not been implemented for the CPU backend.\"));\n}\n\nexport class MathBackendCPU extends KernelBackend {\n  constructor() {\n    super();\n    this.blockSize = 48;\n    this.firstUse = true;\n    this.data = new DataStorage(this, engine());\n  }\n\n  write(values, shape, dtype) {\n    if (this.firstUse) {\n      this.firstUse = false;\n\n      if (env().get('IS_NODE')) {\n        backend_util.warn('\\n============================\\n' + 'Hi there ðŸ‘‹. Looks like you are running TensorFlow.js in ' + 'Node.js. To speed things up dramatically, install our node ' + 'backend, which binds to TensorFlow C++, by running ' + 'npm i @tensorflow/tfjs-node, ' + 'or npm i @tensorflow/tfjs-node-gpu if you have CUDA. ' + 'Then call require(\\'@tensorflow/tfjs-node\\'); (-gpu ' + 'suffix for CUDA) at the start of your program. ' + 'Visit https://github.com/tensorflow/tfjs-node for more details.' + '\\n============================');\n      }\n    }\n\n    const dataId = {};\n    this.data.set(dataId, {\n      values,\n      dtype\n    });\n    return dataId;\n  }\n\n  move(dataId, values, shape, dtype) {\n    this.data.set(dataId, {\n      values,\n      dtype\n    });\n  }\n\n  numDataIds() {\n    return this.data.numDataIds();\n  }\n\n  async read(dataId) {\n    return this.readSync(dataId);\n  }\n\n  readSync(dataId) {\n    const _this$data$get = this.data.get(dataId),\n          dtype = _this$data$get.dtype,\n          complexTensors = _this$data$get.complexTensors;\n\n    if (dtype === 'complex64') {\n      const realValues = this.readSync(complexTensors.real.dataId);\n      const imagValues = this.readSync(complexTensors.imag.dataId);\n      return backend_util.mergeRealAndImagArrays(realValues, imagValues);\n    }\n\n    return this.data.get(dataId).values;\n  }\n\n  bufferSync(t) {\n    const data = this.readSync(t.dataId);\n    let decodedData = data;\n\n    if (t.dtype === 'string') {\n      try {\n        // Decode the bytes into string.\n        decodedData = data.map(d => util.decodeString(d));\n      } catch (_a) {\n        throw new Error('Failed to decode encoded string bytes into utf-8');\n      }\n    }\n\n    return tf.buffer(t.shape, t.dtype, decodedData);\n  }\n\n  makeOutput(values, shape, dtype) {\n    const dataId = this.write(values, shape, dtype);\n    return engine().makeTensorFromDataId(dataId, shape, dtype, this);\n  }\n\n  disposeData(dataId) {\n    if (this.data.has(dataId)) {\n      const _this$data$get2 = this.data.get(dataId),\n            complexTensors = _this$data$get2.complexTensors;\n\n      if (complexTensors != null) {\n        complexTensors.real.dispose();\n        complexTensors.imag.dispose();\n      }\n\n      this.data.delete(dataId);\n    }\n  }\n\n  async time(f) {\n    const start = util.now();\n    f();\n    const kernelMs = util.now() - start;\n    return {\n      kernelMs\n    };\n  }\n\n  memory() {\n    return {\n      // Unreliable due to automatic gc. The numbers above are cumulative.\n      unreliable: true,\n      reasons: ['The reported memory is an upper bound. Due to automatic garbage ' + 'collection, the true allocated memory may be less.']\n    };\n  }\n\n  complex(real, imag) {\n    const result = this.makeOutput(null, real.shape, 'complex64');\n    const resultData = this.data.get(result.dataId); // The backend owns the reference to the underlying real and imaginary\n    // clones. These will explicitly get disposed when the complex tensor is\n    // disposed.\n\n    resultData.complexTensors = {\n      real: engine().keep(real.clone()),\n      imag: engine().keep(imag.clone())\n    };\n    return result;\n  }\n\n  real(input) {\n    const resultData = this.data.get(input.dataId);\n    return resultData.complexTensors.real.clone();\n  }\n\n  imag(input) {\n    const resultData = this.data.get(input.dataId);\n    return resultData.complexTensors.imag.clone();\n  }\n\n  slice(x, begin, size) {\n    assertNotComplex(x, 'slice');\n    const isContinous = slice_util.isSliceContinous(x.shape, begin, size);\n\n    if (isContinous) {\n      const flatOffset = slice_util.computeFlatOffset(begin, x.strides);\n      const length = util.sizeFromShape(size);\n      const vals = this.readSync(x.dataId);\n      return tf.tensor(vals.subarray(flatOffset, flatOffset + length), size, x.dtype);\n    }\n\n    const buffer = tf.buffer(size, x.dtype);\n    const xBuf = this.bufferSync(x);\n\n    for (let i = 0; i < buffer.size; ++i) {\n      const loc = buffer.indexToLoc(i);\n      const xLoc = loc.map((idx, j) => idx + begin[j]);\n      buffer.values[i] = xBuf.get(...xLoc);\n    }\n\n    return buffer.toTensor();\n  }\n\n  stridedSlice(x, begin, end, strides) {\n    assertNotComplex(x, 'stridedSlice');\n    const outShape = slice_util.computeOutShape(begin, end, strides);\n\n    if (outShape.some(axis => axis === 0)) {\n      return tf.tensor([], outShape);\n    }\n\n    const buffer = tf.buffer(outShape, x.dtype);\n    const xBuf = this.bufferSync(x);\n\n    for (let i = 0; i < buffer.size; i++) {\n      const loc = buffer.indexToLoc(i);\n      const newLoc = new Array(loc.length);\n\n      for (let j = 0; j < newLoc.length; j++) {\n        newLoc[j] = loc[j] * strides[j] + begin[j];\n      }\n\n      buffer.set(xBuf.get(...newLoc), ...loc);\n    }\n\n    return buffer.toTensor();\n  }\n\n  diag(x) {\n    const xVals = this.readSync(x.dataId);\n    const buffer = tf.buffer([x.size, x.size], x.dtype);\n    const vals = buffer.values;\n\n    for (let i = 0; i < xVals.length; i++) {\n      vals[i * x.size + i] = xVals[i];\n    }\n\n    return buffer.toTensor();\n  }\n\n  unstack(x, axis) {\n    const num = x.shape[axis];\n    const outShape = new Array(x.rank - 1);\n    let outIndex = 0;\n\n    for (let i = 0; i < x.rank; i++) {\n      if (i !== axis) {\n        outShape[outIndex++] = x.shape[i];\n      }\n    }\n\n    const begin = new Array(x.rank).fill(0);\n    const size = x.shape.slice();\n    size[axis] = 1;\n    const res = new Array(num);\n\n    for (let i = 0; i < res.length; i++) {\n      begin[axis] = i;\n      res[i] = this.slice(x, begin, size).reshape(outShape);\n    }\n\n    return res;\n  }\n\n  reverse(x, axis) {\n    assertNotComplex(x, 'reverse');\n    const buffer = tf.buffer(x.shape, x.dtype);\n    const xBuf = this.bufferSync(x);\n\n    for (let i = 0; i < buffer.size; i++) {\n      const outLoc = buffer.indexToLoc(i);\n      const inLoc = outLoc.slice();\n      axis.forEach(ax => inLoc[ax] = x.shape[ax] - 1 - inLoc[ax]);\n      buffer.set(xBuf.get(...inLoc), ...outLoc);\n    }\n\n    return buffer.toTensor();\n  }\n\n  concat(tensors, axis) {\n    if (tensors[0].dtype === 'complex64') {\n      const reals = tensors.map(t => tf.real(t));\n      const imags = tensors.map(t => tf.imag(t));\n      return tf.complex(this.concat(reals, axis), this.concat(imags, axis));\n    }\n\n    const tensors2D = tensors.map(t => {\n      const innerSize = util.sizeFromShape(t.shape.slice(axis));\n      return t.as2D(-1, innerSize);\n    });\n    const outShape = backend_util.computeOutShape(tensors2D.map(t => t.shape), 1\n    /* axis\n    */\n    );\n    const values = tf.buffer(outShape, tensors[0].dtype).values;\n\n    if (tensors2D[0].shape[0] === 1) {\n      // Use built-in TypedArray.set() method for speed.\n      let offset = 0;\n      tensors2D.forEach(t => {\n        values.set(this.readSync(t.dataId), offset);\n        offset += t.size;\n      });\n    } else {\n      let colOffset = 0;\n      tensors2D.forEach(t => {\n        const tVals = this.readSync(t.dataId);\n        let tIdx = 0;\n\n        for (let row = 0; row < t.shape[0]; ++row) {\n          const resIdx = row * outShape[1] + colOffset;\n\n          for (let col = 0; col < t.shape[1]; ++col) {\n            values[resIdx + col] = tVals[tIdx++];\n          }\n        }\n\n        colOffset += t.shape[1];\n      });\n    }\n\n    const finalOutShape = backend_util.computeOutShape(tensors.map(t => t.shape), axis);\n    return tf.tensor(values, finalOutShape, tensors[0].dtype);\n  }\n\n  neg(x) {\n    assertNotComplex(x, 'neg');\n    return this.multiply(tf.scalar(-1), x);\n  }\n\n  add(a, b) {\n    if (a.dtype === 'complex64' || b.dtype === 'complex64') {\n      return this.broadcastedBinaryComplexOp(a.cast('complex64'), b.cast('complex64'), (aReal, aImag, bReal, bImag) => {\n        return {\n          real: aReal + bReal,\n          imag: aImag + bImag\n        };\n      });\n    }\n\n    return this.broadcastedBinaryOp(a, b, upcastType(a.dtype, b.dtype), (aValue, bValue) => aValue + bValue);\n  }\n\n  addN(tensors) {\n    assertNotComplex(tensors, 'addN');\n    const vals = tensors.map(t => this.readSync(t.dataId));\n    const result = tf.buffer(tensors[0].shape, tensors[0].dtype);\n    const resultVals = result.values;\n\n    for (let i = 0; i < tensors.length; i++) {\n      const currVals = vals[i];\n\n      for (let j = 0; j < resultVals.length; j++) {\n        resultVals[j] += currVals[j];\n      }\n    }\n\n    return result.toTensor();\n  }\n\n  softmax(logits, dim) {\n    const axes = util.parseAxisParam([dim], logits.shape); // TODO(annxingyuan): Call maxImpl rather than op as part of softmax kernel\n    // modularization.\n\n    const maxLogit = max(logits, axes);\n    const expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n    const a = this.subtract(logits, maxLogit.reshape(expandedShape));\n    const b = this.exp(a);\n    const sumExp = this.sum(b, axes).reshape(expandedShape); // TODO(annxingyuan): Call divImpl rather than op as part of softmax\n    // kernel modularization.\n\n    return tf.div(b, sumExp);\n  }\n\n  subtract(a, b) {\n    if (a.dtype === 'complex64' || b.dtype === 'complex64') {\n      return this.broadcastedBinaryComplexOp(a.cast('complex64'), b.cast('complex64'), (aReal, aImag, bReal, bImag) => {\n        return {\n          real: aReal - bReal,\n          imag: aImag - bImag\n        };\n      });\n    }\n\n    return this.broadcastedBinaryOp(a, b, upcastType(a.dtype, b.dtype), (aValue, bValue) => aValue - bValue);\n  }\n\n  pow(a, b) {\n    assertNotComplex([a, b], 'pow');\n    return this.broadcastedBinaryOp(a, b, a.dtype, (aValue, bValue) => Math.pow(aValue, bValue));\n  }\n\n  batchMatMul(a, b, transposeA, transposeB) {\n    assertNotComplex([a, b], 'matMul');\n    const sharedDim = transposeA ? a.shape[1] : a.shape[2];\n    const leftDim = transposeA ? a.shape[2] : a.shape[1];\n    const rightDim = transposeB ? b.shape[1] : b.shape[2];\n    const batchDim = a.shape[0];\n    const aValues = this.readSync(a.dataId);\n    const bValues = this.readSync(b.dataId);\n\n    const _ref = transposeA ? [a.strides[0], 1, a.strides[1]] : [a.strides[0], a.strides[1], 1],\n          _ref2 = _slicedToArray(_ref, 3),\n          aBatch = _ref2[0],\n          aOuterStep = _ref2[1],\n          aInnerStep = _ref2[2];\n\n    const _ref3 = transposeB ? [1, b.strides[1], b.strides[0]] : [b.strides[1], 1, b.strides[0]],\n          _ref4 = _slicedToArray(_ref3, 3),\n          bInnerStep = _ref4[0],\n          bOuterStep = _ref4[1],\n          bBatch = _ref4[2];\n\n    const size = leftDim * rightDim;\n    const result = tf.buffer([batchDim, leftDim, rightDim], a.dtype);\n    const resVals = result.values;\n    const blockSize = this.blockSize;\n\n    for (let b = 0; b < batchDim; b++) {\n      for (let i0 = 0; i0 < leftDim; i0 += blockSize) {\n        for (let j0 = 0; j0 < rightDim; j0 += blockSize) {\n          for (let k0 = 0; k0 < sharedDim; k0 += blockSize) {\n            // for when blockSize doesn't evenly divide the input\n            const iBlock = Math.min(i0 + blockSize, leftDim);\n            const jBlock = Math.min(j0 + blockSize, rightDim);\n            const kBlock = Math.min(k0 + blockSize, sharedDim);\n\n            for (let i = i0; i < iBlock; i++) {\n              for (let j = j0; j < jBlock; j++) {\n                let sum = 0.0;\n\n                for (let k = k0; k < kBlock; k++) {\n                  sum += aValues[b * aBatch + i * aOuterStep + k * aInnerStep] * bValues[k * bInnerStep + j * bOuterStep + b * bBatch];\n                }\n\n                resVals[b * size + (i * rightDim + j)] += sum;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    return result.toTensor();\n  }\n\n  fusedBatchMatMul({\n    a,\n    b,\n    transposeA,\n    transposeB,\n    bias,\n    activation,\n    preluActivationWeights\n  }) {\n    let result = this.batchMatMul(a, b, transposeA, transposeB);\n\n    if (bias) {\n      result = this.add(result, bias);\n    }\n\n    if (activation) {\n      result = mapActivation(this, result, activation, preluActivationWeights);\n    }\n\n    return result;\n  }\n\n  multiply(a, b) {\n    if (a.dtype === 'complex64' || b.dtype === 'complex64') {\n      return this.broadcastedBinaryComplexOp(a.cast('complex64'), b.cast('complex64'), (aReal, aImag, bReal, bImag) => {\n        return {\n          real: aReal * bReal - aImag * bImag,\n          imag: aReal * bImag + aImag * bReal\n        };\n      });\n    }\n\n    return this.broadcastedBinaryOp(a, b, upcastType(a.dtype, b.dtype), (aValue, bValue) => aValue * bValue);\n  }\n\n  floorDiv(a, b) {\n    assertNotComplex([a, b], 'floorDiv');\n\n    const op = (a, b) => Math.floor(a / b);\n\n    const outputDtype = 'int32';\n    return this.broadcastedBinaryOp(a, b, outputDtype, op);\n  }\n\n  sum(x, axes) {\n    assertNotComplex(x, 'sum');\n    backend_util.assertAxesAreInnerMostDims('sum', axes, x.rank);\n\n    const _backend_util$compute = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute2 = _slicedToArray(_backend_util$compute, 2),\n          outShape = _backend_util$compute2[0],\n          reduceShape = _backend_util$compute2[1];\n\n    const resultDtype = upcastType(x.dtype, 'int32');\n    const result = tf.zeros(outShape, resultDtype);\n    const reduceSize = util.sizeFromShape(reduceShape);\n    const vals = this.readSync(result.dataId);\n    const aVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < vals.length; ++i) {\n      const offset = i * reduceSize;\n      let sum = 0;\n\n      for (let j = 0; j < reduceSize; ++j) {\n        sum += aVals[offset + j];\n      }\n\n      vals[i] = sum;\n    }\n\n    return result;\n  }\n\n  prod(x, axes) {\n    assertNotComplex(x, 'sum');\n\n    const _backend_util$compute3 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute4 = _slicedToArray(_backend_util$compute3, 2),\n          outShape = _backend_util$compute4[0],\n          reduceShape = _backend_util$compute4[1];\n\n    const resultDtype = upcastType(x.dtype, 'int32');\n    const result = tf.zeros(outShape, resultDtype);\n    const reduceSize = util.sizeFromShape(reduceShape);\n    const vals = this.readSync(result.dataId);\n    const aVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < vals.length; ++i) {\n      const offset = i * reduceSize;\n      let prod = 1;\n\n      for (let j = 0; j < reduceSize; ++j) {\n        prod *= aVals[offset + j];\n      }\n\n      vals[i] = prod;\n    }\n\n    return result;\n  }\n\n  unsortedSegmentSum(x, segmentIds, numSegments) {\n    assertNotComplex(x, 'unsortedSegmentSum');\n    const res = []; // Reshape the segment id's so that they can be broadcast with\n    // x. The new shape should be [segmentIds.shape, 1, ..., 1]\n\n    const numIters = x.rank - segmentIds.rank;\n\n    for (let i = 0; i < numIters; ++i) {\n      segmentIds = segmentIds.expandDims(i + 1);\n    }\n\n    for (let i = 0; i < numSegments; ++i) {\n      const segmentId = tf.scalar(i, 'int32');\n      const mask = tf.equal(segmentId, segmentIds).asType('float32');\n      const sum = mask.mul(x).sum(0);\n      res.push(sum);\n    }\n\n    return tf.stack(res);\n  }\n\n  argMin(x, axis) {\n    assertNotComplex(x, 'argMin');\n    const axes = [axis];\n    backend_util.assertAxesAreInnerMostDims('argMin', axes, x.rank);\n\n    const _backend_util$compute5 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute6 = _slicedToArray(_backend_util$compute5, 2),\n          outShape = _backend_util$compute6[0],\n          reduceShape = _backend_util$compute6[1];\n\n    const result = tf.zeros(outShape, 'int32');\n    const reduceSize = util.sizeFromShape(reduceShape);\n    const vals = this.readSync(result.dataId);\n    const aVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < vals.length; ++i) {\n      const offset = i * reduceSize;\n      let min = aVals[offset];\n      let minIndex = 0;\n\n      for (let j = 0; j < reduceSize; ++j) {\n        const value = aVals[offset + j];\n\n        if (value < min) {\n          min = value;\n          minIndex = j;\n        }\n      }\n\n      vals[i] = minIndex;\n    }\n\n    return result;\n  }\n\n  argMax(x, axis) {\n    assertNotComplex(x, 'argMax');\n    const axes = [axis];\n    backend_util.assertAxesAreInnerMostDims('argMax', axes, x.rank);\n\n    const _backend_util$compute7 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute8 = _slicedToArray(_backend_util$compute7, 2),\n          outShape = _backend_util$compute8[0],\n          reduceShape = _backend_util$compute8[1];\n\n    const result = tf.zeros(outShape, 'int32');\n    const reduceSize = util.sizeFromShape(reduceShape);\n    const vals = this.readSync(result.dataId);\n    const aVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < vals.length; ++i) {\n      const offset = i * reduceSize;\n      let max = aVals[offset];\n      let maxIndex = 0;\n\n      for (let j = 0; j < reduceSize; ++j) {\n        const value = aVals[offset + j];\n\n        if (value > max) {\n          max = value;\n          maxIndex = j;\n        }\n      }\n\n      vals[i] = maxIndex;\n    }\n\n    return result;\n  }\n\n  cumsum(x, axis, exclusive, reverse) {\n    assertNotComplex(x, 'cumsum');\n\n    if (axis !== x.rank - 1) {\n      throw new Error(\"backend.cumsum in CPU expects an inner-most axis=\".concat(x.rank - 1, \" \") + \"but got axis=\".concat(axis));\n    }\n\n    const resultDtype = upcastType(x.dtype, 'int32');\n    const result = tf.zeros(x.shape, resultDtype);\n    const vals = this.readSync(result.dataId);\n    const aVals = this.readSync(x.dataId);\n    const finalDim = x.shape[x.rank - 1];\n    const indexAdjuster = reverse ? (i, j) => i + finalDim - j - 1 : (i, j) => i + j;\n\n    for (let i = 0; i < aVals.length; i += finalDim) {\n      for (let j = 0; j < finalDim; j++) {\n        const idx = indexAdjuster(i, j);\n\n        if (j === 0) {\n          vals[idx] = exclusive ? 0 : aVals[idx];\n        } else {\n          const prevIdx = indexAdjuster(i, j - 1);\n          vals[idx] = exclusive ? aVals[prevIdx] + vals[prevIdx] : aVals[idx] + vals[prevIdx];\n        }\n      }\n    }\n\n    return result;\n  }\n\n  equal(a, b) {\n    assertNotComplex([a, b], 'equal');\n    return this.broadcastedBinaryOp(a, b, 'bool', (aVal, bVal) => {\n      return aVal === bVal ? 1 : 0;\n    });\n  }\n\n  notEqual(a, b) {\n    assertNotComplex([a, b], 'notEqual');\n    return this.broadcastedBinaryOp(a, b, 'bool', (aVal, bVal) => {\n      return aVal !== bVal ? 1 : 0;\n    });\n  }\n\n  less(a, b) {\n    assertNotComplex([a, b], 'less');\n    return this.broadcastedBinaryOp(a, b, 'bool', (aVal, bVal) => {\n      return aVal < bVal ? 1 : 0;\n    });\n  }\n\n  lessEqual(a, b) {\n    assertNotComplex([a, b], 'lessEqual');\n    return this.broadcastedBinaryOp(a, b, 'bool', (aVal, bVal) => {\n      return aVal <= bVal ? 1 : 0;\n    });\n  }\n\n  greater(a, b) {\n    assertNotComplex([a, b], 'greater');\n    return this.broadcastedBinaryOp(a, b, 'bool', (aVal, bVal) => {\n      return aVal > bVal ? 1 : 0;\n    });\n  }\n\n  greaterEqual(a, b) {\n    assertNotComplex([a, b], 'greaterEqual');\n    return this.broadcastedBinaryOp(a, b, 'bool', (aVal, bVal) => {\n      return aVal >= bVal ? 1 : 0;\n    });\n  }\n\n  logicalNot(x) {\n    assertNotComplex(x, 'logicalNot');\n    const values = this.readSync(x.dataId);\n    const newValues = new Uint8Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      newValues[i] = values[i] ? 0 : 1;\n    }\n\n    return this.makeOutput(newValues, x.shape, 'bool');\n  }\n\n  logicalAnd(a, b) {\n    assertNotComplex([a, b], 'logicalAnd');\n    return this.broadcastedBinaryOp(a, b, 'bool', (aVal, bVal) => {\n      return aVal && bVal;\n    });\n  }\n\n  logicalOr(a, b) {\n    assertNotComplex([a, b], 'logicalOr');\n    return this.broadcastedBinaryOp(a, b, 'bool', (aVal, bVal) => {\n      return aVal || bVal;\n    });\n  }\n\n  select(condition, a, b) {\n    assertNotComplex([condition, a, b], 'select');\n    const values = this.readSync(condition.dataId);\n    const aValues = this.readSync(a.dataId);\n    const bValues = this.readSync(b.dataId);\n    const result = tf.zeros(a.shape, upcastType(a.dtype, b.dtype));\n    const newValues = this.readSync(result.dataId);\n    let index = 0;\n    const offset = condition.rank === 0 || condition.rank > 1 || a.rank === 1 ? 1 : util.sizeFromShape(a.shape.slice(1));\n\n    for (let i = 0; i < values.length; i++) {\n      for (let j = 0; j < offset; j++) {\n        if (values[i] === 1) {\n          newValues[index++] = aValues[i];\n        } else {\n          newValues[index++] = bValues[i];\n        }\n      }\n    }\n\n    return result;\n  }\n\n  where(condition) {\n    assertNotComplex([condition], 'where');\n    const condVals = this.readSync(condition.dataId);\n    return whereImpl(condition.shape, condVals);\n  }\n\n  topk(x, k, sorted) {\n    assertNotComplex(x, 'topk');\n    const xVals = this.readSync(x.dataId);\n    return topkImpl(xVals, x.shape, x.dtype, k, sorted);\n  }\n\n  min(x, axes) {\n    assertNotComplex(x, 'min');\n    backend_util.assertAxesAreInnerMostDims('min', axes, x.rank);\n\n    const _backend_util$compute9 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute10 = _slicedToArray(_backend_util$compute9, 2),\n          outShape = _backend_util$compute10[0],\n          reduceShape = _backend_util$compute10[1];\n\n    const result = tf.zeros(outShape, x.dtype);\n    const reduceSize = util.sizeFromShape(reduceShape);\n    const vals = this.readSync(result.dataId);\n    const aVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < vals.length; ++i) {\n      const offset = i * reduceSize;\n      let min = aVals[offset];\n\n      for (let j = 0; j < reduceSize; ++j) {\n        const value = aVals[offset + j];\n\n        if (value < min) {\n          min = value;\n        }\n      }\n\n      vals[i] = min;\n    }\n\n    return result;\n  }\n\n  minimum(a, b) {\n    assertNotComplex([a, b], 'minimum');\n    return this.broadcastedBinaryOp(a, b, a.dtype, (aVal, bVal) => Math.min(aVal, bVal));\n  }\n\n  mod(a, b) {\n    assertNotComplex([a, b], 'mod');\n    return this.broadcastedBinaryOp(a, b, a.dtype, (aVal, bVal) => {\n      const rem = aVal % bVal;\n\n      if (aVal < 0 && bVal < 0 || aVal >= 0 && bVal >= 0) {\n        return rem;\n      } else {\n        return (rem + bVal) % bVal;\n      }\n    });\n  }\n\n  maximum(a, b) {\n    assertNotComplex([a, b], 'maximum');\n    return this.broadcastedBinaryOp(a, b, a.dtype, (aVal, bVal) => Math.max(aVal, bVal));\n  }\n\n  all(x, axes) {\n    assertNotComplex(x, 'all');\n    backend_util.assertAxesAreInnerMostDims('all', axes, x.rank);\n\n    const _backend_util$compute11 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute12 = _slicedToArray(_backend_util$compute11, 2),\n          outShape = _backend_util$compute12[0],\n          reduceShape = _backend_util$compute12[1];\n\n    const result = tf.zeros(outShape, x.dtype);\n    const reduceSize = util.sizeFromShape(reduceShape);\n    const vals = this.readSync(result.dataId);\n    const aVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < vals.length; ++i) {\n      const offset = i * reduceSize;\n      let all = aVals[offset];\n\n      for (let j = 0; j < reduceSize; ++j) {\n        const value = aVals[offset + j];\n        all = all && value;\n      }\n\n      vals[i] = all;\n    }\n\n    return result;\n  }\n\n  any(x, axes) {\n    assertNotComplex(x, 'any');\n    backend_util.assertAxesAreInnerMostDims('any', axes, x.rank);\n\n    const _backend_util$compute13 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute14 = _slicedToArray(_backend_util$compute13, 2),\n          outShape = _backend_util$compute14[0],\n          reduceShape = _backend_util$compute14[1];\n\n    const result = tf.zeros(outShape, x.dtype);\n    const reduceSize = util.sizeFromShape(reduceShape);\n    const vals = this.readSync(result.dataId);\n    const aVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < vals.length; ++i) {\n      const offset = i * reduceSize;\n      let anyVal = aVals[offset];\n\n      for (let j = 0; j < reduceSize; ++j) {\n        const value = aVals[offset + j];\n        anyVal = anyVal || value;\n      }\n\n      vals[i] = anyVal;\n    }\n\n    return result;\n  }\n\n  squaredDifference(a, b) {\n    assertNotComplex([a, b], 'squaredDifference');\n    return this.broadcastedBinaryOp(a, b, a.dtype, (aVal, bVal) => {\n      const diff = aVal - bVal;\n      return diff * diff;\n    });\n  }\n\n  ceil(x) {\n    assertNotComplex(x, 'ceil');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      newValues[i] = Math.ceil(values[i]);\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  floor(x) {\n    assertNotComplex(x, 'floor');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      newValues[i] = Math.floor(values[i]);\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  sign(x) {\n    assertNotComplex(x, 'x');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      if (values[i] < 0) {\n        newValues[i] = -1;\n      } else if (values[i] > 0) {\n        newValues[i] = 1;\n      } else {\n        newValues[i] = 0;\n      }\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  isNaN(x) {\n    assertNotComplex(x, 'x');\n    const values = this.readSync(x.dataId);\n    const newValues = new Uint8Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      if (Number.isNaN(values[i])) {\n        newValues[i] = 1;\n      }\n    }\n\n    return this.makeOutput(newValues, x.shape, 'bool');\n  }\n\n  isInf(x) {\n    assertNotComplex(x, 'x');\n    const values = this.readSync(x.dataId);\n    const newValues = new Uint8Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      if (Math.abs(values[i]) === Infinity) {\n        newValues[i] = 1;\n      }\n    }\n\n    return this.makeOutput(newValues, x.shape, 'bool');\n  }\n\n  isFinite(x) {\n    assertNotComplex(x, 'x');\n    const values = this.readSync(x.dataId);\n    const newValues = new Uint8Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      if (Number.isFinite(values[i])) {\n        newValues[i] = 1;\n      }\n    }\n\n    return this.makeOutput(newValues, x.shape, 'bool');\n  }\n\n  round(x) {\n    assertNotComplex(x, 'round');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      // The algorithm is based on banker's rounding.\n      const base = Math.floor(values[i]);\n\n      if (values[i] - base < 0.5) {\n        newValues[i] = Math.floor(values[i]);\n      } else if (values[i] - base > 0.5) {\n        newValues[i] = Math.ceil(values[i]);\n      } else {\n        if (base % 2.0 === 0.0) {\n          newValues[i] = base;\n        } else {\n          newValues[i] = base + 1.0;\n        }\n      }\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  exp(x) {\n    assertNotComplex(x, 'exp');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      newValues[i] = Math.exp(values[i]);\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  expm1(x) {\n    assertNotComplex(x, 'expm1');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      newValues[i] = Math.expm1(values[i]);\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  log(x) {\n    assertNotComplex(x, 'log');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      const value = values[i];\n      newValues[i] = Math.log(value);\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  log1p(x) {\n    assertNotComplex(x, 'log1p');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      const value = values[i];\n      newValues[i] = Math.log1p(value);\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  sqrt(x) {\n    assertNotComplex(x, 'sqrt');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      const value = values[i];\n      newValues[i] = Math.sqrt(value);\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  rsqrt(x) {\n    assertNotComplex(x, 'rsqrt');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      const value = values[i];\n      newValues[i] = 1 / Math.sqrt(value);\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  reciprocal(x) {\n    assertNotComplex(x, 'reciprocal');\n    const values = this.readSync(x.dataId);\n    const newValues = new Float32Array(values.length);\n\n    for (let i = 0; i < values.length; ++i) {\n      newValues[i] = 1 / values[i];\n    }\n\n    return this.makeOutput(newValues, x.shape, 'float32');\n  }\n\n  linear(x) {\n    return x;\n  }\n\n  relu(x) {\n    assertNotComplex(x, 'relu');\n    const res = tf.zeros(x.shape, x.dtype);\n    const resVals = this.readSync(res.dataId);\n    const inVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < inVals.length; ++i) {\n      resVals[i] = Math.max(0, inVals[i]);\n    }\n\n    return res;\n  }\n\n  relu6(x) {\n    assertNotComplex(x, 'relu');\n    const res = tf.zeros(x.shape, x.dtype);\n    const resVals = this.readSync(res.dataId);\n    const inVals = this.readSync(x.dataId);\n\n    for (let i = 0; i < inVals.length; ++i) {\n      resVals[i] = Math.min(Math.max(0, inVals[i]), 6);\n    }\n\n    return res;\n  }\n\n  prelu(x, a) {\n    assertNotComplex([x, a], 'prelu');\n    return this.broadcastedBinaryOp(x, a, x.dtype, (xValue, aValue) => xValue < 0 ? aValue * xValue : xValue);\n  }\n\n  elu(x) {\n    assertNotComplex(x, 'elu');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      const v = values[i];\n\n      if (v >= 0) {\n        resultValues[i] = v;\n      } else {\n        resultValues[i] = Math.exp(v) - 1;\n      }\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  eluDer(dy, y) {\n    assertNotComplex([dy, y], 'eluDer');\n    const resultValues = new Float32Array(y.size);\n    const values = this.readSync(y.dataId);\n    const dyValues = this.readSync(dy.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      const v = values[i];\n\n      if (v >= 1) {\n        resultValues[i] = dyValues[i];\n      } else {\n        resultValues[i] = dyValues[i] * (v + 1);\n      }\n    }\n\n    return this.makeOutput(resultValues, y.shape, 'float32');\n  }\n\n  selu(x) {\n    assertNotComplex(x, 'selu'); // Stable and Attracting Fixed Point (0, 1) for Normalized Weights.\n    // see: https://arxiv.org/abs/1706.02515\n\n    const scaleAlpha = backend_util.SELU_SCALEALPHA;\n    const scale = backend_util.SELU_SCALE;\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      const v = values[i];\n\n      if (v >= 0) {\n        resultValues[i] = scale * v;\n      } else {\n        resultValues[i] = scaleAlpha * (Math.exp(v) - 1);\n      }\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  clip(x, min, max) {\n    assertNotComplex(x, 'clip');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      const v = values[i];\n      resultValues[i] = v > max ? max : v < min ? min : v;\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  abs(x) {\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.abs(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  complexAbs(x) {\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < x.size; ++i) {\n      const real = values[i * 2];\n      const imag = values[i * 2 + 1];\n      resultValues[i] = Math.hypot(real, imag);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  int(x) {\n    assertNotComplex(x, 'int');\n    const resultValues = new Int32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = values[i];\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'int32');\n  }\n\n  sigmoid(x) {\n    assertNotComplex(x, 'sigmoid');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = 1 / (1 + Math.exp(-values[i]));\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  softplus(x) {\n    assertNotComplex(x, 'softplus'); // mirrors the implementation of tf.nn.softplus: https://goo.gl/vkcvwX\n    // epsilon is the difference between 1.0 and the next representable float.\n    // For a single precision 32 bit float this should be 2^-23, see:\n    // https://math.byu.edu/~schow/work/IEEEFloatingPoint.htm\n\n    const epsilon = 1.1920928955078125e-7;\n    const threshold = Math.log(epsilon) + 2.0;\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      // Value above which exp(x) may overflow, but softplus(x) == x\n      // is within machine epsilon.\n      const tooLarge = values[i] > -threshold; // Value below which exp(x) may underflow, but softplus(x) == exp(x)\n      // is within machine epsilon.\n\n      const tooSmall = values[i] < threshold;\n      const expX = Math.exp(values[i]);\n      let result;\n\n      if (tooSmall) {\n        result = expX;\n      } else if (tooLarge) {\n        result = values[i];\n      } else {\n        result = Math.log(1.0 + expX);\n      }\n\n      resultValues[i] = result;\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  sin(x) {\n    assertNotComplex(x, 'sin');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.sin(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  cos(x) {\n    assertNotComplex(x, 'cos');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.cos(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  tan(x) {\n    assertNotComplex(x, 'tan');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.tan(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  asin(x) {\n    assertNotComplex(x, 'asin');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.asin(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  acos(x) {\n    assertNotComplex(x, 'acos');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.acos(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  atan(x) {\n    assertNotComplex(x, 'atan');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.atan(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  atan2(a, b) {\n    assertNotComplex([a, b], 'atan2');\n    return this.broadcastedBinaryOp(a, b, a.dtype, (aValue, bValue) => Math.atan2(aValue, bValue));\n  }\n\n  sinh(x) {\n    assertNotComplex(x, 'sinh');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.sinh(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  cosh(x) {\n    assertNotComplex(x, 'cosh');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.cosh(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  tanh(x) {\n    assertNotComplex(x, 'tanh');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = util.tanh(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  asinh(x) {\n    assertNotComplex(x, 'asinh');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.asinh(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  acosh(x) {\n    assertNotComplex(x, 'acosh');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.acosh(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  atanh(x) {\n    assertNotComplex(x, 'atanh');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      resultValues[i] = Math.atanh(values[i]);\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  erf(x) {\n    assertNotComplex(x, 'erf');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n    const p = backend_util.ERF_P;\n    const a1 = backend_util.ERF_A1;\n    const a2 = backend_util.ERF_A2;\n    const a3 = backend_util.ERF_A3;\n    const a4 = backend_util.ERF_A4;\n    const a5 = backend_util.ERF_A5;\n\n    for (let i = 0; i < values.length; ++i) {\n      const sign = Math.sign(values[i]);\n      const v = Math.abs(values[i]);\n      const t = 1.0 / (1.0 + p * v);\n      resultValues[i] = sign * (1.0 - ((((a5 * t + a4) * t + a3) * t + a2) * t + a1) * t * Math.exp(-v * v));\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  step(x, alpha = 0) {\n    assertNotComplex(x, 'step');\n    const resultValues = new Float32Array(x.size);\n    const values = this.readSync(x.dataId);\n\n    for (let i = 0; i < values.length; ++i) {\n      const value = values[i];\n\n      if (isNaN(value)) {\n        resultValues[i] = NaN;\n      } else {\n        resultValues[i] = value > 0 ? 1 : alpha;\n      }\n    }\n\n    return this.makeOutput(resultValues, x.shape, 'float32');\n  }\n\n  fusedConv2d({\n    input,\n    filter,\n    convInfo,\n    bias,\n    activation,\n    preluActivationWeights\n  }) {\n    let result = this.conv2d(input, filter, convInfo);\n\n    if (bias) {\n      result = this.add(result, bias);\n    }\n\n    if (activation) {\n      result = mapActivation(this, result, activation, preluActivationWeights);\n    }\n\n    return result;\n  }\n\n  conv2d(x, filter, convInfo) {\n    assertNotComplex([x, filter], 'conv2d');\n    const filterHeight = convInfo.filterHeight;\n    const filterWidth = convInfo.filterWidth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const padLeft = convInfo.padInfo.left;\n    const padTop = convInfo.padInfo.top;\n    const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    const y = tf.buffer(convInfo.outShape, x.dtype);\n    const xBatchStride = x.strides[0];\n    const xRowStride = isChannelsLast ? x.strides[1] : x.strides[2];\n    const xColStride = isChannelsLast ? x.strides[2] : 1;\n    const xChannelStride = isChannelsLast ? 1 : x.strides[1];\n    const yBatchStride = y.strides[0];\n    const yRowStride = isChannelsLast ? y.strides[1] : y.strides[2];\n    const yColStride = isChannelsLast ? y.strides[2] : 1;\n    const yChannelStride = isChannelsLast ? 1 : y.strides[1];\n    const xVals = this.readSync(x.dataId);\n    const wVals = this.readSync(filter.dataId);\n    const yVals = y.values;\n\n    for (let b = 0; b < convInfo.batchSize; ++b) {\n      const xOffset1 = b * xBatchStride;\n      const yOffset1 = b * yBatchStride;\n\n      for (let yR = 0; yR < convInfo.outHeight; ++yR) {\n        const yOffset2 = yOffset1 + yR * yRowStride;\n        const xRCorner = yR * convInfo.strideHeight - padTop;\n\n        for (let wR = 0; wR < filterHeight; wR++) {\n          const xR = xRCorner + wR * dilationHeight;\n\n          if (xR < 0 || xR >= convInfo.inHeight) {\n            continue;\n          }\n\n          const wOffset1 = wR * filter.strides[0];\n          const xOffset2 = xOffset1 + xR * xRowStride;\n\n          for (let yC = 0; yC < convInfo.outWidth; ++yC) {\n            const yOffset3 = yOffset2 + yC * yColStride;\n            const xCCorner = yC * convInfo.strideWidth - padLeft;\n\n            for (let wC = 0; wC < filterWidth; wC++) {\n              const xC = xCCorner + wC * dilationWidth;\n\n              if (xC < 0 || xC >= convInfo.inWidth) {\n                continue;\n              }\n\n              const wOffset2 = wOffset1 + wC * filter.strides[1];\n              const xOffset3 = xOffset2 + xC * xColStride;\n              let wOffset3 = wOffset2;\n\n              for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n                const xVal = xVals[xOffset3 + d1 * xChannelStride];\n\n                for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n                  yVals[yOffset3 + d2 * yChannelStride] += xVal * wVals[wOffset3 + d2];\n                }\n\n                wOffset3 += convInfo.outChannels;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    return y.toTensor();\n  }\n\n  conv3d(x, filter, convInfo) {\n    const filterDepth = convInfo.filterDepth;\n    const filterHeight = convInfo.filterHeight;\n    const filterWidth = convInfo.filterWidth;\n    const dilationDepth = convInfo.dilationDepth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const padFront = convInfo.padInfo.front;\n    const padLeft = convInfo.padInfo.left;\n    const padTop = convInfo.padInfo.top;\n    const y = tf.buffer(convInfo.outShape, x.dtype);\n    const xVals = this.readSync(x.dataId);\n    const wVals = this.readSync(filter.dataId);\n    const yVals = y.values;\n\n    for (let b = 0; b < convInfo.batchSize; ++b) {\n      const xOffset1 = b * x.strides[0];\n      const yOffset1 = b * y.strides[0];\n\n      for (let yF = 0; yF < convInfo.outDepth; ++yF) {\n        const yOffset2 = yOffset1 + yF * y.strides[1];\n        const xFCorner = yF * convInfo.strideDepth - padFront;\n\n        for (let wF = 0; wF < filterDepth; wF++) {\n          const xF = xFCorner + wF * dilationDepth;\n\n          if (xF < 0 || xF >= convInfo.inDepth) {\n            continue;\n          }\n\n          const wOffset1 = wF * filter.strides[0];\n          const xOffset2 = xOffset1 + xF * x.strides[1];\n\n          for (let yR = 0; yR < convInfo.outHeight; ++yR) {\n            const yOffset3 = yOffset2 + yR * y.strides[2];\n            const xRCorner = yR * convInfo.strideHeight - padTop;\n\n            for (let wR = 0; wR < filterHeight; wR++) {\n              const xR = xRCorner + wR * dilationHeight;\n\n              if (xR < 0 || xR >= convInfo.inHeight) {\n                continue;\n              }\n\n              const wOffset2 = wOffset1 + wR * filter.strides[1];\n              const xOffset3 = xOffset2 + xR * x.strides[2];\n\n              for (let yC = 0; yC < convInfo.outWidth; ++yC) {\n                const yOffset4 = yOffset3 + yC * convInfo.outChannels;\n                const xCCorner = yC * convInfo.strideWidth - padLeft;\n\n                for (let wC = 0; wC < filterWidth; wC++) {\n                  const xC = xCCorner + wC * dilationWidth;\n\n                  if (xC < 0 || xC >= convInfo.inWidth) {\n                    continue;\n                  }\n\n                  const wOffset3 = wOffset2 + wC * filter.strides[2];\n                  const xOffset4 = xOffset3 + xC * convInfo.inChannels;\n                  let wOffset4 = wOffset3;\n\n                  for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n                    const xVal = xVals[xOffset4 + d1];\n\n                    for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n                      yVals[yOffset4 + d2] += xVal * wVals[wOffset4 + d2];\n                    }\n\n                    wOffset4 += convInfo.outChannels;\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    return y.toTensor();\n  }\n\n  conv2dDerInput(dy, filter, convInfo) {\n    assertNotComplex([dy, filter], 'conv2dDerInput');\n    const dx = tf.buffer(convInfo.inShape, 'float32');\n    const dxValues = dx.values;\n    const dyValues = this.readSync(dy.dataId);\n    const fltValues = this.readSync(filter.dataId);\n\n    const _filter$strides = _slicedToArray(filter.strides, 3),\n          fltS0 = _filter$strides[0],\n          fltS1 = _filter$strides[1],\n          fltS2 = _filter$strides[2];\n\n    const batchSize = convInfo.batchSize,\n          filterHeight = convInfo.filterHeight,\n          filterWidth = convInfo.filterWidth,\n          inChannels = convInfo.inChannels,\n          inHeight = convInfo.inHeight,\n          inWidth = convInfo.inWidth,\n          outChannels = convInfo.outChannels,\n          outHeight = convInfo.outHeight,\n          outWidth = convInfo.outWidth,\n          strideHeight = convInfo.strideHeight,\n          strideWidth = convInfo.strideWidth,\n          dataFormat = convInfo.dataFormat;\n    const topPad = filterHeight - 1 - convInfo.padInfo.top;\n    const leftPad = filterWidth - 1 - convInfo.padInfo.left;\n    const isChannelsLast = dataFormat === 'channelsLast';\n    const xBatchStride = dx.strides[0];\n    const xRowStride = isChannelsLast ? dx.strides[1] : dx.strides[2];\n    const xColStride = isChannelsLast ? dx.strides[2] : 1;\n    const xChannelStride = isChannelsLast ? 1 : dx.strides[1];\n    const yBatchStride = dy.strides[0];\n    const yRowStride = isChannelsLast ? dy.strides[1] : dy.strides[2];\n    const yColStride = isChannelsLast ? dy.strides[2] : 1;\n    const yChannelStride = isChannelsLast ? 1 : dy.strides[1];\n\n    for (let b = 0; b < batchSize; ++b) {\n      for (let d1 = 0; d1 < inChannels; ++d1) {\n        for (let xR = 0; xR < inHeight; ++xR) {\n          const xRCorner = xR - topPad;\n          const xRMin = Math.max(0, Math.ceil(xRCorner / strideHeight));\n          const yRMax = Math.min(outHeight, (filterHeight + xRCorner) / strideHeight);\n\n          for (let xC = 0; xC < inWidth; ++xC) {\n            const xCCorner = xC - leftPad;\n            const xCMin = Math.max(0, Math.ceil(xCCorner / strideWidth));\n            const yCMax = Math.min(outWidth, (filterWidth + xCCorner) / strideWidth);\n            let dotProd = 0;\n\n            for (let yR = xRMin; yR < yRMax; ++yR) {\n              const wR = yR * strideHeight - xRCorner;\n\n              for (let yC = xCMin; yC < yCMax; ++yC) {\n                const wC = yC * strideWidth - xCCorner;\n                const dyOffset = yBatchStride * b + yRowStride * yR + yColStride * yC;\n                const fltOffset = fltS0 * (filterHeight - 1 - wR) + fltS1 * (filterWidth - 1 - wC) + fltS2 * d1;\n\n                for (let d2 = 0; d2 < outChannels; ++d2) {\n                  const pixel = dyValues[dyOffset + yChannelStride * d2];\n                  const weight = fltValues[fltOffset + d2];\n                  dotProd += pixel * weight;\n                }\n              }\n            }\n\n            const dxOffset = xBatchStride * b + xRowStride * xR + xColStride * xC + xChannelStride * d1;\n            dxValues[dxOffset] = dotProd;\n          }\n        }\n      }\n    }\n\n    return dx.toTensor();\n  }\n\n  conv3dDerInput(dy, filter, convInfo) {\n    const dx = tf.buffer(convInfo.inShape, 'float32');\n    const dxValues = dx.values;\n\n    const _dx$strides = _slicedToArray(dx.strides, 4),\n          dxS0 = _dx$strides[0],\n          dxS1 = _dx$strides[1],\n          dxS2 = _dx$strides[2],\n          dxS3 = _dx$strides[3];\n\n    const dyValues = this.readSync(dy.dataId);\n\n    const _dy$strides = _slicedToArray(dy.strides, 4),\n          dyS0 = _dy$strides[0],\n          dyS1 = _dy$strides[1],\n          dyS2 = _dy$strides[2],\n          dyS3 = _dy$strides[3];\n\n    const fltValues = this.readSync(filter.dataId);\n\n    const _filter$strides2 = _slicedToArray(filter.strides, 4),\n          fltS0 = _filter$strides2[0],\n          fltS1 = _filter$strides2[1],\n          fltS2 = _filter$strides2[2],\n          fltS3 = _filter$strides2[3];\n\n    const batchSize = convInfo.batchSize,\n          filterDepth = convInfo.filterDepth,\n          filterHeight = convInfo.filterHeight,\n          filterWidth = convInfo.filterWidth,\n          inChannels = convInfo.inChannels,\n          inDepth = convInfo.inDepth,\n          inHeight = convInfo.inHeight,\n          inWidth = convInfo.inWidth,\n          outChannels = convInfo.outChannels,\n          outDepth = convInfo.outDepth,\n          outHeight = convInfo.outHeight,\n          outWidth = convInfo.outWidth,\n          strideDepth = convInfo.strideDepth,\n          strideHeight = convInfo.strideHeight,\n          strideWidth = convInfo.strideWidth;\n    const frontPad = filterDepth - 1 - convInfo.padInfo.front;\n    const topPad = filterHeight - 1 - convInfo.padInfo.top;\n    const leftPad = filterWidth - 1 - convInfo.padInfo.left;\n\n    for (let b = 0; b < batchSize; ++b) {\n      for (let d1 = 0; d1 < inChannels; ++d1) {\n        // Frames of depth\n        for (let xF = 0; xF < inDepth; ++xF) {\n          const xFCorner = xF - frontPad;\n          const xFMin = Math.max(0, Math.ceil(xFCorner / strideDepth));\n          const yFMax = Math.min(outDepth, (filterDepth + xFCorner) / strideDepth); // Rows as per standard 2d matrix notation\n\n          for (let xR = 0; xR < inHeight; ++xR) {\n            const xRCorner = xR - topPad;\n            const xRMin = Math.max(0, Math.ceil(xRCorner / strideHeight));\n            const yRMax = Math.min(outHeight, (filterHeight + xRCorner) / strideHeight); // Columns as per standard 2d matrix notation\n\n            for (let xC = 0; xC < inWidth; ++xC) {\n              const xCCorner = xC - leftPad;\n              const xCMin = Math.max(0, Math.ceil(xCCorner / strideWidth));\n              const yCMax = Math.min(outWidth, (filterWidth + xCCorner) / strideWidth);\n              let dotProd = 0;\n\n              for (let yF = xFMin; yF < yFMax; ++yF) {\n                const wF = yF * strideDepth - xFCorner;\n\n                for (let yR = xRMin; yR < yRMax; ++yR) {\n                  const wR = yR * strideHeight - xRCorner;\n\n                  for (let yC = xCMin; yC < yCMax; ++yC) {\n                    const wC = yC * strideWidth - xCCorner;\n                    const dyOffset = dyS0 * b + dyS1 * yF + dyS2 * yR + dyS3 * yC;\n                    const fltOffset = fltS0 * (filterDepth - 1 - wF) + fltS1 * (filterHeight - 1 - wR) + fltS2 * (filterWidth - 1 - wC) + fltS3 * d1;\n\n                    for (let d2 = 0; d2 < outChannels; ++d2) {\n                      const pixel = dyValues[dyOffset + d2];\n                      const weight = fltValues[fltOffset + d2];\n                      dotProd += pixel * weight;\n                    }\n                  }\n                }\n              }\n\n              dxValues[dxS0 * b + dxS1 * xF + dxS2 * xR + dxS3 * xC + d1] = dotProd;\n            }\n          }\n        }\n      }\n    }\n\n    return dx.toTensor();\n  }\n\n  conv2dDerFilter(x, dy, convInfo) {\n    assertNotComplex([x, dy], 'conv2dDerFilter');\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const filterHeight = convInfo.filterHeight;\n    const filterWidth = convInfo.filterWidth;\n    const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    const dW = tf.buffer(convInfo.filterShape, 'float32');\n    const leftPad = convInfo.padInfo.left;\n    const topPad = convInfo.padInfo.top;\n    const xBuf = this.bufferSync(x);\n    const dyBuf = this.bufferSync(dy);\n\n    for (let wR = 0; wR < filterHeight; ++wR) {\n      const yRMin = Math.max(0, Math.ceil((topPad - wR) / strideHeight));\n      const yRMax = Math.min(convInfo.outHeight, (convInfo.inHeight + topPad - wR) / strideHeight);\n\n      for (let wC = 0; wC < filterWidth; ++wC) {\n        const yCMin = Math.max(0, Math.ceil((leftPad - wC) / strideWidth));\n        const yCMax = Math.min(convInfo.outWidth, (convInfo.inWidth + leftPad - wC) / strideWidth);\n\n        for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n          for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n            // Need to convolve.\n            let dotProd = 0;\n\n            for (let b = 0; b < convInfo.batchSize; ++b) {\n              for (let yR = yRMin; yR < yRMax; ++yR) {\n                const xR = wR + yR * strideHeight - topPad;\n\n                for (let yC = yCMin; yC < yCMax; ++yC) {\n                  const xC = wC + yC * strideWidth - leftPad;\n\n                  if (isChannelsLast) {\n                    dotProd += xBuf.get(b, xR, xC, d1) * dyBuf.get(b, yR, yC, d2);\n                  } else {\n                    dotProd += xBuf.get(b, d1, xR, xC) * dyBuf.get(b, d2, yR, yC);\n                  }\n                }\n              }\n            }\n\n            dW.set(dotProd, wR, wC, d1, d2);\n          }\n        }\n      }\n    }\n\n    return dW.toTensor();\n  }\n\n  conv3dDerFilter(x, dy, convInfo) {\n    const strideDepth = convInfo.strideDepth;\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const filterDepth = convInfo.filterDepth;\n    const filterHeight = convInfo.filterHeight;\n    const filterWidth = convInfo.filterWidth;\n    const dw = tf.buffer(convInfo.filterShape, 'float32');\n    const dwValues = dw.values;\n\n    const _dw$strides = _slicedToArray(dw.strides, 4),\n          dwS0 = _dw$strides[0],\n          dwS1 = _dw$strides[1],\n          dwS2 = _dw$strides[2],\n          dwS3 = _dw$strides[3];\n\n    const dyValues = this.readSync(dy.dataId);\n\n    const _dy$strides2 = _slicedToArray(dy.strides, 4),\n          dyS0 = _dy$strides2[0],\n          dyS1 = _dy$strides2[1],\n          dyS2 = _dy$strides2[2],\n          dyS3 = _dy$strides2[3];\n\n    const xValues = this.readSync(x.dataId);\n\n    const _x$strides = _slicedToArray(x.strides, 4),\n          xS0 = _x$strides[0],\n          xS1 = _x$strides[1],\n          xS2 = _x$strides[2],\n          xS3 = _x$strides[3];\n\n    const frontPad = convInfo.padInfo.front;\n    const leftPad = convInfo.padInfo.left;\n    const topPad = convInfo.padInfo.top;\n\n    for (let wF = 0; wF < filterDepth; ++wF) {\n      const yFMin = Math.max(0, Math.ceil((frontPad - wF) / strideDepth));\n      const yFMax = Math.min(convInfo.outDepth, (convInfo.inDepth + frontPad - wF) / strideDepth);\n      const wOffset1 = wF * dwS0;\n\n      for (let wR = 0; wR < filterHeight; ++wR) {\n        const yRMin = Math.max(0, Math.ceil((topPad - wR) / strideHeight));\n        const yRMax = Math.min(convInfo.outHeight, (convInfo.inHeight + topPad - wR) / strideHeight);\n        const wOffset2 = wR * dwS1 + wOffset1;\n\n        for (let wC = 0; wC < filterWidth; ++wC) {\n          const yCMin = Math.max(0, Math.ceil((leftPad - wC) / strideWidth));\n          const yCMax = Math.min(convInfo.outWidth, (convInfo.inWidth + leftPad - wC) / strideWidth);\n          const wOffset3 = wC * dwS2 + wOffset2;\n\n          for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n            const wOffset4 = d1 * dwS3 + wOffset3;\n\n            for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n              let dotProd = 0;\n\n              for (let b = 0; b < convInfo.batchSize; ++b) {\n                const xOffset1 = b * xS0;\n                const yOffset1 = b * dyS0;\n\n                for (let yF = yFMin; yF < yFMax; ++yF) {\n                  const xF = wF + yF * strideDepth - frontPad;\n                  const xOffset2 = xF * xS1 + xOffset1;\n                  const yOffset2 = yF * dyS1 + yOffset1;\n\n                  for (let yR = yRMin; yR < yRMax; ++yR) {\n                    const xR = wR + yR * strideHeight - topPad;\n                    const xOffset3 = xR * xS2 + xOffset2;\n                    const yOffset3 = yR * dyS2 + yOffset2;\n\n                    for (let yC = yCMin; yC < yCMax; ++yC) {\n                      const xC = wC + yC * strideWidth - leftPad;\n                      const xOffset4 = xC * xS3 + xOffset3;\n                      const yOffset4 = yC * dyS3 + yOffset3;\n                      dotProd += xValues[xOffset4 + d1] * dyValues[yOffset4 + d2];\n                    }\n                  }\n                }\n              }\n\n              dwValues[wOffset4 + d2] = dotProd;\n            }\n          }\n        }\n      }\n    }\n\n    return dw.toTensor();\n  }\n\n  fusedDepthwiseConv2D({\n    input,\n    filter,\n    convInfo,\n    bias,\n    activation,\n    preluActivationWeights\n  }) {\n    let result = this.depthwiseConv2D(input, filter, convInfo);\n\n    if (bias) {\n      result = this.add(result, bias);\n    }\n\n    if (activation) {\n      result = mapActivation(this, result, activation, preluActivationWeights);\n    }\n\n    return result;\n  }\n\n  depthwiseConv2D(x, filter, convInfo) {\n    assertNotComplex([x, filter], 'depthwiseConv2D');\n    const filterHeight = convInfo.filterHeight;\n    const filterWidth = convInfo.filterWidth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const padLeft = convInfo.padInfo.left;\n    const padTop = convInfo.padInfo.top;\n    const chMul = convInfo.outChannels / convInfo.inChannels;\n    const y = tf.buffer(convInfo.outShape, x.dtype);\n    const xVals = this.readSync(x.dataId);\n    const wVals = this.readSync(filter.dataId);\n    const yVals = y.values;\n\n    for (let b = 0; b < convInfo.batchSize; ++b) {\n      const xOffset1 = b * x.strides[0];\n      const yOffset1 = b * y.strides[0];\n\n      for (let yR = 0; yR < convInfo.outHeight; ++yR) {\n        const yOffset2 = yOffset1 + yR * y.strides[1];\n        const xRCorner = yR * convInfo.strideHeight - padLeft;\n\n        for (let wR = 0; wR < filterHeight; ++wR) {\n          const xR = xRCorner + wR * dilationHeight;\n\n          if (xR < 0 || xR >= convInfo.inHeight) {\n            continue;\n          }\n\n          const wOffset1 = wR * filter.strides[0];\n          const xOffset2 = xOffset1 + xR * x.strides[1];\n\n          for (let yC = 0; yC < convInfo.outWidth; ++yC) {\n            const yOffset3 = yOffset2 + yC * y.strides[2];\n            const xCCorner = yC * convInfo.strideWidth - padTop;\n\n            for (let wC = 0; wC < filterWidth; ++wC) {\n              const xC = xCCorner + wC * dilationWidth;\n\n              if (xC < 0 || xC >= convInfo.inWidth) {\n                continue;\n              }\n\n              const wOffset2 = wOffset1 + wC * filter.strides[1];\n              const xOffset3 = xOffset2 + xC * convInfo.inChannels;\n              let yOffset4 = yOffset3;\n              let wOffset3 = wOffset2;\n\n              for (let d1 = 0; d1 < convInfo.inChannels; ++d1) {\n                const xVal = xVals[xOffset3 + d1];\n\n                for (let q = 0; q < chMul; ++q) {\n                  yVals[yOffset4 + q] += xVal * wVals[wOffset3 + q];\n                }\n\n                yOffset4 += chMul;\n                wOffset3 += chMul;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    return y.toTensor();\n  }\n\n  depthwiseConv2DDerInput(dy, filter, convInfo) {\n    assertNotComplex([dy, filter], 'depthwiseConv2DDerInput');\n    const dx = tf.buffer(convInfo.inShape, 'float32');\n    const dxValues = dx.values;\n\n    const _dx$strides2 = _slicedToArray(dx.strides, 3),\n          dxS0 = _dx$strides2[0],\n          dxS1 = _dx$strides2[1],\n          dxS2 = _dx$strides2[2];\n\n    const dyValues = this.readSync(dy.dataId);\n\n    const _dy$strides3 = _slicedToArray(dy.strides, 3),\n          dyS0 = _dy$strides3[0],\n          dyS1 = _dy$strides3[1],\n          dyS2 = _dy$strides3[2];\n\n    const fltValues = this.readSync(filter.dataId);\n\n    const _filter$strides3 = _slicedToArray(filter.strides, 3),\n          fltS0 = _filter$strides3[0],\n          fltS1 = _filter$strides3[1],\n          fltS2 = _filter$strides3[2];\n\n    const batchSize = convInfo.batchSize,\n          filterHeight = convInfo.filterHeight,\n          filterWidth = convInfo.filterWidth,\n          inChannels = convInfo.inChannels,\n          inHeight = convInfo.inHeight,\n          inWidth = convInfo.inWidth,\n          outChannels = convInfo.outChannels,\n          outHeight = convInfo.outHeight,\n          outWidth = convInfo.outWidth,\n          strideHeight = convInfo.strideHeight,\n          strideWidth = convInfo.strideWidth;\n    const topPad = filterHeight - 1 - convInfo.padInfo.top;\n    const leftPad = filterWidth - 1 - convInfo.padInfo.left;\n    const chMul = outChannels / inChannels;\n\n    for (let b = 0; b < batchSize; ++b) {\n      for (let d1 = 0; d1 < inChannels; ++d1) {\n        for (let xR = 0; xR < inHeight; ++xR) {\n          const xRCorner = xR - topPad;\n          const xRMin = Math.max(0, Math.ceil(xRCorner / strideHeight));\n          const yRMax = Math.min(outHeight, (filterHeight + xRCorner) / strideHeight);\n\n          for (let xC = 0; xC < inWidth; ++xC) {\n            const xCCorner = xC - leftPad;\n            const xCMin = Math.max(0, Math.ceil(xCCorner / strideWidth));\n            const yCMax = Math.min(outWidth, (filterWidth + xCCorner) / strideWidth);\n            let dotProd = 0;\n\n            for (let yR = xRMin; yR < yRMax; ++yR) {\n              const wR = yR * strideHeight - xRCorner;\n\n              for (let yC = xCMin; yC < yCMax; ++yC) {\n                const wC = yC * strideWidth - xCCorner;\n                const dyOffset = dyS0 * b + dyS1 * yR + dyS2 * yC;\n                const fltOffset = fltS0 * (filterHeight - 1 - wR) + fltS1 * (filterWidth - 1 - wC) + fltS2 * d1;\n\n                for (let dm = 0; dm < chMul; ++dm) {\n                  const d2 = d1 * chMul + dm;\n                  const pixel = dyValues[dyOffset + d2];\n                  const weight = fltValues[fltOffset + dm];\n                  dotProd += pixel * weight;\n                }\n              }\n            }\n\n            dxValues[dxS0 * b + dxS1 * xR + dxS2 * xC + d1] = dotProd;\n          }\n        }\n      }\n    }\n\n    return dx.toTensor();\n  }\n\n  depthwiseConv2DDerFilter(x, dy, convInfo) {\n    assertNotComplex([x, dy], 'depthwiseConv2DDerFilter');\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const filterHeight = convInfo.filterHeight;\n    const filterWidth = convInfo.filterWidth;\n    const dW = tf.buffer(convInfo.filterShape, 'float32');\n    const leftPad = convInfo.padInfo.left;\n    const topPad = convInfo.padInfo.top;\n    const chMul = convInfo.outChannels / convInfo.inChannels;\n    const xBuf = this.bufferSync(x);\n    const dyBuf = this.bufferSync(dy);\n\n    for (let wR = 0; wR < filterHeight; ++wR) {\n      const yRMin = Math.max(0, Math.ceil((topPad - wR) / strideHeight));\n      const yRMax = Math.min(convInfo.outHeight, (convInfo.inHeight + topPad - wR) / strideHeight);\n\n      for (let wC = 0; wC < filterWidth; ++wC) {\n        const yCMin = Math.max(0, Math.ceil((leftPad - wC) / strideWidth));\n        const yCMax = Math.min(convInfo.outWidth, (convInfo.inWidth + leftPad - wC) / strideWidth);\n\n        for (let d2 = 0; d2 < convInfo.outChannels; ++d2) {\n          const d1 = Math.trunc(d2 / chMul);\n          const dm = d2 % chMul;\n          let dotProd = 0;\n\n          for (let b = 0; b < convInfo.batchSize; ++b) {\n            for (let yR = yRMin; yR < yRMax; ++yR) {\n              const xR = wR + yR * strideHeight - topPad;\n\n              for (let yC = yCMin; yC < yCMax; ++yC) {\n                const xC = wC + yC * strideWidth - leftPad;\n                dotProd += xBuf.get(b, xR, xC, d1) * dyBuf.get(b, yR, yC, d2);\n              }\n            }\n          }\n\n          dW.set(dotProd, wR, wC, d1, dm);\n        }\n      }\n    }\n\n    return dW.toTensor();\n  }\n\n  tile(x, reps) {\n    assertNotComplex(x, 'tile');\n    return tile(this.bufferSync(x), reps);\n  }\n\n  pad(x, paddings, constantValue) {\n    assertNotComplex(x, 'pad');\n    const outShape = paddings.map((p, i) => p[0]\n    /* beforePad */\n    + x.shape[i] + p[1]\n    /* afterPad */\n    );\n    const start = paddings.map(p => p[0]);\n    const xBuffer = this.bufferSync(x);\n    const buffer = tf.buffer(outShape, x.dtype);\n\n    if (constantValue !== 0) {\n      buffer.values.fill(constantValue);\n    }\n\n    for (let i = 0; i < x.size; i++) {\n      const coords = xBuffer.indexToLoc(i);\n      const outCoords = coords.map((c, i) => c + start[i]);\n      buffer.set(xBuffer.get(...coords), ...outCoords);\n    }\n\n    return buffer.toTensor();\n  }\n\n  gather(x, indices, axis) {\n    assertNotComplex([x, indices], 'gather');\n    const newShape = x.shape.slice();\n    const indicesValues = this.readSync(indices.dataId);\n    newShape[axis] = indicesValues.length;\n    const result = tf.buffer(newShape, x.dtype);\n    const xBuf = this.bufferSync(x);\n\n    for (let i = 0; i < result.size; ++i) {\n      const newLoc = result.indexToLoc(i);\n      const originalLoc = newLoc.slice();\n      originalLoc[axis] = indicesValues[newLoc[axis]];\n      const originalIndex = xBuf.locToIndex(originalLoc);\n      result.values[i] = xBuf.values[originalIndex];\n    }\n\n    return result.toTensor();\n  }\n\n  batchToSpaceND(x, blockShape, crops) {\n    assertNotComplex([x], 'batchToSpaceND');\n    const prod = blockShape.reduce((a, b) => a * b);\n    const reshaped = backend_util.getReshaped(x.shape, blockShape, prod);\n    const permuted = backend_util.getPermuted(reshaped.length, blockShape.length);\n    const reshapedPermuted = backend_util.getReshapedPermuted(x.shape, blockShape, prod);\n    const sliceBeginCoords = backend_util.getSliceBeginCoords(crops, blockShape.length);\n    const sliceSize = backend_util.getSliceSize(reshapedPermuted, crops, blockShape.length);\n    return tf.transpose(x.reshape(reshaped), permuted).reshape(reshapedPermuted).slice(sliceBeginCoords, sliceSize);\n  }\n\n  spaceToBatchND(x, blockShape, paddings) {\n    assertNotComplex([x], 'spaceToBatchND');\n    const prod = blockShape.reduce((a, b) => a * b);\n    const completePaddings = [[0, 0]];\n    completePaddings.push(...paddings);\n\n    for (let i = 1 + blockShape.length; i < x.shape.length; ++i) {\n      completePaddings.push([0, 0]);\n    }\n\n    const paddedX = x.pad(completePaddings);\n    const reshapedPaddedShape = backend_util.getReshaped(paddedX.shape, blockShape, prod, false);\n    const permutedReshapedPaddedPermutation = backend_util.getPermuted(reshapedPaddedShape.length, blockShape.length, false);\n    const flattenShape = backend_util.getReshapedPermuted(paddedX.shape, blockShape, prod, false);\n    const paddedXT = tf.transpose(paddedX.reshape(reshapedPaddedShape), permutedReshapedPaddedPermutation);\n    return reshape(paddedXT, flattenShape);\n  }\n\n  maxPool(x, convInfo) {\n    assertNotComplex(x, 'maxPool');\n    const xValues = this.readSync(x.dataId);\n    return pool(xValues, x.shape, x.dtype, x.strides, convInfo, 'max').toTensor();\n  }\n\n  maxPoolBackprop(dy, x, y, convInfo) {\n    assertNotComplex([x, y], 'maxPoolBackprop');\n    const xValues = this.readSync(x.dataId);\n    const maxPosBuf = buffer(convInfo.outShape, x.dtype, maxPoolPositions(xValues, x.shape, x.dtype, convInfo).values);\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n    const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n    const padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n    const padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n    const dx = tf.buffer(x.shape, 'float32');\n    const dyBuf = this.bufferSync(dy);\n\n    for (let b = 0; b < convInfo.batchSize; ++b) {\n      for (let d = 0; d < convInfo.inChannels; ++d) {\n        for (let dxR = 0; dxR < convInfo.inHeight; ++dxR) {\n          for (let dxC = 0; dxC < convInfo.inWidth; ++dxC) {\n            // Shader code begins.\n            const dyRCorner = dxR - padTop;\n            const dyCCorner = dxC - padLeft;\n            let dotProd = 0;\n\n            for (let wR = 0; wR < effectiveFilterHeight; wR += dilationHeight) {\n              const dyR = (dyRCorner + wR) / strideHeight;\n\n              if (dyR < 0 || dyR >= convInfo.outHeight || Math.floor(dyR) !== dyR) {\n                continue;\n              }\n\n              for (let wC = 0; wC < effectiveFilterWidth; wC += dilationWidth) {\n                const dyC = (dyCCorner + wC) / strideWidth;\n\n                if (dyC < 0 || dyC >= convInfo.outWidth || Math.floor(dyC) !== dyC) {\n                  continue;\n                }\n\n                const maxPos = effectiveFilterHeight * effectiveFilterWidth - 1 - maxPosBuf.get(b, dyR, dyC, d);\n                const curPos = wR * effectiveFilterWidth + wC;\n                const mask = maxPos === curPos ? 1 : 0;\n\n                if (mask === 0) {\n                  continue;\n                }\n\n                const pixel = dyBuf.get(b, dyR, dyC, d);\n                dotProd += pixel * mask;\n              }\n            }\n\n            dx.set(dotProd, b, dxR, dxC, d);\n          }\n        }\n      }\n    }\n\n    return dx.toTensor();\n  }\n\n  avgPoolBackprop(dy, x, convInfo) {\n    assertNotComplex([dy, x], 'avgPoolBackprop');\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const filterHeight = convInfo.filterHeight;\n    const filterWidth = convInfo.filterWidth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n    const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n    const padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n    const padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n    const dx = tf.buffer(x.shape, 'float32');\n    const avgMultiplier = 1 / (filterHeight * filterWidth);\n    const dyBuf = this.bufferSync(dy);\n\n    for (let b = 0; b < convInfo.batchSize; ++b) {\n      for (let d = 0; d < convInfo.inChannels; ++d) {\n        for (let dxR = 0; dxR < convInfo.inHeight; ++dxR) {\n          for (let dxC = 0; dxC < convInfo.inWidth; ++dxC) {\n            // Shader code begins.\n            const dyRCorner = dxR - padTop;\n            const dyCCorner = dxC - padLeft;\n            let dotProd = 0;\n\n            for (let wR = 0; wR < effectiveFilterHeight; wR += dilationHeight) {\n              const dyR = (dyRCorner + wR) / strideHeight;\n\n              if (dyR < 0 || dyR >= convInfo.outHeight || Math.floor(dyR) !== dyR) {\n                continue;\n              }\n\n              for (let wC = 0; wC < effectiveFilterWidth; wC += dilationWidth) {\n                const dyC = (dyCCorner + wC) / strideWidth;\n\n                if (dyC < 0 || dyC >= convInfo.outWidth || Math.floor(dyC) !== dyC) {\n                  continue;\n                }\n\n                const pixel = dyBuf.get(b, dyR, dyC, d);\n                dotProd += pixel;\n              }\n            }\n\n            dx.set(dotProd * avgMultiplier, b, dxR, dxC, d);\n          }\n        }\n      }\n    }\n\n    return dx.toTensor();\n  }\n\n  pool3d(x, convInfo, poolType) {\n    assertNotComplex(x, 'pool3d');\n    const strideDepth = convInfo.strideDepth;\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const dilationDepth = convInfo.dilationDepth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const effectiveFilterDepth = convInfo.effectiveFilterDepth;\n    const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n    const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n    const padFront = convInfo.padInfo.front;\n    const padTop = convInfo.padInfo.top;\n    const padLeft = convInfo.padInfo.left;\n    const initialValue = poolType === 'max' ? Number.NEGATIVE_INFINITY : Number.POSITIVE_INFINITY;\n    const xValues = this.readSync(x.dataId);\n    const output = tf.buffer(convInfo.outShape, x.dtype);\n    const outputVals = output.values;\n    const outputBatchStrides = convInfo.outShape[1] * convInfo.outShape[2] * convInfo.outShape[3] * convInfo.outShape[4];\n    const outputDepthStrides = convInfo.outShape[2] * convInfo.outShape[3] * convInfo.outShape[4];\n    const outputRowStrides = convInfo.outShape[3] * convInfo.outShape[4];\n    const outputColStrides = convInfo.outShape[4];\n\n    for (let batch = 0; batch < convInfo.batchSize; ++batch) {\n      const outputBatchOffset = batch * outputBatchStrides;\n      const inputBatchOffset = batch * x.strides[0];\n\n      for (let channel = 0; channel < convInfo.inChannels; ++channel) {\n        for (let yDepth = 0; yDepth < convInfo.outDepth; ++yDepth) {\n          const xDepthCorner = yDepth * strideDepth - padFront;\n          let xDepthMin = xDepthCorner;\n\n          while (xDepthMin < 0) {\n            xDepthMin += dilationDepth;\n          }\n\n          const xDepthMax = Math.min(convInfo.inDepth, effectiveFilterDepth + xDepthCorner);\n          const outputDepthOffset = outputBatchOffset + yDepth * outputDepthStrides;\n\n          for (let yRow = 0; yRow < convInfo.outHeight; ++yRow) {\n            const xRowCorner = yRow * strideHeight - padTop;\n            let xRowMin = xRowCorner;\n\n            while (xRowMin < 0) {\n              xRowMin += dilationHeight;\n            }\n\n            const xRowMax = Math.min(convInfo.inHeight, effectiveFilterHeight + xRowCorner);\n            const outputRowOffset = outputDepthOffset + yRow * outputRowStrides;\n\n            for (let yCol = 0; yCol < convInfo.outWidth; ++yCol) {\n              const xColCorner = yCol * strideWidth - padLeft;\n              let xColMin = xColCorner;\n\n              while (xColMin < 0) {\n                xColMin += dilationWidth;\n              }\n\n              const xColMax = Math.min(convInfo.inWidth, effectiveFilterWidth + xColCorner); // Shader code begins\n\n              const outputColOffset = outputRowOffset + yCol * outputColStrides;\n              let minMaxValue = initialValue;\n              let avgValue = 0;\n              let count = 0;\n\n              for (let xDepth = xDepthMin; xDepth < xDepthMax; xDepth += dilationDepth) {\n                const xDepthOffset = inputBatchOffset + xDepth * x.strides[1];\n\n                for (let xRow = xRowMin; xRow < xRowMax; xRow += dilationHeight) {\n                  const xRowOffset = xDepthOffset + xRow * x.strides[2];\n\n                  for (let xCol = xColMin; xCol < xColMax; xCol += dilationWidth) {\n                    const xColOffset = xRowOffset + xCol * x.strides[3];\n                    const pixel = xValues[xColOffset + channel];\n\n                    if (poolType === 'max' && pixel > minMaxValue) {\n                      minMaxValue = pixel;\n                    } else if (poolType === 'avg') {\n                      avgValue += pixel;\n                      count++;\n                    }\n\n                    if (isNaN(minMaxValue)) {\n                      break;\n                    }\n                  }\n\n                  if (isNaN(minMaxValue)) {\n                    break;\n                  }\n                }\n\n                if (isNaN(minMaxValue)) {\n                  break;\n                }\n              }\n\n              const outputOffset = outputColOffset + channel;\n              outputVals[outputOffset] = poolType === 'avg' ? avgValue / count : minMaxValue;\n            }\n          }\n        }\n      }\n    }\n\n    return output.toTensor();\n  }\n\n  avgPool3d(x, convInfo) {\n    assertNotComplex(x, 'avgPool3d');\n    return this.pool3d(x, convInfo, 'avg').toFloat();\n  }\n\n  avgPool3dBackprop(dy, x, convInfo) {\n    assertNotComplex([dy, x], 'avgPool3dBackprop');\n    const strideDepth = convInfo.strideDepth;\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const filterDepth = convInfo.filterDepth;\n    const filterHeight = convInfo.filterHeight;\n    const filterWidth = convInfo.filterWidth;\n    const dilationDepth = convInfo.dilationDepth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const effectiveFilterDepth = convInfo.effectiveFilterDepth;\n    const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n    const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n    const padFront = effectiveFilterDepth - 1 - convInfo.padInfo.front;\n    const padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n    const padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n    const dx = tf.buffer(x.shape, 'float32');\n    const avgMultiplier = 1 / (filterDepth * filterHeight * filterWidth);\n    const dyBuf = this.bufferSync(dy);\n\n    for (let batch = 0; batch < convInfo.batchSize; ++batch) {\n      for (let channel = 0; channel < convInfo.inChannels; ++channel) {\n        for (let dxDepth = 0; dxDepth < convInfo.inDepth; ++dxDepth) {\n          for (let dxRow = 0; dxRow < convInfo.inHeight; ++dxRow) {\n            for (let dxCol = 0; dxCol < convInfo.inWidth; ++dxCol) {\n              // Shader code begins.\n              const dyDepthCorner = dxDepth - padFront;\n              const dyRowCorner = dxRow - padTop;\n              const dyColCorner = dxCol - padLeft;\n              let dotProd = 0;\n\n              for (let wDepth = 0; wDepth < effectiveFilterDepth; wDepth += dilationDepth) {\n                const dyDepth = (dyDepthCorner + wDepth) / strideDepth;\n\n                if (dyDepth < 0 || dyDepth >= convInfo.outDepth || Math.floor(dyDepth) !== dyDepth) {\n                  continue;\n                }\n\n                for (let wRow = 0; wRow < effectiveFilterHeight; wRow += dilationHeight) {\n                  const dyRow = (dyRowCorner + wRow) / strideHeight;\n\n                  if (dyRow < 0 || dyRow >= convInfo.outHeight || Math.floor(dyRow) !== dyRow) {\n                    continue;\n                  }\n\n                  for (let wCol = 0; wCol < effectiveFilterWidth; wCol += dilationWidth) {\n                    const dyCol = (dyColCorner + wCol) / strideWidth;\n\n                    if (dyCol < 0 || dyCol >= convInfo.outWidth || Math.floor(dyCol) !== dyCol) {\n                      continue;\n                    }\n\n                    const pixel = dyBuf.get(batch, dyDepth, dyRow, dyCol, channel);\n                    dotProd += pixel;\n                  }\n                }\n              }\n\n              dx.set(dotProd * avgMultiplier, batch, dxDepth, dxRow, dxCol, channel);\n            }\n          }\n        }\n      }\n    }\n\n    return dx.toTensor();\n  }\n\n  maxPool3d(x, convInfo) {\n    assertNotComplex(x, 'maxPool3d');\n    return this.pool3d(x, convInfo, 'max').toFloat();\n  }\n\n  maxPool3dPositions(x, convInfo) {\n    const maxPositions = tf.buffer(convInfo.outShape, 'int32');\n    const strideDepth = convInfo.strideDepth;\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const dilationDepth = convInfo.dilationDepth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const effectiveFilterDepth = convInfo.effectiveFilterDepth;\n    const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n    const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n    const padFront = convInfo.padInfo.front;\n    const padTop = convInfo.padInfo.top;\n    const padLeft = convInfo.padInfo.left;\n    const xBuf = this.bufferSync(x);\n\n    for (let batch = 0; batch < convInfo.batchSize; ++batch) {\n      for (let channel = 0; channel < convInfo.inChannels; ++channel) {\n        for (let yDepth = 0; yDepth < convInfo.outDepth; ++yDepth) {\n          const xDepthCorner = yDepth * strideDepth - padFront;\n          let xDepthMin = xDepthCorner;\n\n          while (xDepthMin < 0) {\n            xDepthMin += dilationDepth;\n          }\n\n          const xDepthMax = Math.min(convInfo.inDepth, effectiveFilterDepth + xDepthCorner);\n\n          for (let yRow = 0; yRow < convInfo.outHeight; ++yRow) {\n            const xRowCorner = yRow * strideHeight - padTop;\n            let xRowMin = xRowCorner;\n\n            while (xRowMin < 0) {\n              xRowMin += dilationHeight;\n            }\n\n            const xRowMax = Math.min(convInfo.inHeight, effectiveFilterHeight + xRowCorner);\n\n            for (let yCol = 0; yCol < convInfo.outWidth; ++yCol) {\n              const xColCorner = yCol * strideWidth - padLeft;\n              let xColMin = xColCorner;\n\n              while (xColMin < 0) {\n                xColMin += dilationWidth;\n              }\n\n              const xColMax = Math.min(convInfo.inWidth, effectiveFilterWidth + xColCorner); // Shader code begins\n\n              let maxValue = Number.NEGATIVE_INFINITY;\n              let maxPosition = -1;\n\n              for (let xDepth = xDepthMin; xDepth < xDepthMax; xDepth += dilationDepth) {\n                const wDepth = xDepth - xDepthCorner;\n\n                for (let xRow = xRowMin; xRow < xRowMax; xRow += dilationHeight) {\n                  const wRow = xRow - xRowCorner;\n\n                  for (let xCol = xColMin; xCol < xColMax; xCol += dilationWidth) {\n                    const wCol = xCol - xColCorner;\n                    const pixel = xBuf.get(batch, xDepth, xRow, xCol, channel);\n\n                    if (pixel >= maxValue) {\n                      maxValue = pixel;\n                      maxPosition = wDepth * effectiveFilterHeight * effectiveFilterWidth + wRow * effectiveFilterHeight + wCol;\n                    }\n                  }\n                }\n              }\n\n              maxPositions.set(maxPosition, batch, yDepth, yRow, yCol, channel);\n            }\n          }\n        }\n      }\n    }\n\n    return maxPositions.toTensor();\n  }\n\n  maxPool3dBackprop(dy, x, y, convInfo) {\n    assertNotComplex([x, y], 'maxPool3dBackprop');\n    const maxPositions = this.maxPool3dPositions(x, convInfo);\n    const strideDepth = convInfo.strideDepth;\n    const strideHeight = convInfo.strideHeight;\n    const strideWidth = convInfo.strideWidth;\n    const dilationDepth = convInfo.dilationDepth;\n    const dilationHeight = convInfo.dilationHeight;\n    const dilationWidth = convInfo.dilationWidth;\n    const effectiveFilterDepth = convInfo.effectiveFilterDepth;\n    const effectiveFilterHeight = convInfo.effectiveFilterHeight;\n    const effectiveFilterWidth = convInfo.effectiveFilterWidth;\n    const padFront = effectiveFilterDepth - 1 - convInfo.padInfo.front;\n    const padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n    const padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n    const dx = tf.buffer(x.shape, 'float32');\n    const maxPosBuf = this.bufferSync(maxPositions);\n    const dyBuf = this.bufferSync(dy);\n\n    for (let batch = 0; batch < convInfo.batchSize; ++batch) {\n      for (let channel = 0; channel < convInfo.inChannels; ++channel) {\n        for (let dxDepth = 0; dxDepth < convInfo.inDepth; ++dxDepth) {\n          for (let dxRow = 0; dxRow < convInfo.inHeight; ++dxRow) {\n            for (let dxCol = 0; dxCol < convInfo.inWidth; ++dxCol) {\n              // Shader code begins\n              const dyDepthCorner = dxDepth - padFront;\n              const dyRowCorner = dxRow - padTop;\n              const dyColCorner = dxCol - padLeft;\n              let dotProd = 0;\n\n              for (let wDepth = 0; wDepth < effectiveFilterDepth; wDepth += dilationDepth) {\n                const dyDepth = (dyDepthCorner + wDepth) / strideDepth;\n\n                if (dyDepth < 0 || dyDepth >= convInfo.outDepth || Math.floor(dyDepth) !== dyDepth) {\n                  continue;\n                }\n\n                for (let wRow = 0; wRow < effectiveFilterHeight; wRow += dilationHeight) {\n                  const dyRow = (dyRowCorner + wRow) / strideHeight;\n\n                  if (dyRow < 0 || dyRow >= convInfo.outHeight || Math.floor(dyRow) !== dyRow) {\n                    continue;\n                  }\n\n                  for (let wCol = 0; wCol < effectiveFilterWidth; wCol += dilationWidth) {\n                    const dyCol = (dyColCorner + wCol) / strideWidth;\n\n                    if (dyCol < 0 || dyCol >= convInfo.outWidth || Math.floor(dyCol) !== dyCol) {\n                      continue;\n                    }\n\n                    const maxPos = effectiveFilterDepth * effectiveFilterHeight * effectiveFilterWidth - 1 - maxPosBuf.get(batch, dyDepth, dyRow, dyCol, channel);\n                    const curPos = wDepth * effectiveFilterHeight * effectiveFilterWidth + wRow * effectiveFilterWidth + wCol;\n                    const mask = maxPos === curPos ? 1 : 0;\n\n                    if (mask === 0) {\n                      continue;\n                    }\n\n                    const pixel = dyBuf.get(batch, dyDepth, dyRow, dyCol, channel);\n                    dotProd += pixel * mask;\n                  }\n                }\n              }\n\n              dx.set(dotProd, batch, dxDepth, dxRow, dxCol, channel);\n            }\n          }\n        }\n      }\n    }\n\n    return dx.toTensor();\n  }\n\n  cast(x, dtype) {\n    return backend_util.castTensor(x, dtype, this);\n  }\n\n  reshape(x, shape) {\n    return backend_util.reshapeTensor(x, shape);\n  }\n\n  avgPool(x, convInfo) {\n    assertNotComplex(x, 'avgPool');\n    assertNotComplex(x, 'maxPool');\n    const xValues = this.readSync(x.dataId);\n    return pool(xValues, x.shape, x.dtype, x.strides, convInfo, 'avg').toTensor().toFloat();\n  }\n\n  resizeBilinear(x, newHeight, newWidth, alignCorners) {\n    assertNotComplex(x, 'resizeBilinear');\n\n    const _x$shape = _slicedToArray(x.shape, 4),\n          batch = _x$shape[0],\n          oldHeight = _x$shape[1],\n          oldWidth = _x$shape[2],\n          numChannels = _x$shape[3];\n\n    const xValues = this.readSync(x.dataId);\n    const result = new Float32Array(util.sizeFromShape([batch, newHeight, newWidth, numChannels]));\n    const effectiveInputSize = [alignCorners && newHeight > 1 ? oldHeight - 1 : oldHeight, alignCorners && newWidth > 1 ? oldWidth - 1 : oldWidth];\n    const effectiveOutputSize = [alignCorners && newHeight > 1 ? newHeight - 1 : newHeight, alignCorners && newWidth > 1 ? newWidth - 1 : newWidth];\n    let outputIdx = 0;\n    const effectiveRowSizeRatio = effectiveInputSize[0] / effectiveOutputSize[0];\n    const effectiveColSizeRatio = effectiveInputSize[1] / effectiveOutputSize[1];\n\n    for (let b = 0; b < batch; b++) {\n      for (let r = 0; r < newHeight; r++) {\n        const sourceFracRow = effectiveRowSizeRatio * r;\n        const sourceRowFloor = Math.floor(sourceFracRow);\n        const rowFrac = sourceFracRow - sourceRowFloor;\n        const sourceRowCeil = Math.min(oldHeight - 1, Math.ceil(sourceFracRow));\n        const topRowOffset = b * x.strides[0] + sourceRowFloor * x.strides[1];\n        const botRowOffset = b * x.strides[0] + sourceRowCeil * x.strides[1];\n\n        for (let c = 0; c < newWidth; c++) {\n          const sourceFracCol = effectiveColSizeRatio * c;\n          const sourceColFloor = Math.floor(sourceFracCol);\n          const colFrac = sourceFracCol - sourceColFloor;\n          const sourceColCeil = Math.min(oldWidth - 1, Math.ceil(sourceFracCol));\n          const topLeftOffest = topRowOffset + sourceColFloor * x.strides[2];\n          const botLeftOffset = botRowOffset + sourceColFloor * x.strides[2];\n          const topRightOffset = topRowOffset + sourceColCeil * x.strides[2];\n          const botRightOffest = botRowOffset + sourceColCeil * x.strides[2];\n\n          for (let d = 0; d < numChannels; d++) {\n            // Begin shader.\n            // Compute the fractional index of the source.\n            const topLeft = xValues[topLeftOffest + d];\n            const bottomLeft = xValues[botLeftOffset + d];\n            const topRight = xValues[topRightOffset + d];\n            const bottomRight = xValues[botRightOffest + d];\n            const top = topLeft + (topRight - topLeft) * colFrac;\n            const bottom = bottomLeft + (bottomRight - bottomLeft) * colFrac;\n            const newValue = top + (bottom - top) * rowFrac;\n            result[outputIdx++] = newValue;\n          }\n        }\n      }\n    }\n\n    return tf.tensor(result, [batch, newHeight, newWidth, numChannels]);\n  }\n\n  resizeBilinearBackprop(dy, x, alignCorners) {\n    assertNotComplex([dy, x], 'resizeBilinearBackprop');\n\n    const _x$shape2 = _slicedToArray(x.shape, 4),\n          batch = _x$shape2[0],\n          xHeight = _x$shape2[1],\n          xWidth = _x$shape2[2],\n          depth = _x$shape2[3];\n\n    const _dy$shape = _slicedToArray(dy.shape, 3),\n          yHeight = _dy$shape[1],\n          yWidth = _dy$shape[2];\n\n    const output = new Float32Array(batch * xHeight * xWidth * depth); // In the backwards pass, we want to find the pixels that were generated\n    // for each pixel in the input image the forward pass and add the\n    // corresponding coefficient from dy to the gradient (with some\n    // interpolation).\n\n    const effectiveXSize = [alignCorners && yHeight > 1 ? xHeight - 1 : xHeight, alignCorners && yWidth > 1 ? xWidth - 1 : xWidth];\n    const effectiveYSize = [alignCorners && yHeight > 1 ? yHeight - 1 : yHeight, alignCorners && yWidth > 1 ? yWidth - 1 : yWidth];\n    const heightScale = effectiveXSize[0] / effectiveYSize[0];\n    const widthScale = effectiveXSize[1] / effectiveYSize[1]; // Reference implementation\n    // tslint:disable-next-line:max-line-length\n    // https://github.com/tensorflow/tensorflow/blob/3039375c86a5bbc9610c7725dcaa95d635f87ba2/tensorflow/core/kernels/resize_bilinear_op.cc#L275\n\n    const dyValues = this.readSync(dy.dataId);\n    let offset = 0;\n\n    for (let b = 0; b < batch; b++) {\n      const bOffset = b * x.strides[0];\n\n      for (let r = 0; r < yHeight; r++) {\n        const dxR = r * heightScale;\n        const topDxRIndex = Math.floor(dxR);\n        const bottomDxRIndex = Math.min(Math.ceil(dxR), xHeight - 1);\n        const topDxROffset = bOffset + topDxRIndex * x.strides[1];\n        const bottomDxROffset = bOffset + bottomDxRIndex * x.strides[1];\n        const dxRLerp = dxR - topDxRIndex;\n        const inverseDxRLerp = 1.0 - dxRLerp;\n\n        for (let c = 0; c < yWidth; c++) {\n          const dxC = c * widthScale;\n          const leftDxCIndex = Math.floor(dxC);\n          const rightDxCIndex = Math.min(Math.ceil(dxC), xWidth - 1);\n          const dxCLerp = dxC - leftDxCIndex;\n          const inverseDxCLerp = 1.0 - dxCLerp;\n          const topLeftRCOffset = topDxROffset + leftDxCIndex * x.strides[2];\n          const topRightRCOffset = topDxROffset + rightDxCIndex * x.strides[2];\n          const bottomLeftRCOffset = bottomDxROffset + leftDxCIndex * x.strides[2];\n          const bottomRightRCOffset = bottomDxROffset + rightDxCIndex * x.strides[2];\n          const inverseDxRLerpTimesInverseDxCLerp = inverseDxRLerp * inverseDxCLerp;\n          const inverseDxRLerpTimesDxCLerp = inverseDxRLerp * dxCLerp;\n          const dxRLerpTimesInverseDxCLerp = dxRLerp * inverseDxCLerp;\n          const dxRLerpTimesDxCLerp = dxRLerp * dxCLerp;\n\n          for (let d = 0; d < depth; d++) {\n            const dyVal = dyValues[offset++];\n            output[topLeftRCOffset + d] += dyVal * inverseDxRLerpTimesInverseDxCLerp;\n            output[topRightRCOffset + d] += dyVal * inverseDxRLerpTimesDxCLerp;\n            output[bottomLeftRCOffset + d] += dyVal * dxRLerpTimesInverseDxCLerp;\n            output[bottomRightRCOffset + d] += dyVal * dxRLerpTimesDxCLerp;\n          }\n        }\n      }\n    }\n\n    return tf.tensor4d(output, [batch, xWidth, xHeight, depth], x.dtype);\n  }\n\n  resizeNearestNeighbor(x, newHeight, newWidth, alignCorners) {\n    assertNotComplex(x, 'resizeNearestNeighbor');\n\n    const _x$shape3 = _slicedToArray(x.shape, 4),\n          batch = _x$shape3[0],\n          oldHeight = _x$shape3[1],\n          oldWidth = _x$shape3[2],\n          numChannels = _x$shape3[3];\n\n    const xValues = this.readSync(x.dataId);\n    const output = new Float32Array(batch * newHeight * newWidth * numChannels);\n    const effectiveInputSize = [alignCorners && newHeight > 1 ? oldHeight - 1 : oldHeight, alignCorners && newWidth > 1 ? oldWidth - 1 : oldWidth];\n    const effectiveOutputSize = [alignCorners && newHeight > 1 ? newHeight - 1 : newHeight, alignCorners && newWidth > 1 ? newWidth - 1 : newWidth];\n    const effectiveRowSizeRatio = effectiveInputSize[0] / effectiveOutputSize[0];\n    const effectiveColSizeRatio = effectiveInputSize[1] / effectiveOutputSize[1];\n    let outputOffset = 0;\n\n    for (let b = 0; b < batch; b++) {\n      const batchOffset = b * x.strides[0];\n\n      for (let r = 0; r < newHeight; r++) {\n        const sourceFracRow = effectiveRowSizeRatio * r;\n        const sourceNearestRow = Math.min(oldHeight - 1, alignCorners ? Math.round(sourceFracRow) : Math.floor(sourceFracRow));\n        const rowOffset = batchOffset + sourceNearestRow * x.strides[1];\n\n        for (let c = 0; c < newWidth; c++) {\n          const sourceFracCol = effectiveColSizeRatio * c;\n          const sourceNearestCol = Math.min(oldWidth - 1, alignCorners ? Math.round(sourceFracCol) : Math.floor(sourceFracCol));\n          const colOffset = rowOffset + sourceNearestCol * x.strides[2];\n\n          for (let d = 0; d < numChannels; d++) {\n            // Begin shader.\n            // Compute the fractional index of the source.\n            const newVal = xValues[colOffset + d];\n            output[outputOffset++] = newVal;\n          }\n        }\n      }\n    }\n\n    return tf.tensor(output, [batch, newHeight, newWidth, numChannels], x.dtype);\n  }\n\n  resizeNearestNeighborBackprop(dy, x, alignCorners) {\n    assertNotComplex([dy, x], 'resizeNearestNeighborBackprop');\n\n    const _x$shape4 = _slicedToArray(x.shape, 4),\n          batch = _x$shape4[0],\n          xHeight = _x$shape4[1],\n          xWidth = _x$shape4[2],\n          depth = _x$shape4[3];\n\n    const _dy$shape2 = _slicedToArray(dy.shape, 3),\n          yHeight = _dy$shape2[1],\n          yWidth = _dy$shape2[2];\n\n    const output = new Float32Array(batch * xHeight * xWidth * depth);\n    const dyValues = this.readSync(dy.dataId); // In the backwards pass, we want to find the pixels that were generated\n    // for each pixel in the input image the forward pass\n\n    const effectiveXSize = [alignCorners && yHeight > 1 ? xHeight - 1 : xHeight, alignCorners && yWidth > 1 ? xWidth - 1 : xWidth];\n    const effectiveYSize = [alignCorners && yHeight > 1 ? yHeight - 1 : yHeight, alignCorners && yWidth > 1 ? yWidth - 1 : yWidth];\n    const heightScale = effectiveXSize[0] / effectiveYSize[0];\n    const widthScale = effectiveXSize[1] / effectiveYSize[1];\n    const invHeightScale = 1 / heightScale;\n    const invWidthScale = 1 / widthScale; // This defines the size of the window of values around a particular\n    // index in dy that we want to search for contributions to dx.\n\n    const winHeight = Math.ceil(invHeightScale) * 2 + 2;\n    const winWidth = Math.ceil(invWidthScale) * 2 + 2; // Loop over the output space.\n\n    for (let b = 0; b < batch; b++) {\n      const batchOffset = b * x.strides[0];\n\n      for (let r = 0; r < xHeight; r++) {\n        const rowOffset = batchOffset + r * x.strides[1]; // Compute bounds for where in dy we will look\n\n        const startRLerp = Math.floor(r * invHeightScale);\n        const startDyR = Math.floor(startRLerp - winHeight / 2);\n\n        for (let c = 0; c < xWidth; c++) {\n          const colOffset = rowOffset + c * x.strides[2]; // Compute bounds for where in dy we will look\n\n          const startCLerp = Math.floor(c * invWidthScale);\n          const startDyC = Math.floor(startCLerp - winWidth / 2);\n\n          for (let d = 0; d < depth; d++) {\n            let accum = 0; // loop over dy\n\n            for (let dyRIndex = 0; dyRIndex < winHeight; dyRIndex++) {\n              const dyR = dyRIndex + startDyR; // Guard against the window exceeding the bounds of dy\n\n              if (dyR < 0 || dyR >= yHeight) {\n                continue;\n              }\n\n              const dyROffset = batchOffset + dyR * dy.strides[1];\n              const sourceFracRow = dyR * heightScale;\n              const sourceNearestRow = Math.min(xHeight - 1, alignCorners ? Math.round(sourceFracRow) : Math.floor(sourceFracRow));\n\n              if (r !== sourceNearestRow) {\n                continue;\n              }\n\n              for (let dyCIndex = 0; dyCIndex < winWidth; dyCIndex++) {\n                const dyC = dyCIndex + startDyC; // Guard against the window exceeding the bounds of dy\n\n                if (dyC < 0 || dyC >= yWidth) {\n                  continue;\n                }\n\n                const dyCOffset = dyROffset + dyC * dy.strides[2];\n                const sourceFracCol = dyC * widthScale;\n                const sourceNearestCol = Math.min(xWidth - 1, alignCorners ? Math.round(sourceFracCol) : Math.floor(sourceFracCol));\n\n                if (c === sourceNearestCol) {\n                  accum += dyValues[dyCOffset + d];\n                }\n              }\n            }\n\n            output[colOffset + d] = accum;\n          }\n        }\n      }\n    }\n\n    return tf.tensor4d(output, x.shape, x.dtype);\n  }\n\n  batchNorm(x, mean, variance, offset, scale, varianceEpsilon) {\n    assertNotComplex([x, mean, variance, scale, offset], 'batchNorm');\n    const xVals = this.readSync(x.dataId);\n    const mVals = this.readSync(mean.dataId);\n    const varVals = this.readSync(variance.dataId);\n    const sVals = scale ? this.readSync(scale.dataId) : new Float32Array([1]);\n    const offVals = offset ? this.readSync(offset.dataId) : new Float32Array([0]);\n    const outVals = new Float32Array(xVals.length);\n    const offValsLength = offVals.length;\n    const sValsLength = sVals.length;\n    const varValsLength = varVals.length;\n    const mValsLength = mVals.length;\n    let offi = 0;\n    let mi = 0;\n    let si = 0;\n    let vi = 0;\n\n    for (let i = 0; i < xVals.length; ++i) {\n      outVals[i] = offVals[offi++] + (xVals[i] - mVals[mi++]) * sVals[si++] / Math.sqrt(varVals[vi++] + varianceEpsilon);\n\n      if (offi >= offValsLength) {\n        offi = 0;\n      }\n\n      if (mi >= mValsLength) {\n        mi = 0;\n      }\n\n      if (si >= sValsLength) {\n        si = 0;\n      }\n\n      if (vi >= varValsLength) {\n        vi = 0;\n      }\n    }\n\n    return tf.tensor4d(outVals, x.shape);\n  }\n\n  localResponseNormalization4D(x, depthRadius, bias, alpha, beta) {\n    assertNotComplex(x, 'localResponseNormalization4D');\n    const channels = x.shape[3];\n    const maxD = channels - 1;\n    const xValues = this.readSync(x.dataId);\n    const size = x.size;\n    const result = new Float32Array(size);\n\n    function sumAcrossChannels(offset) {\n      const currentChannel = offset % channels;\n      let beginSumOffset = offset - currentChannel + Math.max(0, currentChannel - depthRadius);\n      const endSumOffset = offset - currentChannel + Math.min(currentChannel + depthRadius, maxD);\n      let sum = 0.0;\n\n      for (; beginSumOffset <= endSumOffset; beginSumOffset++) {\n        const z = xValues[beginSumOffset];\n        sum += z * z;\n      }\n\n      return sum;\n    }\n\n    for (let offset = 0; offset < size; offset++) {\n      const sum = sumAcrossChannels(offset);\n      const val = xValues[offset] * Math.pow(bias + alpha * sum, -beta);\n      result[offset] = val;\n    }\n\n    return tf.tensor4d(result, x.shape);\n  }\n\n  LRNGrad(dy, inputImage, outputImage, depthRadius, bias, alpha, beta) {\n    assertNotComplex(dy, 'LRNGrad');\n    const channels = dy.shape[3];\n    const dyValues = this.readSync(dy.dataId);\n    const inputImageValues = this.readSync(inputImage.dataId);\n    const outputImageValues = this.readSync(outputImage.dataId);\n    const result = new Float32Array(dy.size);\n    const size = dy.size;\n\n    for (let offset = 0; offset < size; offset++) {\n      const currentChannel = offset % channels;\n      const depthBegin = offset - currentChannel + Math.max(0, currentChannel - depthRadius);\n      const depthEnd = offset - currentChannel + Math.min(channels, currentChannel + depthRadius + 1);\n      let norm = 0;\n\n      for (let k = depthBegin; k < depthEnd; k++) {\n        norm += Math.pow(inputImageValues[k], 2);\n      }\n\n      norm = alpha * norm + bias;\n\n      for (let k = depthBegin; k < depthEnd; k++) {\n        let dyi = -2 * alpha * beta * inputImageValues[k] * outputImageValues[offset] / norm;\n\n        if (offset === k) {\n          dyi += Math.pow(norm, -beta);\n        }\n\n        dyi *= dyValues[offset];\n        result[k] += dyi;\n      }\n    }\n\n    return tf.tensor4d(result, dy.shape);\n  }\n\n  multinomial(logits, normalized, numSamples, seed) {\n    assertNotComplex(logits, 'multinomial');\n    const probabilities = normalized ? logits : tf.softmax(logits);\n    const batchSize = probabilities.shape[0];\n    const numEvents = probabilities.shape[1];\n    const res = tf.zeros([batchSize, numSamples], 'int32');\n    const resVals = this.readSync(res.dataId);\n    const probVals = this.readSync(probabilities.dataId);\n\n    for (let b = 0; b < batchSize; ++b) {\n      const offset = b * numEvents; // The cdf won't include the last event. It will be implicit if no other\n      // event happened.\n\n      const cdf = new Float32Array(numEvents - 1);\n      cdf[0] = probVals[offset];\n\n      for (let event = 1; event < cdf.length; ++event) {\n        cdf[event] = cdf[event - 1] + probVals[offset + event];\n      }\n\n      const random = seedrandom.alea(seed.toString());\n      const outOffset = b * numSamples;\n\n      for (let sampleId = 0; sampleId < numSamples; ++sampleId) {\n        const r = random(); // Assume last event happened by default.\n\n        resVals[outOffset + sampleId] = cdf.length;\n\n        for (let event = 0; event < cdf.length; event++) {\n          if (r < cdf[event]) {\n            resVals[outOffset + sampleId] = event;\n            break;\n          }\n        }\n      }\n    }\n\n    return res;\n  }\n\n  oneHot(indices, depth, onValue, offValue) {\n    assertNotComplex(indices, 'oneHot');\n    const res = new Float32Array(indices.size * depth);\n    res.fill(offValue);\n    const indicesVal = this.readSync(indices.dataId);\n\n    for (let event = 0; event < indices.size; ++event) {\n      if (indicesVal[event] >= 0 && indicesVal[event] < depth) {\n        res[event * depth + indicesVal[event]] = onValue;\n      }\n    }\n\n    return tf.tensor2d(res, [indices.size, depth], 'int32');\n  }\n\n  nonMaxSuppression(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold) {\n    assertNotComplex(boxes, 'nonMaxSuppression');\n    const boxesVals = this.readSync(boxes.dataId);\n    const scoresVals = this.readSync(scores.dataId);\n    return nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);\n  }\n\n  fft(x) {\n    return this.fftBatch(x, false);\n  }\n\n  ifft(x) {\n    return this.fftBatch(x, true);\n  }\n  /**\n   * Calculate FFT of inner most elements of batch tensor.\n   */\n\n\n  fftBatch(x, inverse) {\n    const batch = x.shape[0];\n    const innerDim = x.shape[1]; // Collects real and imaginary values separately.\n\n    const realResult = tf.buffer(x.shape, 'float32');\n    const imagResult = tf.buffer(x.shape, 'float32');\n    const real = tf.real(x).as2D(batch, innerDim);\n    const imag = tf.imag(x).as2D(batch, innerDim);\n\n    for (let b = 0; b < batch; b++) {\n      // TODO: Support slice ops for complex type.\n      const r = real.slice([b, 0], [1, innerDim]);\n      const i = imag.slice([b, 0], [1, innerDim]);\n      const input = tf.complex(r, i); // Run FFT by batch element.\n\n      const res = this.readSync(this.fftImpl(input, inverse).dataId);\n\n      for (let d = 0; d < innerDim; d++) {\n        const c = backend_util.getComplexWithIndex(res, d);\n        realResult.values[b * innerDim + d] = c.real;\n        imagResult.values[b * innerDim + d] = c.imag;\n      }\n    }\n\n    const t = tf.complex(realResult.toTensor(), imagResult.toTensor());\n    return t.as2D(batch, innerDim);\n  }\n\n  fftImpl(x, inverse) {\n    const x1D = x.as1D();\n    const n = x1D.size;\n\n    if (this.isExponentOf2(n)) {\n      let result = this.fftRadix2(x1D, n, inverse).as2D(x.shape[0], x.shape[1]);\n\n      if (inverse) {\n        result = tf.complex(tf.real(result).div(tf.scalar(n)), tf.imag(result).div(tf.scalar(n)));\n      }\n\n      return result;\n    } else {\n      const data = this.readSync(x.dataId);\n      const rawOutput = this.fourierTransformByMatmul(data, n, inverse);\n      const output = backend_util.splitRealAndImagArrays(rawOutput);\n      return tf.complex(output.real, output.imag).as2D(x.shape[0], x.shape[1]);\n    }\n  }\n\n  isExponentOf2(size) {\n    return (size & size - 1) === 0;\n  } // FFT using Cooley-Tukey algorithm on radix 2 dimensional input.\n\n\n  fftRadix2(input, size, inverse) {\n    if (size === 1) {\n      return input;\n    }\n\n    const data = this.readSync(input.dataId);\n    const half = size / 2;\n    const evenComplex = backend_util.complexWithEvenIndex(data);\n    let evenTensor = tf.complex(evenComplex.real, evenComplex.imag).as1D();\n    const oddComplex = backend_util.complexWithOddIndex(data);\n    let oddTensor = tf.complex(oddComplex.real, oddComplex.imag).as1D(); // Recursive call for half part of original input.\n\n    evenTensor = this.fftRadix2(evenTensor, half, inverse);\n    oddTensor = this.fftRadix2(oddTensor, half, inverse);\n    const e = backend_util.exponents(size, inverse);\n    const exponent = tf.complex(e.real, e.imag).mul(oddTensor);\n    const addPart = evenTensor.add(exponent);\n    const subPart = evenTensor.sub(exponent);\n    const realTensor = tf.real(addPart).concat(tf.real(subPart));\n    const imagTensor = tf.imag(addPart).concat(tf.imag(subPart));\n    return tf.complex(realTensor, imagTensor).as1D();\n  } // Calculate fourier transform by multplying sinusoid matrix.\n\n\n  fourierTransformByMatmul(data, size, inverse) {\n    const ret = new Float32Array(size * 2); // TODO: Use matmul instead once it supports complex64 type.\n\n    for (let r = 0; r < size; r++) {\n      let real = 0.0;\n      let imag = 0.0;\n\n      for (let c = 0; c < size; c++) {\n        const e = backend_util.exponent(r * c, size, inverse);\n        const term = backend_util.getComplexWithIndex(data, c);\n        real += term.real * e.real - term.imag * e.imag;\n        imag += term.real * e.imag + term.imag * e.real;\n      }\n\n      if (inverse) {\n        real /= size;\n        imag /= size;\n      }\n\n      backend_util.assignToTypedArray(ret, real, imag, r);\n    }\n\n    return ret;\n  }\n\n  depthToSpace(x, blockSize, dataFormat) {\n    util.assert(dataFormat === 'NHWC', () => \"Only NHWC dataFormat supported on CPU for depthToSpace. Got \".concat(dataFormat));\n    util.assert(blockSize > 1, () => \"blockSize should be > 1 for depthToSpace, but was: \".concat(blockSize));\n    const batchSize = x.shape[0];\n    const inputHeight = x.shape[1];\n    const inputWidth = x.shape[2];\n    const inputDepth = x.shape[3];\n    const outputHeight = inputHeight * blockSize;\n    const outputWidth = inputWidth * blockSize;\n    const outputDepth = inputDepth / (blockSize * blockSize);\n    const xValues = this.readSync(x.dataId);\n    const result = new Float32Array(batchSize * outputHeight * outputWidth * outputDepth);\n    let outputIdx = 0;\n\n    for (let b = 0; b < batchSize; ++b) {\n      for (let h = 0; h < outputHeight; ++h) {\n        const inH = Math.floor(h / blockSize);\n        const offsetH = h % blockSize;\n\n        for (let w = 0; w < outputWidth; ++w) {\n          const inW = Math.floor(w / blockSize);\n          const offsetW = w % blockSize;\n          const offsetD = (offsetH * blockSize + offsetW) * outputDepth;\n\n          for (let d = 0; d < outputDepth; ++d) {\n            const inD = d + offsetD;\n            const inputIdx = inD + inputDepth * (inW + inputWidth * (inH + inputHeight * b));\n            result[outputIdx++] = xValues[inputIdx];\n          }\n        }\n      }\n    }\n\n    return tf.tensor4d(result, [batchSize, outputHeight, outputWidth, outputDepth]);\n  }\n\n  broadcastedBinaryOp(a, b, dtype, op) {\n    const newShape = backend_util.assertAndGetBroadcastShape(a.shape, b.shape);\n    const result = tf.buffer(newShape, dtype);\n    const aVals = this.readSync(a.dataId);\n    const bVals = this.readSync(b.dataId);\n    const aBroadcastDims = backend_util.getBroadcastDims(a.shape, newShape);\n    const bBroadcastDims = backend_util.getBroadcastDims(b.shape, newShape);\n    const resVals = result.values;\n\n    if (aBroadcastDims.length + bBroadcastDims.length === 0) {\n      for (let i = 0; i < resVals.length; ++i) {\n        resVals[i] = op(aVals[i % aVals.length], bVals[i % bVals.length]);\n      }\n    } else {\n      const aBuf = this.bufferSync(a);\n      const bBuf = this.bufferSync(b);\n\n      for (let i = 0; i < resVals.length; ++i) {\n        const loc = result.indexToLoc(i);\n        const aLoc = loc.slice(-a.rank);\n        aBroadcastDims.forEach(d => aLoc[d] = 0);\n        const aIndex = aBuf.locToIndex(aLoc);\n        const bLoc = loc.slice(-b.rank);\n        bBroadcastDims.forEach(d => bLoc[d] = 0);\n        const bIndex = bBuf.locToIndex(bLoc);\n        resVals[i] = op(aVals[aIndex], bVals[bIndex]);\n      }\n    }\n\n    return result.toTensor();\n  }\n\n  broadcastedBinaryComplexOp(a, b, op) {\n    const newShape = backend_util.assertAndGetBroadcastShape(a.shape, b.shape);\n    const realResult = tf.buffer(newShape, 'float32');\n    const imagResult = tf.buffer(newShape, 'float32');\n    const aVals = this.readSync(a.dataId);\n    const bVals = this.readSync(b.dataId);\n    const aBroadcastDims = backend_util.getBroadcastDims(a.shape, newShape);\n    const bBroadcastDims = backend_util.getBroadcastDims(b.shape, newShape);\n    const realVals = realResult.values;\n    const imagVals = imagResult.values;\n\n    if (aBroadcastDims.length + bBroadcastDims.length === 0) {\n      for (let i = 0; i < realVals.length; i++) {\n        const aIdx = i % aVals.length;\n        const bIdx = i % bVals.length;\n        const result = op(aVals[aIdx * 2], aVals[aIdx * 2 + 1], bVals[bIdx * 2], bVals[bIdx * 2 + 1]);\n        realVals[i] = result.real;\n        imagVals[i] = result.imag;\n      }\n    } else {\n      const aRealBuf = this.bufferSync(this.data.get(a.dataId).complexTensors.real);\n      const bRealBuf = this.bufferSync(this.data.get(b.dataId).complexTensors.real);\n\n      for (let i = 0; i < realVals.length; i++) {\n        const loc = realResult.indexToLoc(i);\n        const aLoc = loc.slice(-a.rank);\n        aBroadcastDims.forEach(d => aLoc[d] = 0);\n        const aIndex = aRealBuf.locToIndex(aLoc);\n        const bLoc = loc.slice(-b.rank);\n        bBroadcastDims.forEach(d => bLoc[d] = 0);\n        const bIndex = bRealBuf.locToIndex(bLoc);\n        const opResult = op(aVals[aIndex * 2], aVals[aIndex * 2 + 1], bVals[bIndex * 2], bVals[bIndex * 2 + 1]);\n        realVals[i] = opResult.real;\n        imagVals[i] = opResult.imag;\n      }\n    }\n\n    return this.complex(realResult.toTensor(), imagResult.toTensor());\n  }\n\n  split(x, sizeSplits, axis) {\n    return split(x, sizeSplits, axis);\n  }\n\n  dispose() {}\n\n  floatPrecision() {\n    return 32;\n  }\n  /** Returns the smallest representable number.  */\n\n\n  epsilon() {\n    return super.epsilon();\n  }\n\n  cropAndResize(images, boxes, boxIndex, cropSize, method, extrapolationValue) {\n    const _images$shape = _slicedToArray(images.shape, 4),\n          batch = _images$shape[0],\n          imageHeight = _images$shape[1],\n          imageWidth = _images$shape[2],\n          numChannels = _images$shape[3];\n\n    const numBoxes = boxes.shape[0];\n\n    const _cropSize = _slicedToArray(cropSize, 2),\n          cropHeight = _cropSize[0],\n          cropWidth = _cropSize[1];\n\n    const output = tf.buffer([numBoxes, cropHeight, cropWidth, numChannels], 'float32');\n    const boxVals = this.readSync(boxes.dataId);\n    const boxIndVals = this.readSync(boxIndex.dataId);\n    const imageVals = this.readSync(images.dataId);\n    const inStride = images.strides; // to calculate flat indexes into image\n\n    const outStride = output.strides; // to calculate flat indexes into output\n    // Reference implementation\n    // tslint:disable-next-line:max-line-length\n    // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/crop_and_resize_op.cc\n\n    for (let b = 0; b < numBoxes; b++) {\n      const startInd = b * 4;\n      const y1 = boxVals[startInd];\n      const x1 = boxVals[startInd + 1];\n      const y2 = boxVals[startInd + 2];\n      const x2 = boxVals[startInd + 3];\n      const bInd = boxIndVals[b];\n\n      if (bInd >= batch) {\n        continue;\n      }\n\n      const heightScale = cropHeight > 1 ? (y2 - y1) * (imageHeight - 1) / (cropHeight - 1) : 0;\n      const widthScale = cropWidth > 1 ? (x2 - x1) * (imageWidth - 1) / (cropWidth - 1) : 0;\n\n      for (let y = 0; y < cropHeight; y++) {\n        const yInd = cropHeight > 1 ? y1 * (imageHeight - 1) + y * heightScale : 0.5 * (y1 + y2) * (imageHeight - 1);\n\n        if (yInd < 0 || yInd > imageHeight - 1) {\n          for (let x = 0; x < cropWidth; x++) {\n            for (let c = 0; c < numChannels; c++) {\n              const ind = c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n              output.values[ind] = extrapolationValue;\n            }\n          }\n\n          continue;\n        }\n\n        if (method === 'bilinear') {\n          const topInd = Math.floor(yInd);\n          const bottomInd = Math.ceil(yInd);\n          const yLerp = yInd - topInd;\n\n          for (let x = 0; x < cropWidth; x++) {\n            const xInd = cropWidth > 1 ? x1 * (imageWidth - 1) + x * widthScale : 0.5 * (x1 + x2) * (imageWidth - 1);\n\n            if (xInd < 0 || xInd > imageWidth - 1) {\n              for (let c = 0; c < numChannels; c++) {\n                const ind = c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n                output.values[ind] = extrapolationValue;\n              }\n\n              continue;\n            }\n\n            const leftInd = Math.floor(xInd);\n            const rightInd = Math.ceil(xInd);\n            const xLerp = xInd - leftInd;\n\n            for (let c = 0; c < numChannels; c++) {\n              let ind = c + leftInd * inStride[2] + topInd * inStride[1] + bInd * inStride[0];\n              const topLeft = imageVals[ind];\n              ind = c + rightInd * inStride[2] + topInd * inStride[1] + bInd * inStride[0];\n              const topRight = imageVals[ind];\n              ind = c + leftInd * inStride[2] + bottomInd * inStride[1] + bInd * inStride[0];\n              const bottomLeft = imageVals[ind];\n              ind = c + rightInd * inStride[2] + bottomInd * inStride[1] + bInd * inStride[0];\n              const bottomRight = imageVals[ind];\n              const top = topLeft + (topRight - topLeft) * xLerp;\n              const bottom = bottomLeft + (bottomRight - bottomLeft) * xLerp;\n              ind = c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n              output.values[ind] = top + (bottom - top) * yLerp;\n            }\n          }\n        } else {\n          // method == \"nearest\"\n          for (let x = 0; x < cropWidth; ++x) {\n            const xInd = cropWidth > 1 ? x1 * (imageWidth - 1) + x * widthScale : 0.5 * (x1 + x2) * (imageWidth - 1);\n\n            if (xInd < 0 || xInd > imageWidth - 1) {\n              for (let c = 0; c < numChannels; c++) {\n                const ind = c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n                output.values[ind] = extrapolationValue;\n              }\n\n              continue;\n            }\n\n            const closestX = Math.round(xInd);\n            const closestY = Math.round(yInd);\n\n            for (let c = 0; c < numChannels; c++) {\n              const inInd = c + closestX * inStride[2] + closestY * inStride[1] + bInd * inStride[0];\n              const outInd = c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n              output.values[outInd] = imageVals[inInd];\n            }\n          }\n        }\n      }\n    }\n\n    return output.toTensor();\n  }\n\n  sparseToDense(sparseIndices, sparseValues, outputShape, defaultValue) {\n    const _backend_util$calcula = backend_util.calculateShapes(sparseValues, sparseIndices, outputShape),\n          sliceRank = _backend_util$calcula.sliceRank,\n          numUpdates = _backend_util$calcula.numUpdates,\n          sliceSize = _backend_util$calcula.sliceSize,\n          strides = _backend_util$calcula.strides,\n          outputSize = _backend_util$calcula.outputSize;\n\n    const sumDupeIndices = false;\n    return this.scatter(sparseIndices, sparseValues, outputShape, outputSize, sliceSize, numUpdates, sliceRank, strides, defaultValue, sumDupeIndices);\n  }\n\n  gatherND(x, indices) {\n    const indicesShape = indices.shape;\n    const sliceRank = indicesShape[indicesShape.length - 1];\n\n    const _backend_util$prepare = backend_util.prepareAndValidate(x, indices),\n          _backend_util$prepare2 = _slicedToArray(_backend_util$prepare, 4),\n          resultShape = _backend_util$prepare2[0],\n          numSlices = _backend_util$prepare2[1],\n          sliceSize = _backend_util$prepare2[2],\n          strides = _backend_util$prepare2[3];\n\n    if (numSlices === 0) {\n      return tf.tensor([], resultShape, x.dtype);\n    }\n\n    const buffer = new TensorBuffer([numSlices, sliceSize], x.dtype);\n    const indicesData = this.readSync(indices.dataId);\n    const xData = this.readSync(x.dataId);\n\n    for (let i = 0; i < numSlices; i++) {\n      const index = [];\n      let flattenIndex = 0;\n\n      for (let j = 0; j < sliceRank; j++) {\n        const dim = indicesData[i * sliceRank + j];\n        flattenIndex += dim * strides[j];\n        index.push(dim);\n      }\n\n      if (flattenIndex < 0 || flattenIndex >= x.size / sliceSize) {\n        throw new Error(\"Invalid indices: \".concat(index, \" does not index into \").concat(x.shape));\n      }\n\n      for (let k = 0; k < sliceSize; k++) {\n        buffer.values[i * sliceSize + k] = xData[flattenIndex * sliceSize + k];\n      }\n    }\n\n    return buffer.toTensor().reshape(resultShape);\n  }\n\n  scatterND(indices, updates, shape) {\n    const _backend_util$calcula2 = backend_util.calculateShapes(updates, indices, shape),\n          sliceRank = _backend_util$calcula2.sliceRank,\n          numUpdates = _backend_util$calcula2.numUpdates,\n          sliceSize = _backend_util$calcula2.sliceSize,\n          strides = _backend_util$calcula2.strides,\n          outputSize = _backend_util$calcula2.outputSize;\n\n    const defaultValue = tf.scalar(0);\n    const sumDupeIndices = true;\n    return this.scatter(indices, updates, shape, outputSize, sliceSize, numUpdates, sliceRank, strides, defaultValue, sumDupeIndices);\n  }\n\n  fill(shape, value, dtype) {\n    dtype = dtype || util.inferDtype(value);\n    const values = util.getArrayFromDType(dtype, util.sizeFromShape(shape));\n    values.fill(value);\n    return engine().makeTensor(values, shape, dtype, this);\n  }\n\n  onesLike(x) {\n    if (x.dtype === 'string') {\n      throw new Error('onesLike is not supported for string tensors');\n    } else {\n      return this.fill(x.shape, 1, x.dtype);\n    }\n  }\n\n  zerosLike(x) {\n    const values = util.getArrayFromDType(x.dtype, util.sizeFromShape(x.shape));\n    return this.makeOutput(values, x.shape, x.dtype);\n  }\n\n  linspace(start, stop, num) {\n    return backend_util.linspaceImpl(start, stop, num);\n  }\n\n  scatter(indices, updates, shape, outputSize, sliceSize, numUpdates, sliceRank, strides, defaultValue, sumDupeIndices) {\n    const flattenShape = [outputSize / sliceSize, sliceSize];\n    const indicesData = this.readSync(indices.dataId);\n    const updatesData = this.readSync(updates.dataId);\n\n    if (outputSize === 0) {\n      return tf.tensor([], shape, updates.dtype);\n    }\n\n    const buffer = new TensorBuffer(flattenShape, updates.dtype);\n    buffer.values.fill(this.readSync(defaultValue.dataId)[0]);\n\n    for (let i = 0; i < numUpdates; i++) {\n      const index = [];\n      let flattenIndex = 0;\n\n      for (let j = 0; j < sliceRank; j++) {\n        const dim = indicesData[i * sliceRank + j];\n        index.push(dim);\n        flattenIndex += dim * strides[j];\n      }\n\n      if (flattenIndex < 0 || flattenIndex >= outputSize / sliceSize) {\n        throw new Error(\"Invalid indices: \".concat(index, \" does not index into \").concat(shape));\n      }\n\n      for (let k = 0; k < sliceSize; k++) {\n        if (sumDupeIndices) {\n          buffer.values[flattenIndex * sliceSize + k] += updatesData[i * sliceSize + k];\n        } else {\n          buffer.values[flattenIndex * sliceSize + k] = updates.rank === 0 ? updatesData[0] : updatesData[i * sliceSize + k];\n        }\n      }\n    }\n\n    return buffer.toTensor().reshape(shape);\n  }\n\n}","map":null,"metadata":{},"sourceType":"module"}