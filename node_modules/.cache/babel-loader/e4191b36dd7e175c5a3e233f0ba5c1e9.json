{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, Dilation2DBackpropFilter, util } from '@tensorflow/tfjs-core';\nexport const dilation2dBackpropFilterConfig = {\n  kernelName: Dilation2DBackpropFilter,\n  backendName: 'cpu',\n  kernelFunc: ({\n    inputs,\n    backend,\n    attrs\n  }) => {\n    const x = inputs.x,\n          filter = inputs.filter,\n          dy = inputs.dy;\n    const strides = attrs.strides,\n          pad = attrs.pad,\n          dilations = attrs.dilations;\n    const cpuBackend = backend;\n    const $x = util.toNestedArray(x.shape, cpuBackend.data.get(x.dataId).values);\n    const $filter = util.toNestedArray(filter.shape, cpuBackend.data.get(filter.dataId).values);\n\n    const _backend_util$compute = backend_util.computeDilation2DInfo(x.shape, filter.shape, strides, pad, 'NHWC'\n    /* dataFormat */\n    , dilations),\n          batchSize = _backend_util$compute.batchSize,\n          inHeight = _backend_util$compute.inHeight,\n          inWidth = _backend_util$compute.inWidth,\n          inChannels = _backend_util$compute.inChannels,\n          outHeight = _backend_util$compute.outHeight,\n          outWidth = _backend_util$compute.outWidth,\n          padInfo = _backend_util$compute.padInfo,\n          strideHeight = _backend_util$compute.strideHeight,\n          strideWidth = _backend_util$compute.strideWidth,\n          filterHeight = _backend_util$compute.filterHeight,\n          filterWidth = _backend_util$compute.filterWidth,\n          dilationHeight = _backend_util$compute.dilationHeight,\n          dilationWidth = _backend_util$compute.dilationWidth,\n          outShape = _backend_util$compute.outShape;\n\n    util.assert(dy.rank === outShape.length, () => \"Error in \".concat(Dilation2DBackpropFilter, \", dy \") + \"must have the same rank as output \".concat(outShape.length, \", but got \") + \"\".concat(dy.rank));\n    const $dy = util.toNestedArray(outShape, cpuBackend.data.get(dy.dataId).values); // The computed filter gradients has the same dimensions as the filter:\n    // [filterHeight, filterWidth, depth]\n\n    const gradients = util.makeZerosNestedTypedArray(filter.shape, filter.dtype); // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    // This implementation follows the TF c++ implementation:\n    // https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/dilation_ops.cc\n\n    for (let b = 0; b < batchSize; ++b) {\n      for (let hOut = 0; hOut < outHeight; ++hOut) {\n        const hBeg = hOut * strideHeight - padInfo.top;\n\n        for (let wOut = 0; wOut < outWidth; ++wOut) {\n          const wBeg = wOut * strideWidth - padInfo.left;\n\n          for (let d = 0; d < inChannels; ++d) {\n            let curVal = Number.MIN_SAFE_INTEGER;\n            let hMax = 0;\n            let wMax = 0;\n\n            for (let h = 0; h < filterHeight; ++h) {\n              const hIn = hBeg + h * dilationHeight;\n\n              if (hIn >= 0 && hIn < inHeight) {\n                for (let w = 0; w < filterWidth; ++w) {\n                  const wIn = wBeg + w * dilationWidth;\n\n                  if (wIn >= 0 && wIn < inWidth) {\n                    const val = $x[b][hIn][wIn][d] + $filter[h][w][d];\n\n                    if (val > curVal) {\n                      curVal = val;\n                      hMax = h;\n                      wMax = w;\n                    }\n                  }\n                }\n              }\n            }\n\n            gradients[hMax][wMax][d] += $dy[b][hOut][wOut][d];\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(util.toTypedArray(gradients, x.dtype), filter.shape, filter.dtype);\n    return {\n      dataId,\n      shape: filter.shape,\n      dtype: filter.dtype\n    };\n  }\n};","map":null,"metadata":{},"sourceType":"module"}